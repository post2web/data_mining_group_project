{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Lab 2\n",
    "MSDS 7331 Data Mining - Section 403 - Lab 2\n",
    "\n",
    "Team: Ivelin Angelov, Yao Yao, Kaitlin Kirasich, Albert Asuncion\n",
    "\n",
    "<a id=\"top\"></a>\n",
    "# Contents\n",
    "* <a href=\"#Imports\">Imports</a>\n",
    "* <a href=\"#Define\">Define and Prepare Class Variables</a>\n",
    "    - <a href=\"#define_c\">Classification Variables</a>\n",
    "    - <a href=\"#define_r\">Regression Variables</a>\n",
    "\n",
    "* <a href=\"#Describe\">Describe the Final Dataset</a>\n",
    "    - <a href=\"#describe_c\">Classification Dataset</a>\n",
    "    - <a href=\"#describe_r\">Regression Dataset</a>\n",
    "\n",
    "* <a href=\"#Evaluation\">Explain Evaluation Metrics</a>\n",
    "    - <a href=\"#metrcs_c\">Classification Metrics</a>\n",
    "    - <a href=\"#metrcs_r\">Regression Metrics</a>\n",
    "    \n",
    "* <a href=\"#Splits\">Training and Testing Splits</a>\n",
    "    - <a href=\"#splits_c\">For Classification</a>\n",
    "    - <a href=\"#splits_r\">For Regression</a>\n",
    "    \n",
    "* <a href=\"#Models\">Three Different Classification/Regression Models</a>\n",
    "    - <a href=\"#Classification_m\">Classification Models</a>\n",
    "        - <a href=\"#KNN_c_m\">K Nearest Neighbors</a>\n",
    "        - <a href=\"#RandomForest_c_m\">Random Forest</a>\n",
    "        - <a href=\"#NaiveBayes_c_m\">Naive Bayes</a>\n",
    "    - <a href=\"#Regression_m\">Regression Models</a>\n",
    "        - <a href=\"#KNN_r_m\">K Nearest Neighbors</a>\n",
    "        - <a href=\"#RandomForest_r_m\">Random Forest</a>\n",
    "        - <a href=\"#GaussianProcess_r_m\">Gaussian Regression</a>\n",
    "\n",
    "* <a href=\"#Analysis\">Visualizations of Results and Analysis</a>\n",
    "    - <a href=\"#Classification_a\">Analysis of Classification Models</a>\n",
    "        - <a href=\"#KNN_c_a\">Analysis of K Nearest Neighbors</a>\n",
    "        - <a href=\"#RandomForest_c_a\">Analysis of  Forest</a>\n",
    "        - <a href=\"#NaiveBayes_c_a\">Analysis of Naive Bayes</a>\n",
    "    - <a href=\"#Regression_a\">Regression Models</a>\n",
    "        - <a href=\"#KNN_r_a\">Analysis of K Nearest Neighbors</a>\n",
    "        - <a href=\"#RandomForest_r_a\">Analysis of Random Forest</a>\n",
    "        - <a href=\"#GaussianProcess_r_a\">Analysis of Gaussian Regression</a>\n",
    "\n",
    "* <a href=\"#Advantages\">Advantages of Each Model</a>\n",
    "    - <a href=\"#advantages_c\">Classification Models</a>\n",
    "    - <a href=\"#advantages_r\">Regression Models</a>\n",
    "        \n",
    "* <a href=\"#Attributes\">Important Attributes</a>\n",
    "    - <a href=\"#attributes_c\">Classification Models</a>\n",
    "    - <a href=\"#attributes_r\">Regression Models</a>\n",
    "    \n",
    "* <a href=\"#Deployment\">Deployment</a>\n",
    "* <a href=\"#Exceptional\">Exceptional Work</a>\n",
    "    - Feture Elimination\n",
    "    - Two dimentional Linear Discriminant Analysis\n",
    "* <a href=\"#References\">References</a>\n",
    "________________________________________________________________________________________________________\n",
    "<a id=\"Imports\"></a>\n",
    "# Imports & Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def output_variables_table(variables):\n",
    "    variables = variables.sort_index()\n",
    "    rows = ['<tr><th>Variable</th><th>Type</th><th>Scale</th><th>Description</th></tr>']\n",
    "    for vname, atts in variables.iterrows():\n",
    "        if vname not in dataset.columns:\n",
    "            continue\n",
    "        atts = atts.to_dict()\n",
    "        # add scale if TBD\n",
    "        if atts['scale'] == 'TBD':\n",
    "            if atts['type'] in ['nominal', 'ordinal']:\n",
    "                uniques = dataset[vname].unique()\n",
    "                uniques = list(uniques.astype(str))\n",
    "                if len(uniques) < 10:\n",
    "                    atts['scale'] = '[%s]' % ', '.join(uniques)\n",
    "                else:\n",
    "                    atts['scale'] = '[%s]' % (', '.join(uniques[:5]) + ', ... (%d More)' % len(uniques))\n",
    "            if atts['type'] in ['ratio', 'interval']:\n",
    "                atts['scale'] = '(%d, %d)' % (dataset[vname].min(), dataset[vname].max())\n",
    "        row = (vname, atts['type'], atts['scale'], atts['description'])\n",
    "        rows.append('<tr><td>%s</td><td>%s</td><td>%s</td><td>%s</td></tr>' % row)\n",
    "    return HTML('<table>%s</table>' % ''.join(rows))\n",
    "\n",
    "\n",
    "# Define an accuracy plot\n",
    "def per_class_accuracy(ytrue, yhat):\n",
    "    conf = mt.confusion_matrix(ytrue,yhat)\n",
    "    norm_conf = conf.astype('float') / conf.sum(axis=1)[:, np.newaxis]\n",
    "    return np.diag(norm_conf)\n",
    "\n",
    "def plot_class_acc(ytrue, yhat, classes, title=''):\n",
    "    acc_list = per_class_accuracy(y, yhat)\n",
    "    pd.DataFrame(acc_list, index=pd.Index(classes, name='Classes')).plot(kind='bar')\n",
    "    plt.xlabel('Class value (one per face)')\n",
    "    plt.ylabel('Accuracy within class')\n",
    "    plt.title(title+\", Total Acc=%.1f\"%(100*mt.accuracy_score(ytrue,yhat)))\n",
    "    plt.grid()\n",
    "    plt.ylim([0,1])\n",
    "    plt.show()\n",
    "    \n",
    "# Plot the feature importances of the forest\n",
    "def plot_feature_importance(ytrue, yhat, rt, title=''):\n",
    "    importances = rt.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in rt.estimators_],\n",
    "             axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    for f in range(X.shape[1]):\n",
    "        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "    plt.figure()\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "    plt.xticks(range(X.shape[1]), indices)\n",
    "    plt.xlim([-1, X.shape[1]])\n",
    "    plt.show()\n",
    "    \n",
    "def print_accuracy(model_name, y_test, yhat, scores):\n",
    "    scores = np.array(scores)\n",
    "    \n",
    "    print('----------------- %s Evaluation -----------------' % model_name)\n",
    "    print(\" F1 Score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    print(' Accuracy', mt.accuracy_score(y_test, yhat))\n",
    "    print(' Precision', mt.precision_score(y_test, yhat, average='weighted'))\n",
    "    print(' Recall', mt.recall_score(y_test, yhat, average='weighted'))\n",
    "    \n",
    "def confusion_matrix(ytrue, yhat, classes):\n",
    "    index = pd.MultiIndex.from_product([['True Class'], classes])\n",
    "    columns = pd.MultiIndex.from_product([['Predicted Class'], classes])\n",
    "    return pd.DataFrame(mt.confusion_matrix(y, yhat), index=index, columns=columns)\n",
    "\n",
    "def roc_curve(ytrue, yhat, clf):\n",
    "    for i, label in enumerate(clf.classes_):\n",
    "        fpr, tpr, _ = mt.roc_curve(y, yhat_score[:, i], pos_label=label)\n",
    "        roc_auc = mt.auc(fpr, tpr)\n",
    "\n",
    "        plt.plot(fpr, tpr, label='class {0} with {1} instances (area = {2:0.2f})'\n",
    "                                       ''.format(label, sum(y==label), roc_auc))\n",
    "\n",
    "        plt.title('ROC Curve')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def get_dataset_subset(dataset, n=1000):\n",
    "    return {\n",
    "        'X': dataset['X'].iloc[:n],\n",
    "        'y': dataset['y'].iloc[:n]\n",
    "    }\n",
    "\n",
    "\n",
    "seed = 0\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"Define\"></a>\n",
    "# Define and Prepare Class Variables\n",
    "<b>10 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis.\n",
    "</i>\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"define_c\"></a>\n",
    "## Classification Datasets:\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (116761, 2105)\n",
      "regionidcounty\n",
      "1286    35417\n",
      "2061    10261\n",
      "3101    71083\n",
      "Name: regionidcounty, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>Variable</th><th>Type</th><th>Scale</th><th>Description</th></tr><tr><td>airconditioningtypeid</td><td>nominal</td><td>[0, 1, 5, 13, 11, 9, 3]</td><td>Type of cooling system present in the home (if any)</td></tr><tr><td>assessmentyear</td><td>interval</td><td>(2015, 2015)</td><td>The year of the property tax assessment</td></tr><tr><td>bathroomcnt</td><td>ordinal</td><td>[2.5, 1.0, 2.0, 1.5, 3.0, ... (23 More)]</td><td>Number of bathrooms in home including fractional bathrooms</td></tr><tr><td>bedroomcnt</td><td>ordinal</td><td>[3, 2, 4, 5, 6, ... (17 More)]</td><td>Number of bedrooms in home</td></tr><tr><td>buildingqualitytypeid</td><td>ordinal</td><td>[7, 4, 10, 1, 8, 12, 6, 11]</td><td>Overall assessment of condition of the building from best (lowest) to worst (highest)</td></tr><tr><td>calculatedbathnbr</td><td>ordinal</td><td>[2.5, 1.0, 2.0, 1.5, 3.0, ... (23 More)]</td><td>Number of bathrooms in home including fractional bathroom</td></tr><tr><td>calculatedfinishedsquarefeet</td><td>ratio</td><td>(0, 10925)</td><td>Calculated total finished living area of the home</td></tr><tr><td>censustractandblock</td><td>nominal</td><td>[61110023618600.0, 61110015230000.0, 61110006841300.0, 61110052978700.0, 61110040395800.0, ... (445 More)]</td><td>Census tract and block ID combined - also contains blockgroup assignment by extension</td></tr><tr><td>finishedsquarefeet12</td><td>ratio</td><td>(0, 6615)</td><td>Finished living area</td></tr><tr><td>finishedsquarefeet50</td><td>ratio</td><td>(0, 8352)</td><td>Size of the finished living area on the first (entry) floor of the home</td></tr><tr><td>fips</td><td>nominal</td><td>[6111, 6037, 6059]</td><td>Federal Information Processing Standard code - see https://en.wikipedia.org/wiki/FIPS_county_code for more details</td></tr><tr><td>fireplacecnt</td><td>ordinal</td><td>[0, 1, 2, 3, 4, 5]</td><td>Number of fireplaces in a home (if any)</td></tr><tr><td>fullbathcnt</td><td>ordinal</td><td>[2.0, 1.0, 3.0, 7.0, 5.0, ... (19 More)]</td><td>Number of full bathrooms (sink, shower + bathtub, and toilet) present in home</td></tr><tr><td>garagecarcnt</td><td>ordinal</td><td>[2.0, 1.0, 3.0, 0.0, 4.0, ... (15 More)]</td><td>Total number of garages on the lot including an attached garage</td></tr><tr><td>garagetotalsqft</td><td>ratio</td><td>(0, 1610)</td><td>Total number of square feet of all garages on lot including an attached garage</td></tr><tr><td>hashottuborspa</td><td>ordinal</td><td>[0, 1]</td><td>Does the home have a hot tub or spa</td></tr><tr><td>heatingorsystemtypeid</td><td>nominal</td><td>[0, 2, 7, 6, 24, ... (13 More)]</td><td>Type of home heating system</td></tr><tr><td>landtaxvaluedollarcnt</td><td>ratio</td><td>(22, 2477536)</td><td>The assessed value of the land area of the parcel</td></tr><tr><td>location_type</td><td>nominal</td><td>[nan, PRIMARY, ACCEPTABLE, NOT ACCEPTABLE]</td><td>Primary, Acceptable, Not Acceptable</td></tr><tr><td>lotsizesquarefeet</td><td>ratio</td><td>(0, 1710750)</td><td>Area of the lot in square feet</td></tr><tr><td>numberofstories</td><td>ordinal</td><td>[2, 1, 3, 4]</td><td>Number of stories or levels the home has</td></tr><tr><td>parcelid</td><td>nominal</td><td>[17073783, 17088994, 17100444, 17102429, 17109604, ... (90150 More)]</td><td>Unique identifier for parcels (lots)</td></tr><tr><td>poolcnt</td><td>ordinal</td><td>[0.0, 1.0]</td><td>Number of pools on the lot (if any)</td></tr><tr><td>poolsizesum</td><td>ratio</td><td>(0, 1476)</td><td>Total square footage of all pools on property</td></tr><tr><td>pooltypeid10</td><td>nominal</td><td>[0, 1]</td><td>Spa or Hot Tub</td></tr><tr><td>pooltypeid2</td><td>nominal</td><td>[0, 1]</td><td>Pool with Spa/Hot Tub</td></tr><tr><td>pooltypeid7</td><td>nominal</td><td>[0, 1]</td><td>Pool without hot tub</td></tr><tr><td>propertycountylandusecode</td><td>nominal</td><td>[1128, 1129, 1111, 1110, 010C, ... (77 More)]</td><td>County land use code i.e. it's zoning at the county level</td></tr><tr><td>propertylandusetypeid</td><td>nominal</td><td>[265, 266, 261, 269, 246, ... (14 More)]</td><td>Type of land use the property is zoned for</td></tr><tr><td>propertyzoningdesc</td><td>nominal</td><td>[0, LARD3, LARS, LARE11, LCA21*, ... (1997 More)]</td><td>Description of the allowed land uses (zoning) for that property</td></tr><tr><td>roomcnt</td><td>ordinal</td><td>[5, 4, 8, 6, 9, ... (16 More)]</td><td>Total number of rooms in the principal residence</td></tr><tr><td>structuretaxvaluedollarcnt</td><td>ratio</td><td>(100, 2181198)</td><td>The assessed value of the built structure on the parcel</td></tr><tr><td>taxamount</td><td>ratio</td><td>(49, 51292)</td><td>The total property tax assessed for that assessment year</td></tr><tr><td>taxdelinquencyflag</td><td>nominal</td><td>[0, 1]</td><td>Property taxes for this parcel are past due as of 2015</td></tr><tr><td>taxdelinquencyyear</td><td>interval</td><td>(0, 26)</td><td>Year</td></tr><tr><td>taxvaluedollarcnt</td><td>ratio</td><td>(22, 4052186)</td><td>The total tax assessed value of the parcel</td></tr><tr><td>threequarterbathnbr</td><td>ordinal</td><td>[1, 0, 2, 4, 3]</td><td>Number of 3/4 bathrooms in house (shower + sink + toilet)</td></tr><tr><td>unitcnt</td><td>ordinal</td><td>[1, 2, 4, 3, 9, 13, 5, 6, 11]</td><td>Number of units the structure is built into (i.e. 2 = duplex, 3 = triplex, etc...)</td></tr><tr><td>yardbuildingsqft17</td><td>interval</td><td>(0, 1485)</td><td>Patio in yard</td></tr><tr><td>yardbuildingsqft26</td><td>interval</td><td>(0, 1366)</td><td>Storage shed/building in yard</td></tr><tr><td>yearbuilt</td><td>interval</td><td>(1885, 2015)</td><td>The Year the principal residence was built</td></tr><tr><td>zipcode_type</td><td>nominal</td><td>[nan, STANDARD, PO BOX, MILITARY, UNIQUE]</td><td>Standard, PO BOX Only, Unique, Military(implies APO or FPO)</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables = pd.read_csv('../../datasets/variables.csv').set_index('name')\n",
    "dataset = pd.read_csv('../../datasets/train.csv', low_memory=False)\n",
    "\n",
    "# remove unneeded variables\n",
    "del dataset['logerror']\n",
    "del dataset['transactiondate']\n",
    "del dataset['city']\n",
    "del dataset['price_per_sqft']\n",
    "\n",
    "\n",
    "# delete all location information because we want to predict the couty\n",
    "# and those feature will give it up to easy\n",
    "y = dataset['regionidcounty'].copy()\n",
    "del dataset['regionidcounty']\n",
    "del dataset['regionidcity']\n",
    "del dataset['regionidzip']\n",
    "del dataset['regionidneighborhood']\n",
    "del dataset['rawcensustractandblock']\n",
    "del dataset['latitude']\n",
    "del dataset['longitude']\n",
    "\n",
    "output_variables = output_variables_table(variables)\n",
    "\n",
    "nominal = variables[variables['type'].isin(['nominal'])]\n",
    "nominal = nominal[nominal.index.isin(dataset.columns)]\n",
    "continuous = variables[~variables['type'].isin(['nominal'])]\n",
    "continuous = continuous[continuous.index.isin(dataset.columns)]\n",
    "\n",
    "nominal_data = dataset[nominal.index]\n",
    "nominal_data = pd.get_dummies(nominal_data, drop_first=True)\n",
    "nominal_data = nominal_data[nominal_data.columns[~nominal_data.columns.isin(nominal.index)]]\n",
    "\n",
    "continuous_data = dataset[continuous.index]\n",
    "\n",
    "dataset = pd.concat([continuous_data, nominal_data], axis=1)\n",
    "\n",
    "columns = dataset.columns\n",
    "variables = variables[variables.index.isin(dataset.columns)]\n",
    "\n",
    "# shuffle the dataset (just in case)\n",
    "dataset = dataset.sample(frac=1, random_state=seed)\n",
    "\n",
    "X = dataset\n",
    "dataset_class = {\n",
    "    'X': X,\n",
    "    'y': y\n",
    "}\n",
    "print('Dataset shape:', X.shape)\n",
    "print(y.groupby(y).size())\n",
    "output_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"define_r\"></a>\n",
    "## Regression Datasets:\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "variables = pd.read_csv('../../datasets/variables.csv').set_index('name')\n",
    "dataset = pd.read_csv('../../datasets/train.csv', low_memory=False)\n",
    "\n",
    "# remove unneeded variables\n",
    "del dataset['logerror']\n",
    "del dataset['transactiondate']\n",
    "del dataset['city']\n",
    "del dataset['price_per_sqft']\n",
    "\n",
    "output_variables = output_variables_table(variables)\n",
    "\n",
    "nominal = variables[variables['type'].isin(['nominal'])]\n",
    "nominal = nominal[nominal.index.isin(dataset.columns)]\n",
    "continuous = variables[~variables['type'].isin(['nominal'])]\n",
    "continuous = continuous[continuous.index.isin(dataset.columns)]\n",
    "\n",
    "nominal_data = dataset[nominal.index]\n",
    "nominal_data = pd.get_dummies(nominal_data, drop_first=True)\n",
    "nominal_data = nominal_data[nominal_data.columns[~nominal_data.columns.isin(nominal.index)]]\n",
    "\n",
    "continuous_data = dataset[continuous.index]\n",
    "\n",
    "dataset = pd.concat([continuous_data, nominal_data], axis=1)\n",
    "\n",
    "columns = dataset.columns\n",
    "variables = variables[variables.index.isin(dataset.columns)]\n",
    "\n",
    "# shuffle the dataset (just in case)\n",
    "dataset = dataset.sample(frac=1, random_state=seed)\n",
    "\n",
    "X = dataset\n",
    "y = X['taxamount'].copy()\n",
    "del X['taxamount']\n",
    "# del X['taxvaluedollarcnt']????\n",
    "\n",
    "dataset_reg = {\n",
    "    'X': X,\n",
    "    'y': y\n",
    "}\n",
    "print('Dataset shape:', X.shape)\n",
    "y.plot(kind='box')\n",
    "output_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"Describe\"></a>\n",
    "# Describe the Final Dataset\n",
    "<b>5 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).\n",
    "</i>\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"describe_c\"></a>\n",
    "## Classification Datasets:\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Since we are using the same Zillow dataset that we used in the previous lab, most of the data was already cleaned up.  However, the purpose of our classification dataset is to predict the county each property is located in.  Therefore our final model was striped of all columns relating to location such as latitude, longitude, city, and zipcode.  We also removed variables we did not need such as logerror, transaction date, and price per square foot.\n",
    "\n",
    "We did not create any new columns for the classification dataset but we did categorical variable into indicator variables.\n",
    "\n",
    "The final shape of our classification dataset is 116761 instances and 2105 remaining columns.  The three counties we are trying to predict have sizes of about 35k, 10k, and 71k so an accuracy below 0.61 will mean that we are better off with classifing each as the latter county."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"describe_r\"></a>\n",
    "## Regression Datasets:\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"Evaluation\"></a>\n",
    "# Explain Evaluation Metrics\n",
    "<b>10 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "Choose and explain your evaluation metrics that you will use (i.e., accuracy, precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up any assertions.\n",
    "</i>\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"metrcs_c\"></a>\n",
    "## Classification Metrics:\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>\n",
    "\n",
    "For our evaluation, we will be taking into account the accuracy and F-measure.  In order to compute the F-measure, we will need the precision and recall.  Because F-measure is a weighted average of these, we think a better F-measure score means the model has a better precision and recall.\n",
    "\n",
    "Accuracy is the ratio of correct predictions to the total number of observations. It is calculated as: (TP+TN) / (TP+FP+FN+TN). The closer accuracy is to 1, the more accurate the model is, with one caveat. For high accuracy to be a reliable indicator, the dataset has to be symmetric, i.e. total false positives are about equal to false negatives. Otherwise, we need to review other parameters as well.\n",
    "\n",
    "Precision is the ratio of correctly predicted positive observations to the total positive observations. It is calculated as: TP / (TP+FP).  \n",
    "\n",
    "Recall is the ratio of correctly predicted positive observations to all actual positives. It is calculated as TP / (TP+FN). The consequences of type 2 errors, predicting a false negative, are not extreme so we think recall is an appropriate measure of completeness.\n",
    "\n",
    "Finally, we will also use F-measure which is essentially a weighted average of the precision and recall into one simple statistic. F-measure This will be a number between 0 and 1 where closer to 1 is better and approaching 0 is worse. It overcomes the limitations of accuracy whenever false positives and false negatives are not about equal or symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"metrcs_r\"></a>\n",
    "## Regression Metrics:\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"Splits\"></a>\n",
    "# Training and Testing Splits\n",
    "<b>10 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "Choose the method you will use for dividing your data into training and testing splits (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time.\n",
    "</i>\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Chart of train size while accuracy continues to increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "X = dataset_class['X']\n",
    "y = dataset_class['y']\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=.1, random_state=0)\n",
    "\n",
    "result = []\n",
    "\n",
    "# uniform distribution values between 0 and 1\n",
    "np.random.seed(seed)\n",
    "mask = np.random.rand(len(X))\n",
    "\n",
    "for frac in np.linspace(.01, 1, 20):\n",
    "    mask_frac = mask<=frac\n",
    "    X_frac = X[mask_frac]\n",
    "    y_frac = y[mask_frac]\n",
    "\n",
    "    clf = RandomForestClassifier(max_depth=200, n_estimators=40, random_state=seed)\n",
    "    clf.fit(X_frac, y_frac)\n",
    "    y_hat = clf.predict(X_test)\n",
    "    \n",
    "    result.append({\n",
    "        'f1_score': mt.f1_score(y_test, y_hat, average='weighted'),\n",
    "        'count': len(X_frac),\n",
    "        'frac': frac\n",
    "    })\n",
    "\n",
    "pd.DataFrame(result).plot('count', 'f1_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"splits_c\"></a>\n",
    "## Classification Splits:\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>\n",
    "\n",
    "For the classification task we choose to use Stratified K-Fold cross validation with 5 folds.  We chose stratified in order to preserve the percentage of samples in each class.  We also had a very large dataset so splitting into more than 5 folds would have been computationally expensive with not a large enough return on value.  We felt that splitting the data into 5 folds would be enough splits to reduce the weight of any outliers or noise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"splits_r\"></a>\n",
    "## Regression Splits:\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>\n",
    "\n",
    "For the regression task we choose to use K-Fold cross validation with 5 folds.\n",
    "\n",
    "# TODO say why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"Models\"></a>\n",
    "# Three Different Classification/Regression Models\n",
    "<b>20 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!\n",
    "</i>\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"Classification_m\"></a>\n",
    "## Classification Models:\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"KNN_c_m\"></a>\n",
    "### Definition and optimization of K Nearest Neighbors (KD Tree) \n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA, SparsePCA\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "\n",
    "X = dataset_class['X']\n",
    "y = dataset_class['y']\n",
    "\n",
    "result = []\n",
    "scores = []\n",
    "\n",
    "for n_neighbors in range(2, 30)[::5]:\n",
    "    yhat = np.zeros(y.shape) # we will fill this with predictions\n",
    "    cv = StratifiedKFold(n_splits=n_splits, random_state=seed)\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # in order to reduce the time for training KNeighborsClassifier\n",
    "        # we reduce the dimetions of the data from 1717 to 100 and we use kd_tree algorithm\n",
    "        pca = PCA(n_components=100, random_state=seed)\n",
    "        pca.fit(X_train)\n",
    "        X_train = pca.transform(X_train)\n",
    "        X_test = pca.transform(X_test)\n",
    "\n",
    "        clf = KNeighborsClassifier(n_neighbors=n_neighbors, algorithm='kd_tree', weights='distance')\n",
    "        clf.fit(X_train, y_train)\n",
    "        yhat[test_index] = clf.predict(X_test)\n",
    "\n",
    "    f1_score = mt.f1_score(y, yhat, average='weighted')\n",
    "    print ('n_neighbors:', n_neighbors, ', f1_score:', f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"RandomForest_c_m\"></a>\n",
    "### Definition and optimization of Random Forest\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = dataset_class['X']\n",
    "y = dataset_class['y']\n",
    "\n",
    "result = []\n",
    "index = []\n",
    "for max_depth in range(1, 401)[::50]:\n",
    "    yhat = np.zeros(y.shape, dtype=int) # we will fill this with predictions\n",
    "    cv = StratifiedKFold(n_splits=n_splits, random_state=seed)\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        clf = RandomForestClassifier(max_depth=max_depth, random_state=seed, n_estimators=40)\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        clf.fit(X_train, y_train)\n",
    "        yhat[test_index] = clf.predict(X_test)\n",
    "\n",
    "    f1_score = mt.f1_score(y, yhat, average='weighted')\n",
    "    print ('max_depth:', max_depth, 'F1 score:', f1_score)\n",
    "    result.append(f1_score)\n",
    "    index.append(max_depth)\n",
    "\n",
    "plt.title('F1 score for different max_depth')\n",
    "pd.Series(result, index=pd.Index(index, name='max_depth'), name='f1_score').plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"NaiveBayes_c_m\"></a>\n",
    "### Definition and optimization of  Naive Bayes (Gaussian)\n",
    "\n",
    "The Naive Bayes doesn't have any paramethers to optimize\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "X = dataset_class['X']\n",
    "y = dataset_class['y']\n",
    "\n",
    "\n",
    "yhat = np.zeros(y.shape) # we will fill this with predictions\n",
    "cv = StratifiedKFold(n_splits=n_splits, random_state=seed)\n",
    "scores = []\n",
    "\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "    clf = GaussianNB()\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    yhat[test_index] = clf.predict(X_test)\n",
    "    \n",
    "    f1_score = mt.f1_score(y_test, clf.predict(X_test), average='weighted')\n",
    "    scores.append(f1_score)\n",
    "\n",
    "scores = np.array(scores)\n",
    "print(\"F1: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "print_accuracy('MultinomialNB', y, yhat)\n",
    "confusion_matrix(y, yhat, clf.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"Classification_m\"></a>\n",
    "## Definition and optimization of  Regression models:\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"KNN_r_m\"></a>\n",
    "## Definition and optimization of  K Nearest Neighbours\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "yhat = np.zeros(y.shape)\n",
    "cv = KFold(n_splits=10, random_state=0)\n",
    "n_neighbors_result = []\n",
    "leaf_size_result = []\n",
    "\n",
    "for n_neighbors in [1,2,3,4,5,6]:\n",
    "    for leaf_size in [50,100,200,300]:\n",
    "        for train_index, test_index in cv.split(X, y):\n",
    "            clf = KNeighborsRegressor(n_neighbors=n_neighbors)\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            clf.fit(X_train, y_train)\n",
    "            yhat[test_index] = clf.predict(X_test)\n",
    "        print(\"n_neighbors: %.f, leaf_size: %.f, MSE: %.f, R^2: %0.3f\" % (n_neighbors, leaf_size, mean_squared_error(y, yhat), r2_score(y, yhat)))\n",
    "        n_neighbors_result.append((n_neighbors, r2_score(y, yhat)))\n",
    "        leaf_size_result.append((leaf_size, r2_score(y, yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"RandomForest_r_m\"></a>\n",
    "## Definition and optimization of  Random Forest\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X = dataset_reg['X']\n",
    "y = dataset_reg['y']\n",
    "\n",
    "yhat = np.zeros(y.shape)\n",
    "cv = KFold(n_splits=10, random_state=0)\n",
    "\n",
    "for max_depth in [1,2,3,4]:\n",
    "    for n_estimators in [50, 100, 200, 300]:\n",
    "        for train_index, test_index in cv.split(X, y):\n",
    "            clf = RandomForestRegressor(max_depth=max_depth, n_estimators=n_estimators, random_state=0)\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            clf.fit(X_train, y_train)\n",
    "            yhat[test_index] = clf.predict(X_test)\n",
    "        print(\"max_depth: %.f, n_estimators: %.f, MSE: %.f, R^2: %0.3f\" % (max_depth, n_estimators, mean_squared_error(y, yhat), r2_score(y, yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"GaussianProcess_r_m\"></a>\n",
    "## Definition and optimization of  Gaussian Regression\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rom sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X = dataset_reg['X']\n",
    "y = dataset_reg['y']\n",
    "\n",
    "yhat = np.zeros(y.shape)\n",
    "cv = KFold(n_splits=10, random_state=0)\n",
    "\n",
    "for alpha in np.linspace(1e-15, 0.001, 5):\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        clf = GaussianProcessRegressor(normalize_y=True, alpha=alpha, random_state=0)\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        clf.fit(X_train, y_train)\n",
    "        yhat[test_index] = clf.predict(X_test)\n",
    "    print(\"alpha: %f, MSE: %.f, R^2: %0.3f\" % (alpha, mean_squared_error(y, yhat), r2_score(y, yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"Analysis\"></a>\n",
    "# Visualizations of Results and Analysis\n",
    "<b>10 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model.\n",
    "</i>\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"Classification_a\"></a>\n",
    "## Analysis of Classification model:\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Results and Analysis of a Dummy model\n",
    "This model is only predicting the most frequent class. It is used for a base line to compare other models to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print_accuracy('Dummy', y, [3101] * len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"KNN_c_a\"></a>\n",
    "### Results and Analysis of K Nearest Neighbors (KD Tree)\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = dataset_class['X']\n",
    "y = dataset_class['y']\n",
    "\n",
    "scores = []\n",
    "yhat = np.zeros(y.shape)\n",
    "yhat_score = np.zeros((len(y), 3))\n",
    "\n",
    "cv = StratifiedKFold(n_splits=n_splits, random_state=seed)\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "    clf = KNeighborsClassifier(n_neighbors=17, algorithm='kd_tree', weights='distance')\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    pca = PCA(n_components=100, random_state=seed)\n",
    "    pca.fit(X_train)\n",
    "    X_train = pca.transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    yhat[test_index] = clf.predict(X_test)\n",
    "    yhat_score[test_index] = clf.predict_proba(X_test)\n",
    "    \n",
    "    f1_score = mt.f1_score(y_test, clf.predict(X_test), average='weighted')\n",
    "    scores.append(f1_score)\n",
    "\n",
    "print_accuracy('KD Tree Classifier', y, yhat, scores)\n",
    "plot_class_acc(y, yhat, clf.classes_, title=\"KD Tree Classifier\")\n",
    "confusion_matrix(y, yhat, clf.classes_)\n",
    "roc_curve(y, yhat, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"RandomForest_c_a\"></a>\n",
    "## Results and Analysis of Random Forest\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = dataset_class['X']\n",
    "y = dataset_class['y']\n",
    "\n",
    "scores = []\n",
    "yhat = np.zeros(y.shape)\n",
    "yhat_score = np.zeros((len(y), 3))\n",
    "\n",
    "cv = StratifiedKFold(n_splits=n_splits, random_state=seed)\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "\n",
    "    \n",
    "    clf = RandomForestClassifier(random_state=seed, max_depth=200, n_estimators=40)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    yhat[test_index] = clf.predict(X_test)\n",
    "    yhat_score[test_index] = clf.predict_proba(X_test)\n",
    "    \n",
    "    f1_score = mt.f1_score(y_test, clf.predict(X_test), average='weighted')\n",
    "    scores.append(f1_score)\n",
    "\n",
    "print_accuracy('KD Tree Classifier', y, yhat, scores)\n",
    "plot_class_acc(y, yhat, clf.classes_, title=\"KD Tree Classifier\")\n",
    "confusion_matrix(y, yhat, clf.classes_)\n",
    "roc_curve(y, yhat, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"NaiveBayes_c_a\"></a>\n",
    "## Results and Analysis Naive Bayes\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "X = dataset_class['X']\n",
    "y = dataset_class['y']\n",
    "\n",
    "scores = []\n",
    "yhat = np.zeros(y.shape)\n",
    "yhat_score = np.zeros((len(y), 3))\n",
    "\n",
    "cv = StratifiedKFold(n_splits=n_splits, random_state=seed)\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "\n",
    "    clf = GaussianNB()\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    yhat[test_index] = clf.predict(X_test)\n",
    "    yhat_score[test_index] = clf.predict_proba(X_test)\n",
    "    \n",
    "    f1_score = mt.f1_score(y_test, clf.predict(X_test), average='weighted')\n",
    "    scores.append(f1_score)\n",
    "\n",
    "print_accuracy('KD Tree Classifier', y, yhat, scores)\n",
    "plot_class_acc(y, yhat, clf.classes_, title=\"KD Tree Classifier\")\n",
    "confusion_matrix(y, yhat, clf.classes_)\n",
    "roc_curve(y, yhat, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"Regression_a\"></a>\n",
    "## Analysis of Regression models:\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"KNN_r_a\"></a>\n",
    "## Results and Analysis K Nearest Neighbours\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>\n",
    "\n",
    "# TODO find the code for this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"RandomForest_r_a\"></a>\n",
    "## Results and Analysis Random Forest\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = dataset_reg['X']\n",
    "y = dataset_reg['y']\n",
    "\n",
    "mses = []\n",
    "r2s = []\n",
    "yhat = np.zeros(y.shape) # we will fill this with predictions\n",
    "cv = KFold(n_splits=10, random_state=seed)\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "    clf = RandomForestRegressor(max_depth=4, n_estimators=200, random_state=0)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    clf.fit(X_train, y_train)\n",
    "    yhat[test_index] = clf.predict(X_test)\n",
    "\n",
    "    mses.append(mean_squared_error(y_test, clf.predict(X_test)))\n",
    "    r2s.append(r2_score(y_test, clf.predict(X_test)))\n",
    "\n",
    "print('Evaluation metrics:')\n",
    "print(\"MSE: %0.2f (+/- %0.2f)\" % (np.mean(mses), np.std(mses) * 2))\n",
    "print(\"R2: %0.2f (+/- %0.2f)\" % (np.mean(r2s), np.std(r2s) * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"GaussianProcess_r_a\"></a>\n",
    "## Results and Analysis Gaussian Regression\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = dataset_reg['X']\n",
    "y = dataset_reg['y']\n",
    "\n",
    "mses = []\n",
    "r2s = []\n",
    "yhat = np.zeros(y.shape) # we will fill this with predictions\n",
    "cv = KFold(n_splits=10, random_state=seed)\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "    clf = GaussianProcessRegressor(alpha=1e-15, normalize_y=True, random_state=0)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    clf.fit(X_train, y_train)\n",
    "    yhat[test_index] = clf.predict(X_test)\n",
    "    \n",
    "    \n",
    "    mses.append(mean_squared_error(y_test, clf.predict(X_test)))\n",
    "    r2s.append(r2_score(y_test, clf.predict(X_test)))\n",
    "\n",
    "print(\"MSE: %0.2f (+/- %0.2f)\" % (np.mean(mses), np.std(mses) * 2))\n",
    "print(\"R2: %0.2f (+/- %0.2f)\" % (np.mean(r2s), np.std(r2s) * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"Advantages\"></a>\n",
    "# Advantages of Each Model\n",
    "<b>10 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniques—be sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course.\n",
    "</i>\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"advantages_c\"></a>\n",
    "## Analysis of Classification model:\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### KNN - KD tree\n",
    "K nearest neighbors classification is different than other classification models in that it does not attempt to make a model but only stores instances of the training data.  The classification will build a tree which at each node is a rule based on the dimensions of its k nearest neighbors and leading to each leaf representing a class.  The advantages of K nearest neighbors is that it is simple and converges to the correct decision surface as data goes to infinity.  It can be used with multiclass data sets as well as more complex algorithms such as the KD tree. In our dataset we used the KD tree algorithm in order to speed up the KNN classification by indexing the tree.\n",
    "\n",
    "### Why we did PCA with KNN\n",
    "KNN computes the distance between each neighbor's dimensions.  We have so many dimensions in our dataset that even with 100 neighbors the accuracy was still continuing to grow and taking a long time.  We wanted to continue to increase the number of neighbors until the accuracy increase plateaued but without reducing the dimensions using PCA, it would have taken too long.\n",
    "\n",
    "### Random forest\n",
    "Random forest is an ensemble classification algorithm which in nature is a huge advantage because because predicting off of one decision tree vs an ensemble of them the ensemble will usually win.  Another advantage is that the forest can often correct a tree's overfitting of the training set.\n",
    "\n",
    "### Gassian Naive Bayes\n",
    "Naive Bayes largest advantage is that it is extremely simple, it is just counting up probabilities.  When training sets are small, Naive Bayes is good because of its high bias and low variance which will not overfit the training data.  However, as datasets grow larger such as our dataset, the high bias will prevent the model from being powerful enough to have a high accuracy.  Gassian Naive Bayes is a normally distributed NB classifier.  Its advantages are that it is fast and can make probabilistic decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"advantages_r\"></a>\n",
    "## Analysis of Regression model:\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TODO fix this\n",
    "advantages of GaussianProcessRegressor\n",
    "http://scikit-learn.org/stable/modules/gaussian_process.html\n",
    "\n",
    "The prediction interpolates the observations, faster\n",
    "https://stats.stackexchange.com/questions/207183/main-advantages-of-gaussian-process-models\n",
    "easy to manipulate and use\n",
    "https://www.quora.com/What-are-some-advantages-of-using-Gaussian-Process-Models-vs-SVMs\n",
    "provides full probabilistic prediction\n",
    "advantages of RandomForestRegressor\n",
    "https://www.quora.com/What-are-some-advantages-of-using-a-random-forest-over-a-decision-tree-given-that-a-decision-tree-is-simpler\n",
    "by averaging several trees, it reduces overfitting and is therefore more accurate\n",
    "By using multiple trees, it reduces variance from outliers from the train and test data\n",
    "https://www.datasciencecentral.com/profiles/blogs/random-forests-algorithm\n",
    "Generates an internal unbiased estimate of the generalization error as the forest building progresses\n",
    "Provides effective methods for estimating missing data\n",
    "Capabilities of the above can be extended to unlabeled data, leading to unsupervised clustering, data views and outlier detection\n",
    "advantages of KNeighborsRegressor\n",
    "https://stats.stackexchange.com/questions/104255/why-would-anyone-use-knn-for-regression\n",
    "scales up to big data.\n",
    "https://machinelearningmastery.com/k-nearest-neighbors-for-machine-learning/\n",
    "Address Missing Data\n",
    "lower dimensionality KNN can benefit from feature selection that reduces the dimensionality of the input feature space\n",
    "https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/#pros-and-cons-of-knn\n",
    "KNN works just as easily with multiclass data sets\n",
    "the non-parametric nature of KNN gives it an edge in certain settings where the data may be highly “unusual”.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"Attributes\"></a>\n",
    "# Important Attributes\n",
    "<b>10 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task.\n",
    "</i>\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Feature importance for classifaction dataset according to Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = dataset_class['X']\n",
    "y = dataset_class['y']\n",
    "\n",
    "clf = RandomForestClassifier(random_state=seed, max_depth = 5)\n",
    "clf.fit(X, y)\n",
    "importances = clf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in clf.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Random Forest Feature ranking:\")\n",
    "for f in range(X.shape[1]):\n",
    "    if importances[indices[f]] > 0.05:\n",
    "        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.title(\"Random Forest Feature importances\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\n",
    "feature_names = X.columns\n",
    "plt.xticks(range(10), feature_names)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlim([-1, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Feature importance for regression dataset according to Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-da39a983f6dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimportances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RandomForestRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "clf = RandomForestRegressor(max_depth=10, n_estimators=50, random_state=0)\n",
    "clf.fit(X, y)\n",
    "importances = clf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in clf.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Random Forest Feature ranking:\")\n",
    "for f in range(X.shape[1]):\n",
    "    if importances[indices[f]] > 0.05:\n",
    "        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.title(\"Random Forest Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\n",
    "feature_names = X.columns\n",
    "plt.xticks(range(10), feature_names)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlim([-1, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"Deployment\"></a>\n",
    "# Deployment\n",
    "<b>5 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "How useful is your model for interested parties (i.e., the companies or organizations that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.?\n",
    "</i>\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Zillow is at the forefront of real estate data analytics, refining its proprietary Zestimate price calculation tool since 2006. According to Steph Humphries, Chief Analytics Officer & Economist, Zillow’s goal is to provide buyers and sellers with valuation that “starts the conversation”. He noted that since Zillow began offering publicly available real estate data from disparate sources into a single platform, the gap between sellers’ prices and buyers’ offer prices has significantly decreased.\n",
    "\n",
    "The Zillow dataset was provided for the purpose of evaluating Zestimate’s accuracy based upon the variable `logerror` which is the difference of log(Zestimate) - log(SalePrice). For purposes of this lab assignment, we developed regression and classification models on the `regionidcounty` field (i.e. the county identifier). ****** TODO Insert insights form regression and classification models here *******\n",
    "\n",
    "For companies in the real estate space, classification models based on “hedonic” (i.e. physical characteristics) attributes provide valuable insight for buying, selling, and investment decisions. Our classification model can be adapted to more granular levels as cities and municipalities. Buyers, sellers, and investors alike can glean insights into which hedonic features have the highest importance to specific locations. This may drive investment decisions knowing how important certain attributes are for targeted locations. Knowing which features are highly important in certain locations can drive remodeling decisions to make properties more attractive to potential buyers. The value-add of this model for these companies can be measured in terms of returns on investment.\n",
    "\n",
    "This can be valuable for the rental market as well, as Humphries explains Zillow’s collaborations with Airbnb. Airbnb can direct marketing efforts to areas with specific property attributes. This can also be used to provide the break-even horizon for making rent versus own decisions. \n",
    "\n",
    "In addition, loan refinancing companies can utilize this model along with Zillow’s liens and taxes database to target homeowners in specific areas. To track effectiveness of marketing campaigns based on this model, response rates can be tracked as well as conversion rates.\n",
    "\n",
    "To further improve the effectiveness of the model, we should expand the model to include sales prices, liens, taxes, as well as identify biased data such as short sales, foreclosures, and “arms-length” transactions (i.e. sales to relatives). All these are readily available from Zillow, as they collect an enormous amount of data which we imagine is updated frequently. Humphries put in context when he said, Zillow started with 34,000 statistical models per month in 2006, they now generate between 7 and 8 million models every night.\n",
    "\n",
    "Reference: http://www.zdnet.com/article/zillow-machine-learning-and-data-in-real-estate/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"Exceptional\"></a>\n",
    "# Exceptional Work\n",
    "<b>10 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "You have free reign to provide additional analyses. One idea: grid search parameters in a parallelized fashion and visualize the\n",
    "performances across attributes. Which parameters are most significant for making a good model for each classification algorithm?\n",
    "</i>\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Feature elimination\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Two dimentional Linear Discriminant Analysis\n",
    "\n",
    "The idea is to see if there are separatable clusters by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "X = dataset_class['X']\n",
    "y = dataset_class['y']\n",
    "\n",
    "lde = LDA(n_components=2)\n",
    "X_lde = lde.fit(X, y).transform(X)\n",
    "\n",
    "colors = y.astype(str)\n",
    "colors[colors=='3101'] = 'g'\n",
    "colors[colors=='2061'] = 'b'\n",
    "colors[colors=='1286'] = 'r'\n",
    "\n",
    "plt.scatter(X_lde[:, 1], X_lde[:, 0], s=2, c=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"References\"></a>\n",
    "# References:\n",
    "\n",
    "- Kernels from Kaggle competition: https://www.kaggle.com/c/zillow-prize-1/kernels\n",
    "- Scikitlearn logistic regression: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "- Scikitlearn linear SVC: http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "- Stackoverflow pandas questions: https://stackoverflow.com/questions/tagged/pandas\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
