{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Zillow Dataset Classification and Regression Prediction Models\n",
    "MSDS 7331 Data Mining - Section 403 - Lab 2\n",
    "\n",
    "Team: Ivelin Angelov, Yao Yao, Kaitlin Kirasich, Albert Asuncion\n",
    "\n",
    "<a id=\"top\"></a>\n",
    "# Contents\n",
    "* <a href=\"#Imports\">Imports</a>\n",
    "* <a href=\"#Define\">Define and Prepare Class Variables</a>\n",
    "    - <a href=\"#define_c\">Classification Variables</a>\n",
    "    - <a href=\"#define_r\">Regression Variables</a>\n",
    "\n",
    "* <a href=\"#Describe\">Describe the Final Dataset</a>\n",
    "    - <a href=\"#describe_c\">Classification Dataset</a>\n",
    "    - <a href=\"#describe_r\">Regression Dataset</a>\n",
    "\n",
    "* <a href=\"#Evaluation\">Explain Evaluation Metrics</a>\n",
    "    - <a href=\"#metrcs_c\">Classification Metrics</a>\n",
    "    - <a href=\"#metrcs_r\">Regression Metrics</a>\n",
    "    \n",
    "* <a href=\"#Splits\">Training and Testing Splits</a>\n",
    "    - <a href=\"#splits_c\">For Classification</a>\n",
    "    - <a href=\"#splits_r\">For Regression</a>\n",
    "    \n",
    "* <a href=\"#Models\">Three Different Classification/Regression Models</a>\n",
    "    - <a href=\"#Classification_m\">Classification Models</a>\n",
    "        - <a href=\"#KNN_c_m\">K Nearest Neighbors</a>\n",
    "        - <a href=\"#RandomForest_c_m\">Random Forest</a>\n",
    "        - <a href=\"#NaiveBayes_c_m\">Naive Bayes</a>\n",
    "    - <a href=\"#Regression_m\">Regression Models</a>\n",
    "        - <a href=\"#KNN_r_m\">K Nearest Neighbors</a>\n",
    "        - <a href=\"#RandomForest_r_m\">Random Forest</a>\n",
    "        - <a href=\"#GaussianProcess_r_m\">Gaussian Regression</a>\n",
    "\n",
    "* <a href=\"#Analysis\">Visualizations of Results and Analysis</a>\n",
    "    - <a href=\"#Classification_a\">Analysis of Classification Models</a>\n",
    "        - <a href=\"#KNN_c_a\">Analysis of K Nearest Neighbors</a>\n",
    "        - <a href=\"#RandomForest_c_a\">Analysis of  Forest</a>\n",
    "        - <a href=\"#NaiveBayes_c_a\">Analysis of Naive Bayes</a>\n",
    "    - <a href=\"#Regression_a\">Regression Models</a>\n",
    "        - <a href=\"#KNN_r_a\">Analysis of K Nearest Neighbors</a>\n",
    "        - <a href=\"#RandomForest_r_a\">Analysis of Random Forest</a>\n",
    "        - <a href=\"#GaussianProcess_r_a\">Analysis of Gaussian Regression</a>\n",
    "\n",
    "* <a href=\"#Advantages\">Advantages of Each Model</a>\n",
    "    - <a href=\"#advantages_c\">Classification Models</a>\n",
    "    - <a href=\"#advantages_r\">Regression Models</a>\n",
    "        \n",
    "* <a href=\"#Attributes\">Important Attributes</a>\n",
    "    - <a href=\"#attributes_c\">Classification Models</a>\n",
    "    - <a href=\"#attributes_r\">Regression Models</a>\n",
    "    \n",
    "* <a href=\"#Deployment\">Deployment</a>\n",
    "* <a href=\"#Exceptional\">Exceptional Work</a>\n",
    "    - Feature Elimination\n",
    "    - Two dimensional Linear Discriminant Analysis\n",
    "* <a href=\"#References\">References</a>\n",
    "________________________________________________________________________________________________________\n",
    "<a id=\"Imports\"></a>\n",
    "# Imports & Custom Functions\n",
    "We chose to use the same Zillow dataset from Lab 1 for this exploration in regression and classification. For the origin and purpose of dataset as well as a detailed description of the dataset, refer to https://github.com/post2web/data_mining_group_project/blob/master/notebooks/lab1.ipynb. \n",
    "\n",
    "The function `output_variables_table` shows if the variable is nominal or ordinal for further use on classification or regression. The functions `per_class_accuracy` and `confusion_matrix` show the confusion table for correctly and incorrectly identified classification prediction results. The function `plot_class_acc shows` the visual accuracies of classification. The function `plot_feature_importance` shows the feature importance of classification values. The function `print_accuracy` shows the accuracy scores of the classification models. The function `get_dataset_subset` obtains a subset of the full dataset for modeling and prediction.\n",
    "\n",
    "We will be using a seed of 0. Due to our dataset being extremely large, we are using 5 folds for the CPU usage and runtime to be more managable to run through the prediction models for both classification and regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def output_variables_table(variables):\n",
    "    variables = variables.sort_index()\n",
    "    rows = ['<tr><th>Variable</th><th>Type</th><th>Scale</th><th>Description</th></tr>']\n",
    "    for vname, atts in variables.iterrows():\n",
    "        if vname not in dataset.columns:\n",
    "            continue\n",
    "        atts = atts.to_dict()\n",
    "        # add scale if TBD\n",
    "        if atts['scale'] == 'TBD':\n",
    "            if atts['type'] in ['nominal', 'ordinal']:\n",
    "                uniques = dataset[vname].unique()\n",
    "                uniques = list(uniques.astype(str))\n",
    "                if len(uniques) < 10:\n",
    "                    atts['scale'] = '[%s]' % ', '.join(uniques)\n",
    "                else:\n",
    "                    atts['scale'] = '[%s]' % (', '.join(uniques[:5]) + ', ... (%d More)' % len(uniques))\n",
    "            if atts['type'] in ['ratio', 'interval']:\n",
    "                atts['scale'] = '(%d, %d)' % (dataset[vname].min(), dataset[vname].max())\n",
    "        row = (vname, atts['type'], atts['scale'], atts['description'])\n",
    "        rows.append('<tr><td>%s</td><td>%s</td><td>%s</td><td>%s</td></tr>' % row)\n",
    "    return HTML('<table>%s</table>' % ''.join(rows))\n",
    "\n",
    "\n",
    "# Define an accuracy plot\n",
    "def per_class_accuracy(ytrue, yhat):\n",
    "    conf = mt.confusion_matrix(ytrue,yhat)\n",
    "    norm_conf = conf.astype('float') / conf.sum(axis=1)[:, np.newaxis]\n",
    "    return np.diag(norm_conf)\n",
    "\n",
    "def plot_class_acc(ytrue, yhat, classes, title=''):\n",
    "    acc_list = per_class_accuracy(y, yhat)\n",
    "    pd.DataFrame(acc_list, index=pd.Index(classes, name='Classes')).plot(kind='bar')\n",
    "    plt.xlabel('Class value (one per face)')\n",
    "    plt.ylabel('Accuracy within class')\n",
    "    plt.title(title+\", Total Acc=%.1f\"%(100*mt.accuracy_score(ytrue,yhat)))\n",
    "    plt.grid()\n",
    "    plt.ylim([0,1])\n",
    "    plt.show()\n",
    "    \n",
    "# Plot the feature importances of the forest\n",
    "def plot_feature_importance(ytrue, yhat, rt, title=''):\n",
    "    importances = rt.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in rt.estimators_],\n",
    "             axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    for f in range(X.shape[1]):\n",
    "        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "    plt.figure()\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "    plt.xticks(range(X.shape[1]), indices)\n",
    "    plt.xlim([-1, X.shape[1]])\n",
    "    plt.show()\n",
    "    \n",
    "def print_accuracy(model_name, y_test, yhat, scores):\n",
    "    scores = np.array(scores)\n",
    "    \n",
    "    print('----------------- %s Evaluation -----------------' % model_name)\n",
    "    print(\" F1 Score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    print(' Accuracy', mt.accuracy_score(y_test, yhat))\n",
    "    print(' Precision', mt.precision_score(y_test, yhat, average='weighted'))\n",
    "    print(' Recall', mt.recall_score(y_test, yhat, average='weighted'))\n",
    "    \n",
    "def confusion_matrix(ytrue, yhat, classes):\n",
    "    index = pd.MultiIndex.from_product([['True Class'], classes])\n",
    "    columns = pd.MultiIndex.from_product([['Predicted Class'], classes])\n",
    "    return pd.DataFrame(mt.confusion_matrix(y, yhat), index=index, columns=columns)\n",
    "\n",
    "def roc_curve(ytrue, yhat, clf):\n",
    "    for i, label in enumerate(clf.classes_):\n",
    "        fpr, tpr, _ = mt.roc_curve(y, yhat_score[:, i], pos_label=label)\n",
    "        roc_auc = mt.auc(fpr, tpr)\n",
    "\n",
    "        plt.plot(fpr, tpr, label='class {0} with {1} instances (area = {2:0.2f})'\n",
    "                                       ''.format(label, sum(y==label), roc_auc))\n",
    "\n",
    "        plt.title('ROC Curve')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def get_dataset_subset(dataset, n=1000):\n",
    "    return {\n",
    "        'X': dataset['X'].iloc[:n],\n",
    "        'y': dataset['y'].iloc[:n]\n",
    "    }\n",
    "\n",
    "\n",
    "seed = 0\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Define\"></a>\n",
    "# Define and Prepare Class Variables\n",
    "<b>10 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis.\n",
    "</i>\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"define_c\"></a>\n",
    "## Classification Datasets:\n",
    "\n",
    "The classification dataset removes `logerror` and `transactiondate` because they were for the purposes of the Kaggle competition and were not complete for the training set. The column that was created for \"New Features\" from Lab 1 (`city` and `pricepersqft`) were also removed for the sake of simplicity of only using original data for the prediction process. The table generated shows the type of data used for classification purposes.\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (116761, 2105)\n",
      "regionidcounty\n",
      "1286    35417\n",
      "2061    10261\n",
      "3101    71083\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>Variable</th><th>Type</th><th>Scale</th><th>Description</th></tr><tr><td>airconditioningtypeid</td><td>nominal</td><td>[0, 1, 5, 13, 11, 9, 3]</td><td>Type of cooling system present in the home (if any)</td></tr><tr><td>assessmentyear</td><td>interval</td><td>(2015, 2015)</td><td>The year of the property tax assessment</td></tr><tr><td>bathroomcnt</td><td>ordinal</td><td>[2.5, 1.0, 2.0, 1.5, 3.0, ... (23 More)]</td><td>Number of bathrooms in home including fractional bathrooms</td></tr><tr><td>bedroomcnt</td><td>ordinal</td><td>[3, 2, 4, 5, 6, ... (17 More)]</td><td>Number of bedrooms in home</td></tr><tr><td>buildingqualitytypeid</td><td>ordinal</td><td>[7, 4, 10, 1, 8, 12, 6, 11]</td><td>Overall assessment of condition of the building from best (lowest) to worst (highest)</td></tr><tr><td>calculatedbathnbr</td><td>ordinal</td><td>[2.5, 1.0, 2.0, 1.5, 3.0, ... (23 More)]</td><td>Number of bathrooms in home including fractional bathroom</td></tr><tr><td>calculatedfinishedsquarefeet</td><td>ratio</td><td>(0, 10925)</td><td>Calculated total finished living area of the home</td></tr><tr><td>censustractandblock</td><td>nominal</td><td>[61110023618600.0, 61110015230000.0, 61110006841300.0, 61110052978700.0, 61110040395800.0, ... (445 More)]</td><td>Census tract and block ID combined - also contains blockgroup assignment by extension</td></tr><tr><td>finishedsquarefeet12</td><td>ratio</td><td>(0, 6615)</td><td>Finished living area</td></tr><tr><td>finishedsquarefeet50</td><td>ratio</td><td>(0, 8352)</td><td>Size of the finished living area on the first (entry) floor of the home</td></tr><tr><td>fips</td><td>nominal</td><td>[6111, 6037, 6059]</td><td>Federal Information Processing Standard code - see https://en.wikipedia.org/wiki/FIPS_county_code for more details</td></tr><tr><td>fireplacecnt</td><td>ordinal</td><td>[0, 1, 2, 3, 4, 5]</td><td>Number of fireplaces in a home (if any)</td></tr><tr><td>fullbathcnt</td><td>ordinal</td><td>[2.0, 1.0, 3.0, 7.0, 5.0, ... (19 More)]</td><td>Number of full bathrooms (sink, shower + bathtub, and toilet) present in home</td></tr><tr><td>garagecarcnt</td><td>ordinal</td><td>[2.0, 1.0, 3.0, 0.0, 4.0, ... (15 More)]</td><td>Total number of garages on the lot including an attached garage</td></tr><tr><td>garagetotalsqft</td><td>ratio</td><td>(0, 1610)</td><td>Total number of square feet of all garages on lot including an attached garage</td></tr><tr><td>hashottuborspa</td><td>ordinal</td><td>[0, 1]</td><td>Does the home have a hot tub or spa</td></tr><tr><td>heatingorsystemtypeid</td><td>nominal</td><td>[0, 2, 7, 6, 24, ... (13 More)]</td><td>Type of home heating system</td></tr><tr><td>landtaxvaluedollarcnt</td><td>ratio</td><td>(22, 2477536)</td><td>The assessed value of the land area of the parcel</td></tr><tr><td>location_type</td><td>nominal</td><td>[nan, PRIMARY, ACCEPTABLE, NOT ACCEPTABLE]</td><td>Primary, Acceptable, Not Acceptable</td></tr><tr><td>lotsizesquarefeet</td><td>ratio</td><td>(0, 1710750)</td><td>Area of the lot in square feet</td></tr><tr><td>numberofstories</td><td>ordinal</td><td>[2, 1, 3, 4]</td><td>Number of stories or levels the home has</td></tr><tr><td>parcelid</td><td>nominal</td><td>[17073783, 17088994, 17100444, 17102429, 17109604, ... (90150 More)]</td><td>Unique identifier for parcels (lots)</td></tr><tr><td>poolcnt</td><td>ordinal</td><td>[0.0, 1.0]</td><td>Number of pools on the lot (if any)</td></tr><tr><td>poolsizesum</td><td>ratio</td><td>(0, 1476)</td><td>Total square footage of all pools on property</td></tr><tr><td>pooltypeid10</td><td>nominal</td><td>[0, 1]</td><td>Spa or Hot Tub</td></tr><tr><td>pooltypeid2</td><td>nominal</td><td>[0, 1]</td><td>Pool with Spa/Hot Tub</td></tr><tr><td>pooltypeid7</td><td>nominal</td><td>[0, 1]</td><td>Pool without hot tub</td></tr><tr><td>propertycountylandusecode</td><td>nominal</td><td>[1128, 1129, 1111, 1110, 010C, ... (77 More)]</td><td>County land use code i.e. it's zoning at the county level</td></tr><tr><td>propertylandusetypeid</td><td>nominal</td><td>[265, 266, 261, 269, 246, ... (14 More)]</td><td>Type of land use the property is zoned for</td></tr><tr><td>propertyzoningdesc</td><td>nominal</td><td>[0, LARD3, LARS, LARE11, LCA21*, ... (1997 More)]</td><td>Description of the allowed land uses (zoning) for that property</td></tr><tr><td>roomcnt</td><td>ordinal</td><td>[5, 4, 8, 6, 9, ... (16 More)]</td><td>Total number of rooms in the principal residence</td></tr><tr><td>structuretaxvaluedollarcnt</td><td>ratio</td><td>(100, 2181198)</td><td>The assessed value of the built structure on the parcel</td></tr><tr><td>taxamount</td><td>ratio</td><td>(49, 51292)</td><td>The total property tax assessed for that assessment year</td></tr><tr><td>taxdelinquencyflag</td><td>nominal</td><td>[0, 1]</td><td>Property taxes for this parcel are past due as of 2015</td></tr><tr><td>taxdelinquencyyear</td><td>interval</td><td>(0, 26)</td><td>Year</td></tr><tr><td>taxvaluedollarcnt</td><td>ratio</td><td>(22, 4052186)</td><td>The total tax assessed value of the parcel</td></tr><tr><td>threequarterbathnbr</td><td>ordinal</td><td>[1, 0, 2, 4, 3]</td><td>Number of 3/4 bathrooms in house (shower + sink + toilet)</td></tr><tr><td>unitcnt</td><td>ordinal</td><td>[1, 2, 4, 3, 9, 13, 5, 6, 11]</td><td>Number of units the structure is built into (i.e. 2 = duplex, 3 = triplex, etc...)</td></tr><tr><td>yardbuildingsqft17</td><td>interval</td><td>(0, 1485)</td><td>Patio in yard</td></tr><tr><td>yardbuildingsqft26</td><td>interval</td><td>(0, 1366)</td><td>Storage shed/building in yard</td></tr><tr><td>yearbuilt</td><td>interval</td><td>(1885, 2015)</td><td>The Year the principal residence was built</td></tr><tr><td>zipcode_type</td><td>nominal</td><td>[nan, STANDARD, PO BOX, MILITARY, UNIQUE]</td><td>Standard, PO BOX Only, Unique, Military(implies APO or FPO)</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables = pd.read_csv('../../datasets/variables.csv').set_index('name')\n",
    "dataset = pd.read_csv('../../datasets/train.csv', low_memory=False)\n",
    "\n",
    "# remove unneeded variables\n",
    "del dataset['logerror']\n",
    "del dataset['transactiondate']\n",
    "del dataset['city']\n",
    "del dataset['price_per_sqft']\n",
    "\n",
    "\n",
    "# delete all location information because we want to predict the couty\n",
    "# and those feature will give it up to easy\n",
    "y = dataset['regionidcounty'].copy()\n",
    "del dataset['regionidcounty']\n",
    "del dataset['regionidcity']\n",
    "del dataset['regionidzip']\n",
    "del dataset['regionidneighborhood']\n",
    "del dataset['rawcensustractandblock']\n",
    "del dataset['latitude']\n",
    "del dataset['longitude']\n",
    "\n",
    "output_variables = output_variables_table(variables)\n",
    "\n",
    "nominal = variables[variables['type'].isin(['nominal'])]\n",
    "nominal = nominal[nominal.index.isin(dataset.columns)]\n",
    "continuous = variables[~variables['type'].isin(['nominal'])]\n",
    "continuous = continuous[continuous.index.isin(dataset.columns)]\n",
    "\n",
    "nominal_data = dataset[nominal.index]\n",
    "nominal_data = pd.get_dummies(nominal_data, drop_first=True)\n",
    "nominal_data = nominal_data[nominal_data.columns[~nominal_data.columns.isin(nominal.index)]]\n",
    "\n",
    "continuous_data = dataset[continuous.index]\n",
    "\n",
    "dataset = pd.concat([continuous_data, nominal_data], axis=1)\n",
    "\n",
    "columns = dataset.columns\n",
    "variables = variables[variables.index.isin(dataset.columns)]\n",
    "\n",
    "# shuffle the dataset (just in case)\n",
    "dataset = dataset.sample(frac=1, random_state=seed)\n",
    "\n",
    "X = dataset\n",
    "dataset_class = {\n",
    "    'X': X,\n",
    "    'y': y\n",
    "}\n",
    "print('Dataset shape:', X.shape)\n",
    "print(y.groupby(y).size())\n",
    "output_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"define_r\"></a>\n",
    "## Regression Datasets:\n",
    "\n",
    "The regression dataset removes `logerror` and `transactiondate` because they were for the purposes of the Kaggle competition and were not complete for the training set. The column that was created for \"New Features\" from Lab 1 (`city` and `pricepersqft`) were also removed for the sake of simplicity of only using original data for the prediction process. We are only using nominal and continuous data types for regression purposes.\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (116761, 2106)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>Variable</th><th>Type</th><th>Scale</th><th>Description</th></tr><tr><td>airconditioningtypeid</td><td>nominal</td><td>[0, 1, 5, 13, 11, 9, 3]</td><td>Type of cooling system present in the home (if any)</td></tr><tr><td>assessmentyear</td><td>interval</td><td>(2015, 2015)</td><td>The year of the property tax assessment</td></tr><tr><td>bathroomcnt</td><td>ordinal</td><td>[2.5, 1.0, 2.0, 1.5, 3.0, ... (23 More)]</td><td>Number of bathrooms in home including fractional bathrooms</td></tr><tr><td>bedroomcnt</td><td>ordinal</td><td>[3, 2, 4, 5, 6, ... (17 More)]</td><td>Number of bedrooms in home</td></tr><tr><td>buildingqualitytypeid</td><td>ordinal</td><td>[7, 4, 10, 1, 8, 12, 6, 11]</td><td>Overall assessment of condition of the building from best (lowest) to worst (highest)</td></tr><tr><td>calculatedbathnbr</td><td>ordinal</td><td>[2.5, 1.0, 2.0, 1.5, 3.0, ... (23 More)]</td><td>Number of bathrooms in home including fractional bathroom</td></tr><tr><td>calculatedfinishedsquarefeet</td><td>ratio</td><td>(0, 10925)</td><td>Calculated total finished living area of the home</td></tr><tr><td>censustractandblock</td><td>nominal</td><td>[61110023618600.0, 61110015230000.0, 61110006841300.0, 61110052978700.0, 61110040395800.0, ... (445 More)]</td><td>Census tract and block ID combined - also contains blockgroup assignment by extension</td></tr><tr><td>finishedsquarefeet12</td><td>ratio</td><td>(0, 6615)</td><td>Finished living area</td></tr><tr><td>finishedsquarefeet50</td><td>ratio</td><td>(0, 8352)</td><td>Size of the finished living area on the first (entry) floor of the home</td></tr><tr><td>fips</td><td>nominal</td><td>[6111, 6037, 6059]</td><td>Federal Information Processing Standard code - see https://en.wikipedia.org/wiki/FIPS_county_code for more details</td></tr><tr><td>fireplacecnt</td><td>ordinal</td><td>[0, 1, 2, 3, 4, 5]</td><td>Number of fireplaces in a home (if any)</td></tr><tr><td>fullbathcnt</td><td>ordinal</td><td>[2.0, 1.0, 3.0, 7.0, 5.0, ... (19 More)]</td><td>Number of full bathrooms (sink, shower + bathtub, and toilet) present in home</td></tr><tr><td>garagecarcnt</td><td>ordinal</td><td>[2.0, 1.0, 3.0, 0.0, 4.0, ... (15 More)]</td><td>Total number of garages on the lot including an attached garage</td></tr><tr><td>garagetotalsqft</td><td>ratio</td><td>(0, 1610)</td><td>Total number of square feet of all garages on lot including an attached garage</td></tr><tr><td>hashottuborspa</td><td>ordinal</td><td>[0, 1]</td><td>Does the home have a hot tub or spa</td></tr><tr><td>heatingorsystemtypeid</td><td>nominal</td><td>[0, 2, 7, 6, 24, ... (13 More)]</td><td>Type of home heating system</td></tr><tr><td>landtaxvaluedollarcnt</td><td>ratio</td><td>(22, 2477536)</td><td>The assessed value of the land area of the parcel</td></tr><tr><td>latitude</td><td>interval</td><td>(33339296, 34816008)</td><td>Latitude of the middle of the parcel multiplied by 10e6</td></tr><tr><td>location_type</td><td>nominal</td><td>[nan, PRIMARY, ACCEPTABLE, NOT ACCEPTABLE]</td><td>Primary, Acceptable, Not Acceptable</td></tr><tr><td>longitude</td><td>interval</td><td>(-119447864, -117554928)</td><td>Longitude of the middle of the parcel multiplied by 10e6</td></tr><tr><td>lotsizesquarefeet</td><td>ratio</td><td>(0, 1710750)</td><td>Area of the lot in square feet</td></tr><tr><td>numberofstories</td><td>ordinal</td><td>[2, 1, 3, 4]</td><td>Number of stories or levels the home has</td></tr><tr><td>parcelid</td><td>nominal</td><td>[17073783, 17088994, 17100444, 17102429, 17109604, ... (90150 More)]</td><td>Unique identifier for parcels (lots)</td></tr><tr><td>poolcnt</td><td>ordinal</td><td>[0.0, 1.0]</td><td>Number of pools on the lot (if any)</td></tr><tr><td>poolsizesum</td><td>ratio</td><td>(0, 1476)</td><td>Total square footage of all pools on property</td></tr><tr><td>pooltypeid10</td><td>nominal</td><td>[0, 1]</td><td>Spa or Hot Tub</td></tr><tr><td>pooltypeid2</td><td>nominal</td><td>[0, 1]</td><td>Pool with Spa/Hot Tub</td></tr><tr><td>pooltypeid7</td><td>nominal</td><td>[0, 1]</td><td>Pool without hot tub</td></tr><tr><td>propertycountylandusecode</td><td>nominal</td><td>[1128, 1129, 1111, 1110, 010C, ... (77 More)]</td><td>County land use code i.e. it's zoning at the county level</td></tr><tr><td>propertylandusetypeid</td><td>nominal</td><td>[265, 266, 261, 269, 246, ... (14 More)]</td><td>Type of land use the property is zoned for</td></tr><tr><td>propertyzoningdesc</td><td>nominal</td><td>[0, LARD3, LARS, LARE11, LCA21*, ... (1997 More)]</td><td>Description of the allowed land uses (zoning) for that property</td></tr><tr><td>rawcensustractandblock</td><td>nominal</td><td>[61110022, 61110015, 61110007, 61110008, 61110014, ... (1418 More)]</td><td>Census tract and block ID combined - also contains blockgroup assignment by extension</td></tr><tr><td>regionidcity</td><td>nominal</td><td>[34543, 26965, 51239, 13150, 25974, ... (178 More)]</td><td>City in which the property is located (if any)</td></tr><tr><td>regionidcounty</td><td>nominal</td><td>[2061, 3101, 1286]</td><td>County in which the property is located</td></tr><tr><td>regionidneighborhood</td><td>nominal</td><td>[0, 48570, 268588, 40548, 27987, ... (495 More)]</td><td>Neighborhood in which the property is located</td></tr><tr><td>regionidzip</td><td>nominal</td><td>[97081, 97083, 97113, 97084, 97089, ... (389 More)]</td><td>Zip code in which the property is located</td></tr><tr><td>roomcnt</td><td>ordinal</td><td>[5, 4, 8, 6, 9, ... (16 More)]</td><td>Total number of rooms in the principal residence</td></tr><tr><td>structuretaxvaluedollarcnt</td><td>ratio</td><td>(100, 2181198)</td><td>The assessed value of the built structure on the parcel</td></tr><tr><td>taxamount</td><td>ratio</td><td>(49, 51292)</td><td>The total property tax assessed for that assessment year</td></tr><tr><td>taxdelinquencyflag</td><td>nominal</td><td>[0, 1]</td><td>Property taxes for this parcel are past due as of 2015</td></tr><tr><td>taxdelinquencyyear</td><td>interval</td><td>(0, 26)</td><td>Year</td></tr><tr><td>taxvaluedollarcnt</td><td>ratio</td><td>(22, 4052186)</td><td>The total tax assessed value of the parcel</td></tr><tr><td>threequarterbathnbr</td><td>ordinal</td><td>[1, 0, 2, 4, 3]</td><td>Number of 3/4 bathrooms in house (shower + sink + toilet)</td></tr><tr><td>unitcnt</td><td>ordinal</td><td>[1, 2, 4, 3, 9, 13, 5, 6, 11]</td><td>Number of units the structure is built into (i.e. 2 = duplex, 3 = triplex, etc...)</td></tr><tr><td>yardbuildingsqft17</td><td>interval</td><td>(0, 1485)</td><td>Patio in yard</td></tr><tr><td>yardbuildingsqft26</td><td>interval</td><td>(0, 1366)</td><td>Storage shed/building in yard</td></tr><tr><td>yearbuilt</td><td>interval</td><td>(1885, 2015)</td><td>The Year the principal residence was built</td></tr><tr><td>zipcode_type</td><td>nominal</td><td>[nan, STANDARD, PO BOX, MILITARY, UNIQUE]</td><td>Standard, PO BOX Only, Unique, Military(implies APO or FPO)</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFJCAYAAACyzKU+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAQAwhngDAGDM/wDUndGefmYU3wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb3eea58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "variables = pd.read_csv('../../datasets/variables.csv').set_index('name')\n",
    "dataset = pd.read_csv('../../datasets/train.csv', low_memory=False)\n",
    "\n",
    "# remove unneeded variables\n",
    "del dataset['logerror']\n",
    "del dataset['transactiondate']\n",
    "del dataset['city']\n",
    "del dataset['price_per_sqft']\n",
    "\n",
    "output_variables = output_variables_table(variables)\n",
    "\n",
    "nominal = variables[variables['type'].isin(['nominal'])]\n",
    "nominal = nominal[nominal.index.isin(dataset.columns)]\n",
    "continuous = variables[~variables['type'].isin(['nominal'])]\n",
    "continuous = continuous[continuous.index.isin(dataset.columns)]\n",
    "\n",
    "nominal_data = dataset[nominal.index]\n",
    "nominal_data = pd.get_dummies(nominal_data, drop_first=True)\n",
    "nominal_data = nominal_data[nominal_data.columns[~nominal_data.columns.isin(nominal.index)]]\n",
    "\n",
    "continuous_data = dataset[continuous.index]\n",
    "\n",
    "dataset = pd.concat([continuous_data, nominal_data], axis=1)\n",
    "\n",
    "columns = dataset.columns\n",
    "variables = variables[variables.index.isin(dataset.columns)]\n",
    "\n",
    "# shuffle the dataset (just in case)\n",
    "dataset = dataset.sample(frac=1, random_state=seed)\n",
    "\n",
    "X = dataset\n",
    "y = X['taxamount'].copy()\n",
    "del X['taxamount']\n",
    "del X['price_per_sqft']\n",
    "\n",
    "dataset_reg = {\n",
    "    'X': X,\n",
    "    'y': y\n",
    "}\n",
    "print('Dataset shape:', X.shape)\n",
    "y.plot(kind='box')\n",
    "output_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Describe\"></a>\n",
    "# Describe the Final Dataset\n",
    "<b>5 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).\n",
    "</i>\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"describe_c\"></a>\n",
    "## Classification Datasets:\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using the same Zillow dataset that we used in the previous lab, most of the data was already cleaned up.  However, the purpose of our classification dataset is to predict the county each property is located in. Therefore our final model removed all columns relating to location such as `latitude`, `longitude`, `city`, and `zipcode`.  We also removed variables we did not need such as `logerror`, `transactiondate`, `and price_per_sqft`.\n",
    "\n",
    "We did not create any new columns for the classification dataset but we did categorical variable into indicator variables.\n",
    "\n",
    "The final shape of our classification dataset is 116761 instances and 2105 columns.  The three counties we are trying to predict have sizes of about 35k, 10k, and 71k so an accuracy below 0.61 will mean that we are better off classifying each with the latter county."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"describe_r\"></a>\n",
    "## Regression Datasets:\n",
    "\n",
    "The regression dataset removes logerror and transactiondate because they were for the purposes of the Kaggle competition and were not complete for the training set. The column that was created for \"New Features\" from Lab 1 (`city` and `price_per_sqft`) were also removed for the sake of simplicity of only using original data for the prediction process.\n",
    "\n",
    "We are only using nominal and continuous data types for regression purposes. The final shape of our classification dataset is 116761 instances and 2106 columns. The varaiable that we are predicting, `taxamount`, is right skewed, with outlier property costing more than the standard deviation.\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Evaluation\"></a>\n",
    "# Explain Evaluation Metrics\n",
    "<b>10 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "Choose and explain your evaluation metrics that you will use (i.e., accuracy, precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up any assertions.\n",
    "</i>\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"metrcs_c\"></a>\n",
    "## Classification Metrics:\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>\n",
    "\n",
    "For our evaluation, we will be taking into account the accuracy and F-measure.  In order to compute the F-measure, we will need the precision and recall.  Because F-measure is a weighted average of these, we think a better F-measure score means the model has a better precision and recall.\n",
    "\n",
    "Accuracy is the ratio of correct predictions to the total number of observations. It is calculated as: (TP+TN) / (TP+FP+FN+TN). The closer accuracy is to 1, the more accurate the model is, with one caveat. For high accuracy to be a reliable indicator, the dataset has to be symmetric, i.e. total false positives are about equal to false negatives. Otherwise, we need to review other parameters as well.\n",
    "\n",
    "Precision is the ratio of correctly predicted positive observations to the total positive observations. It is calculated as: TP / (TP+FP).  \n",
    "\n",
    "Recall is the ratio of correctly predicted positive observations to all actual positives. It is calculated as TP / (TP+FN). The consequences of type 2 errors, predicting a false negative, are not extreme so we think recall is an appropriate measure of completeness.\n",
    "\n",
    "Finally, we will also use F-measure which is essentially a weighted average of the precision and recall into one simple statistic. F-measure This will be a number between 0 and 1 where closer to 1 is better and approaching 0 is worse. It overcomes the limitations of accuracy whenever false positives and false negatives are not about equal or symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"metrcs_r\"></a>\n",
    "## Regression Metrics:\n",
    "\n",
    "For our evaluation of regression prediction models, we are looking at mean squared error (MSE) and R^2. With the large data size and right skew of taxamount, we are trying to minimize MSE and have a R^2 value close to 1. Whichever model with optimal parameters that could reduce MSE and increase R^2 while using less CPU and less runtime would be the best regression model to use for the prediction of the dataset.\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Splits\"></a>\n",
    "# Training and Testing Splits\n",
    "<b>10 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "Choose the method you will use for dividing your data into training and testing splits (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time.\n",
    "</i>\n",
    "\n",
    "Due to our dataset being extremely large, we are using 5 folds for the CPU usage and runtime to be more managable to run through the prediction models for both classification and regression.\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chart of train size while accuracy continues to increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-42ddd305a8fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_frac\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_frac\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    324\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 326\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'balanced'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    740\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    348\u001b[0m                                            self.min_impurity_split)\n\u001b[1;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "X = dataset_class['X']\n",
    "y = dataset_class['y']\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=.1, random_state=0)\n",
    "\n",
    "result = []\n",
    "\n",
    "# uniform distribution values between 0 and 1\n",
    "np.random.seed(seed)\n",
    "mask = np.random.rand(len(X))\n",
    "\n",
    "for frac in np.linspace(.01, 1, 20):\n",
    "    mask_frac = mask<=frac\n",
    "    X_frac = X[mask_frac]\n",
    "    y_frac = y[mask_frac]\n",
    "\n",
    "    clf = RandomForestClassifier(max_depth=200, n_estimators=40, random_state=seed)\n",
    "    clf.fit(X_frac, y_frac)\n",
    "    y_hat = clf.predict(X_test)\n",
    "    \n",
    "    result.append({\n",
    "        'f1_score': mt.f1_score(y_test, y_hat, average='weighted'),\n",
    "        'count': len(X_frac),\n",
    "        'frac': frac\n",
    "    })\n",
    "\n",
    "pd.DataFrame(result).plot('count', 'f1_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"splits_c\"></a>\n",
    "## Classification Splits:\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>\n",
    "\n",
    "For the classification task we choose to use Stratified K-Fold cross validation with 5 folds.  We chose stratified in order to preserve the percentage of samples in each class.  We also had a very large dataset so splitting into more than 5 folds would have been computationally expensive with not a large enough return on value.  We felt that splitting the data into 5 folds would be enough splits to reduce the weight of any outliers or noise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"splits_r\"></a>\n",
    "## Regression Splits:\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>\n",
    "\n",
    "For the regression task we choose to use K-Fold cross validation with 5 folds. We chose K-Fold in order to preserve the percentage of samples in each class. We also had a very large dataset so splitting into more than 5 folds would have been computationally expensive with not a large enough return on value. We felt that splitting the data into 5 folds would be enough splits to reduce the weight of any outliers or noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Models\"></a>\n",
    "# Three Different Classification/Regression Models\n",
    "<b>20 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!\n",
    "</i>\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Classification_m\"></a>\n",
    "## Classification Models:\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"KNN_c_m\"></a>\n",
    "### Definition and optimization of K Nearest Neighbors (KD Tree) \n",
    "\n",
    "K Nearest Neighbors is for the prediction of values based on training their nearest neighbors by a certain n_neighbors count in order to form classification models to predict the y_hat for the test set.\n",
    "\n",
    "# TODO: fill in once final optimal parameters are known\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA, SparsePCA\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "\n",
    "X = dataset_class['X']\n",
    "y = dataset_class['y']\n",
    "\n",
    "result = []\n",
    "scores = []\n",
    "\n",
    "for n_neighbors in range(2, 30)[::5]:\n",
    "    yhat = np.zeros(y.shape) # we will fill this with predictions\n",
    "    cv = StratifiedKFold(n_splits=n_splits, random_state=seed)\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # in order to reduce the time for training KNeighborsClassifier\n",
    "        # we reduce the dimetions of the data from 1717 to 100 and we use kd_tree algorithm\n",
    "        pca = PCA(n_components=100, random_state=seed)\n",
    "        pca.fit(X_train)\n",
    "        X_train = pca.transform(X_train)\n",
    "        X_test = pca.transform(X_test)\n",
    "\n",
    "        clf = KNeighborsClassifier(n_neighbors=n_neighbors, algorithm='kd_tree', weights='distance')\n",
    "        clf.fit(X_train, y_train)\n",
    "        yhat[test_index] = clf.predict(X_test)\n",
    "\n",
    "    f1_score = mt.f1_score(y, yhat, average='weighted')\n",
    "    print ('n_neighbors:', n_neighbors, ', f1_score:', f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"RandomForest_c_m\"></a>\n",
    "### Definition and optimization of Random Forest\n",
    "\n",
    "Random forest is for the prediction of values based on training decision trees by by a certain max depth in order to form classification models to predict the y_hat for the test set.\n",
    "\n",
    "# TODO: fill in once final optimal parameters are known\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = dataset_class['X']\n",
    "y = dataset_class['y']\n",
    "\n",
    "result = []\n",
    "index = []\n",
    "for max_depth in range(1, 401)[::50]:\n",
    "    yhat = np.zeros(y.shape, dtype=int) # we will fill this with predictions\n",
    "    cv = StratifiedKFold(n_splits=n_splits, random_state=seed)\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        clf = RandomForestClassifier(max_depth=max_depth, random_state=seed, n_estimators=40)\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        clf.fit(X_train, y_train)\n",
    "        yhat[test_index] = clf.predict(X_test)\n",
    "\n",
    "    f1_score = mt.f1_score(y, yhat, average='weighted')\n",
    "    print ('max_depth:', max_depth, 'F1 score:', f1_score)\n",
    "    result.append(f1_score)\n",
    "    index.append(max_depth)\n",
    "\n",
    "plt.title('F1 score for different max_depth')\n",
    "pd.Series(result, index=pd.Index(index, name='max_depth'), name='f1_score').plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"NaiveBayes_c_m\"></a>\n",
    "### Definition and optimization of  Naive Bayes (Gaussian)\n",
    "\n",
    "The Naive Bayes doesn't have any paramethers to optimize and uses maximum likelihood training to classify and predict for the test set.\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "X = dataset_class['X']\n",
    "y = dataset_class['y']\n",
    "\n",
    "\n",
    "yhat = np.zeros(y.shape) # we will fill this with predictions\n",
    "cv = StratifiedKFold(n_splits=n_splits, random_state=seed)\n",
    "scores = []\n",
    "\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "    clf = GaussianNB()\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    yhat[test_index] = clf.predict(X_test)\n",
    "    \n",
    "    f1_score = mt.f1_score(y_test, clf.predict(X_test), average='weighted')\n",
    "    scores.append(f1_score)\n",
    "\n",
    "scores = np.array(scores)\n",
    "print(\"F1: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "print_accuracy('MultinomialNB', y, yhat)\n",
    "confusion_matrix(y, yhat, clf.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Classification_m\"></a>\n",
    "## Definition and optimization of  Regression models:\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"KNN_r_m\"></a>\n",
    "## Definition and optimization of K Nearest Neighbors\n",
    "\n",
    "K Nearest Neighbors is for the prediction of values based on training their nearest neighbors by a certain n_neighbors and leaf_size count in order to form regression models to predict the y_hat for the test set.\n",
    "\n",
    "# TODO: fill in once final optimal parameters are known\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "yhat = np.zeros(y.shape)\n",
    "cv = KFold(n_splits=10, random_state=0)\n",
    "n_neighbors_result = []\n",
    "leaf_size_result = []\n",
    "\n",
    "for n_neighbors in [1,2,3,4,5,6]:\n",
    "    for leaf_size in [50,100,200,300]:\n",
    "        for train_index, test_index in cv.split(X, y):\n",
    "            clf = KNeighborsRegressor(n_neighbors=n_neighbors)\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            clf.fit(X_train, y_train)\n",
    "            yhat[test_index] = clf.predict(X_test)\n",
    "        print(\"n_neighbors: %.f, leaf_size: %.f, MSE: %.f, R^2: %0.3f\" % (n_neighbors, leaf_size, mean_squared_error(y, yhat), r2_score(y, yhat)))\n",
    "        n_neighbors_result.append((n_neighbors, r2_score(y, yhat)))\n",
    "        leaf_size_result.append((leaf_size, r2_score(y, yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"RandomForest_r_m\"></a>\n",
    "## Definition and optimization of  Random Forest\n",
    "\n",
    "Random forest is for the prediction of values based on training decision trees by by a certain max depth and n_estimators in order to form regression models to predict the y_hat for the test set.\n",
    "\n",
    "# TODO: fill in once final optimal parameters are known\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X = dataset_reg['X']\n",
    "y = dataset_reg['y']\n",
    "\n",
    "yhat = np.zeros(y.shape)\n",
    "cv = KFold(n_splits=10, random_state=0)\n",
    "\n",
    "for max_depth in [1,2,3,4]:\n",
    "    for n_estimators in [50, 100, 200, 300]:\n",
    "        for train_index, test_index in cv.split(X, y):\n",
    "            clf = RandomForestRegressor(max_depth=max_depth, n_estimators=n_estimators, random_state=0)\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            clf.fit(X_train, y_train)\n",
    "            yhat[test_index] = clf.predict(X_test)\n",
    "        print(\"max_depth: %.f, n_estimators: %.f, MSE: %.f, R^2: %0.3f\" % (max_depth, n_estimators, mean_squared_error(y, yhat), r2_score(y, yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"GaussianProcess_r_m\"></a>\n",
    "## Definition and optimization of  Gaussian Regression\n",
    "\n",
    "Gaussian Regression is for the prediction of values based on normally distributed variables where alpha could be optimized in order to form regression models to predict the y_hat for the test set.\n",
    "\n",
    "# TODO: fill in once final optimal parameters are known\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rom sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X = dataset_reg['X']\n",
    "y = dataset_reg['y']\n",
    "\n",
    "yhat = np.zeros(y.shape)\n",
    "cv = KFold(n_splits=10, random_state=0)\n",
    "\n",
    "for alpha in np.linspace(1e-15, 0.001, 5):\n",
    "    for train_index, test_index in cv.split(X, y):\n",
    "        clf = GaussianProcessRegressor(normalize_y=True, alpha=alpha, random_state=0)\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        clf.fit(X_train, y_train)\n",
    "        yhat[test_index] = clf.predict(X_test)\n",
    "    print(\"alpha: %f, MSE: %.f, R^2: %0.3f\" % (alpha, mean_squared_error(y, yhat), r2_score(y, yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Analysis\"></a>\n",
    "# Visualizations of Results and Analysis\n",
    "<b>10 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model.\n",
    "</i>\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Classification_a\"></a>\n",
    "## Analysis of Classification model:\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Analysis of a Dummy model\n",
    "This model is only predicting the most frequent class. It is used for a base line to compare other models to. The dummy model of 3101 is better at predicting classifiers than some of the classification methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_accuracy('Dummy', y, [3101] * len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"KNN_c_a\"></a>\n",
    "### Results and Analysis of K Nearest Neighbors (KD Tree)\n",
    "\n",
    "# TODO: fill in once final accuracy values are known\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = dataset_class['X']\n",
    "y = dataset_class['y']\n",
    "\n",
    "scores = []\n",
    "yhat = np.zeros(y.shape)\n",
    "yhat_score = np.zeros((len(y), 3))\n",
    "\n",
    "cv = StratifiedKFold(n_splits=n_splits, random_state=seed)\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "    clf = KNeighborsClassifier(n_neighbors=17, algorithm='kd_tree', weights='distance')\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    pca = PCA(n_components=100, random_state=seed)\n",
    "    pca.fit(X_train)\n",
    "    X_train = pca.transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    yhat[test_index] = clf.predict(X_test)\n",
    "    yhat_score[test_index] = clf.predict_proba(X_test)\n",
    "    \n",
    "    f1_score = mt.f1_score(y_test, clf.predict(X_test), average='weighted')\n",
    "    scores.append(f1_score)\n",
    "\n",
    "print_accuracy('KD Tree Classifier', y, yhat, scores)\n",
    "plot_class_acc(y, yhat, clf.classes_, title=\"KD Tree Classifier\")\n",
    "confusion_matrix(y, yhat, clf.classes_)\n",
    "roc_curve(y, yhat, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"RandomForest_c_a\"></a>\n",
    "## Results and Analysis of Random Forest\n",
    "\n",
    "# TODO: fill in once final accuracy values are known\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = dataset_class['X']\n",
    "y = dataset_class['y']\n",
    "\n",
    "scores = []\n",
    "yhat = np.zeros(y.shape)\n",
    "yhat_score = np.zeros((len(y), 3))\n",
    "\n",
    "cv = StratifiedKFold(n_splits=n_splits, random_state=seed)\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "\n",
    "    \n",
    "    clf = RandomForestClassifier(random_state=seed, max_depth=200, n_estimators=40)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    yhat[test_index] = clf.predict(X_test)\n",
    "    yhat_score[test_index] = clf.predict_proba(X_test)\n",
    "    \n",
    "    f1_score = mt.f1_score(y_test, clf.predict(X_test), average='weighted')\n",
    "    scores.append(f1_score)\n",
    "\n",
    "print_accuracy('KD Tree Classifier', y, yhat, scores)\n",
    "plot_class_acc(y, yhat, clf.classes_, title=\"KD Tree Classifier\")\n",
    "confusion_matrix(y, yhat, clf.classes_)\n",
    "roc_curve(y, yhat, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"NaiveBayes_c_a\"></a>\n",
    "## Results and Analysis Naive Bayes\n",
    "\n",
    "# TODO: fill in once final accuracy values are known\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "X = dataset_class['X']\n",
    "y = dataset_class['y']\n",
    "\n",
    "scores = []\n",
    "yhat = np.zeros(y.shape)\n",
    "yhat_score = np.zeros((len(y), 3))\n",
    "\n",
    "cv = StratifiedKFold(n_splits=n_splits, random_state=seed)\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "\n",
    "    clf = GaussianNB()\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    yhat[test_index] = clf.predict(X_test)\n",
    "    yhat_score[test_index] = clf.predict_proba(X_test)\n",
    "    \n",
    "    f1_score = mt.f1_score(y_test, clf.predict(X_test), average='weighted')\n",
    "    scores.append(f1_score)\n",
    "\n",
    "print_accuracy('KD Tree Classifier', y, yhat, scores)\n",
    "plot_class_acc(y, yhat, clf.classes_, title=\"KD Tree Classifier\")\n",
    "confusion_matrix(y, yhat, clf.classes_)\n",
    "roc_curve(y, yhat, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Regression_a\"></a>\n",
    "## Analysis of Regression models:\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"KNN_r_a\"></a>\n",
    "## Results and Analysis K Nearest Neighbors\n",
    "\n",
    "# TODO: fill in once final accuracy values are known\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = dataset_reg['X']\n",
    "y = dataset_reg['y']\n",
    "\n",
    "mses = []\n",
    "r2s = []\n",
    "yhat = np.zeros(y.shape) # we will fill this with predictions\n",
    "cv = KFold(n_splits=10, random_state=seed)\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "    clf = KNeighborsRegressor(n_neighbors=6, leaf_size=300)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    clf.fit(X_train, y_train)\n",
    "    yhat[test_index] = clf.predict(X_test)\n",
    "\n",
    "    mses.append(mean_squared_error(y_test, clf.predict(X_test)))\n",
    "    r2s.append(r2_score(y_test, clf.predict(X_test)))\n",
    "\n",
    "print('Evaluation metrics:')\n",
    "print(\"MSE: %0.2f (+/- %0.2f)\" % (np.mean(mses), np.std(mses) * 2))\n",
    "print(\"R2: %0.2f (+/- %0.2f)\" % (np.mean(r2s), np.std(r2s) * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"RandomForest_r_a\"></a>\n",
    "## Results and Analysis Random Forest\n",
    "\n",
    "# TODO: fill in once final accuracy values are known\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = dataset_reg['X']\n",
    "y = dataset_reg['y']\n",
    "\n",
    "mses = []\n",
    "r2s = []\n",
    "yhat = np.zeros(y.shape) # we will fill this with predictions\n",
    "cv = KFold(n_splits=10, random_state=seed)\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "    clf = RandomForestRegressor(max_depth=4, n_estimators=200, random_state=0)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    clf.fit(X_train, y_train)\n",
    "    yhat[test_index] = clf.predict(X_test)\n",
    "\n",
    "    mses.append(mean_squared_error(y_test, clf.predict(X_test)))\n",
    "    r2s.append(r2_score(y_test, clf.predict(X_test)))\n",
    "\n",
    "print('Evaluation metrics:')\n",
    "print(\"MSE: %0.2f (+/- %0.2f)\" % (np.mean(mses), np.std(mses) * 2))\n",
    "print(\"R2: %0.2f (+/- %0.2f)\" % (np.mean(r2s), np.std(r2s) * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"GaussianProcess_r_a\"></a>\n",
    "## Results and Analysis Gaussian Regression\n",
    "\n",
    "# TODO: fill in once final accuracy values are known\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = dataset_reg['X']\n",
    "y = dataset_reg['y']\n",
    "\n",
    "mses = []\n",
    "r2s = []\n",
    "yhat = np.zeros(y.shape) # we will fill this with predictions\n",
    "cv = KFold(n_splits=10, random_state=seed)\n",
    "for train_index, test_index in cv.split(X, y):\n",
    "    clf = GaussianProcessRegressor(alpha=1e-15, normalize_y=True, random_state=0)\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    clf.fit(X_train, y_train)\n",
    "    yhat[test_index] = clf.predict(X_test)\n",
    "    \n",
    "    \n",
    "    mses.append(mean_squared_error(y_test, clf.predict(X_test)))\n",
    "    r2s.append(r2_score(y_test, clf.predict(X_test)))\n",
    "\n",
    "print(\"MSE: %0.2f (+/- %0.2f)\" % (np.mean(mses), np.std(mses) * 2))\n",
    "print(\"R2: %0.2f (+/- %0.2f)\" % (np.mean(r2s), np.std(r2s) * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Advantages\"></a>\n",
    "# Advantages of Each Model\n",
    "<b>10 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniques—be sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course.\n",
    "</i>\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"advantages_c\"></a>\n",
    "## Analysis of Classification model:\n",
    "### K Nearest Neighbors- KD tree\n",
    "K nearest neighbors classification is different than other classification models in that it does not attempt to make a model but only stores instances of the training data.  The classification will build a tree which at each node is a rule based on the dimensions of its k nearest neighbors and leading to each leaf representing a class.  The advantages of K nearest neighbors is that it is simple and converges to the correct decision surface as data goes to infinity.  It can be used with multiclass data sets as well as more complex algorithms such as the KD tree. In our dataset we used the KD tree algorithm in order to speed up the KNN classification by indexing the tree.\n",
    "\n",
    "### Why we did PCA with KNN\n",
    "KNN computes the distance between each neighbor's dimensions.  We have so many dimensions in our dataset that even with 100 neighbors the accuracy was still continuing to grow and taking a long time.  We wanted to continue to increase the number of neighbors until the accuracy increase plateaued but without reducing the dimensions using PCA, it would have taken too long.\n",
    "\n",
    "### Random forest\n",
    "Random forest is an ensemble classification algorithm which in nature is a huge advantage because because predicting off of one decision tree vs an ensemble of them the ensemble will usually win.  Another advantage is that the forest can often correct a tree's overfitting of the training set.\n",
    "\n",
    "### Gassian Naive Bayes\n",
    "Naive Bayes largest advantage is that it is extremely simple, it is just counting up probabilities.  When training sets are small, Naive Bayes is good because of its high bias and low variance which will not overfit the training data.  However, as datasets grow larger such as our dataset, the high bias will prevent the model from being powerful enough to have a high accuracy.  Gassian Naive Bayes is a normally distributed NB classifier.  Its advantages are that it is fast and can make probabilistic decisions\n",
    "# TODO find winner model from optimal parameters and repeat accuracy levels\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"advantages_r\"></a>\n",
    "## Analysis of Regression model:\n",
    "### K Nearest Neighbors\n",
    "The advantages of K nearest neighbors is that it is non-parametric and can address missing and unusual data for regression prediction. Dimensionality reduction can be used to speed up the prediction modeling process because the model could be trained with nearest neighbors and leaf size from the results of PCA.\n",
    "\n",
    "### Random forest\n",
    "The advantages of Random forest is that by averaging multiple trees, it reduces overfitting, reduces variance from outliers, and is therefore more accurate. It is unbiased in the estimate of the generalization error for the forest building progress and provides effective methods for estimating missing data. Random forest can extended to unlabeled data, leading to unsupervised clustering.\n",
    "\n",
    "### Gaussian Process Regressor\n",
    "The advantage of Gaussian Process Regressor is that it is fast and uses less CPU and runtime. However, it is more used towards data that have normal distributions. It provides the full probabilistic prediction and interpolates the observations for faster prediction.\n",
    "# TODO find winner model from optimal parameters and repeat accuracy levels\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Attributes\"></a>\n",
    "# Important Attributes\n",
    "<b>10 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task.\n",
    "</i>\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance for classifaction dataset according to Random Forest\n",
    "\n",
    "# TODO fill in once features are known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = dataset_class['X']\n",
    "y = dataset_class['y']\n",
    "\n",
    "clf = RandomForestClassifier(random_state=seed, max_depth = 5)\n",
    "clf.fit(X, y)\n",
    "importances = clf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in clf.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Random Forest Feature ranking:\")\n",
    "for f in range(X.shape[1]):\n",
    "    if importances[indices[f]] > 0.05:\n",
    "        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.title(\"Random Forest Feature importances\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\n",
    "feature_names = X.columns\n",
    "plt.xticks(range(10), feature_names)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlim([-1, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance for regression dataset according to Random Forest\n",
    "\n",
    "# TODO fill in once features are known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestRegressor(max_depth=10, n_estimators=50, random_state=0)\n",
    "clf.fit(X, y)\n",
    "importances = clf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in clf.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Random Forest Feature ranking:\")\n",
    "for f in range(X.shape[1]):\n",
    "    if importances[indices[f]] > 0.05:\n",
    "        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.title(\"Random Forest Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\n",
    "feature_names = X.columns\n",
    "plt.xticks(range(10), feature_names)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlim([-1, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Deployment\"></a>\n",
    "# Deployment\n",
    "<b>5 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "How useful is your model for interested parties (i.e., the companies or organizations that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.?\n",
    "</i>\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zillow began offering publicly available real estate data from disparate sources into a single platform, the gap between sellers’ prices and buyers’ offer prices has significantly decreased.\n",
    "\n",
    "The Zillow dataset was provided for the purpose of evaluating Zestimate’s accuracy based upon the variable `logerror` which is the difference of log(Zestimate) - log(SalePrice). For purposes of this lab assignment, we developed regression models on `taxamount` and classification models on the `regionidcounty` field (i.e. the county identifier). \n",
    "\n",
    "# TODO Insert insights form regression and classification models here\n",
    "_ , _ , and _ are the best indicators for the regression prediction of `taxamount`, where _ ...\n",
    "\n",
    "_ , _ , and _ are the best indicators for classification prediction of `regionidcounty`, where _ ...\n",
    "\n",
    "For companies in the real estate space, classification models based on physical attributes, which provide valuable insight for buying, selling, and investment decisions. Our classification model can be adapted to more granular levels as cities and municipalities. Buyers, sellers, and investors alike can gain insights into which features have the highest importance to specific locations. This may drive investment decisions knowing how important certain attributes are for targeted locations. Knowing which features are highly important in certain locations can drive remodeling decisions to make properties more attractive to potential buyers. The value-add of this model for these companies can be measured in terms of returns on investment.\n",
    "\n",
    "Deployment of the model can be valuable for the rental market as well, where Airbnb can direct marketing efforts to areas with specific property attributes. Deployment of the model can also be used to provide the break-even horizon for making rent versus own decisions. In addition, loan refinancing companies can utilize this model along with Zillow’s liens and taxes database to target homeowners in specific areas.\n",
    "\n",
    "To further improve the effectiveness of the model, we should expand the model to include sales prices, liens, taxes, as well as identify biased data such as short sales, foreclosures, and “arms-length” transactions (i.e. sales to relatives). All these are readily available from Zillow, as they collect an enormous amount of data which we imagine is updated frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Exceptional\"></a>\n",
    "# Exceptional Work\n",
    "<b>10 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "You have free reign to provide additional analyses. One idea: grid search parameters in a parallelized fashion and visualize the\n",
    "performances across attributes. Which parameters are most significant for making a good model for each classification algorithm?\n",
    "</i>\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches Considered for Balanced Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the shortcomings of classification, K-nearest neighbors in particular, is the tendency to bias in favor of the majority class. To eliminate the bias, a number of approaches can be utilized including StratifiedKFold, which is the approach we use for our classification models. To be thorough, we explored a set of “imbalanced learn” algorithms, imblearn.RandomOverSampler, imblearn.SMOTE, and imblearn.ADASYN.\n",
    "\n",
    "1. StratifiedKFold - A variant of KFold, this ensures each class is represented equally (i.e. equal weights) as the algorithm performs each fold. Stratification is performed on the training dataset \"on the fly\" as opposed to performing it as part of data preprocessing.\n",
    "2. imblearn.RandomOverSampler - As a separate package, imblearn was developed to address the problem of imbalanced data sets; it is performed at data preprocessing. RandomOverSampler,in particular, performs a naive over sampling with replacement, duplicating original samples from the minority class. (Under sampling is the alternate approach).\n",
    "3. imblearn.SMOTE - SMOTE compensates for classes that are difficult to separate by performing over and under sampling using Tomek’s link or edited nearest neighbors cleaning methods.\n",
    "4. imblearn.ADASYN - Adaptive Synthetic Sampling Approach (ADASYN) is similar to SMOTE as it generates samples by interpolation but it focuses on the wrongly classified k-nearest neighbors.\n",
    "\n",
    "After considering these methods, we settled on the StratifiedKFold for simplicity since accuracies across the different approaches were practically equivalent.\n",
    "\n",
    "# TODO - insert graphic here (input\\imblearn.png), if possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature elimination\n",
    "<a href=\"#top\">⏫ Back to Top</a>\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two dimentional Linear Discriminant Analysis\n",
    "\n",
    "The idea is to see if there are separatable clusters by class\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "X = dataset_class['X']\n",
    "y = dataset_class['y']\n",
    "\n",
    "lde = LDA(n_components=2)\n",
    "X_lde = lde.fit(X, y).transform(X)\n",
    "\n",
    "colors = y.astype(str)\n",
    "colors[colors=='3101'] = 'g'\n",
    "colors[colors=='2061'] = 'b'\n",
    "colors[colors=='1286'] = 'r'\n",
    "\n",
    "plt.scatter(X_lde[:, 1], X_lde[:, 0], s=2, c=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"References\"></a>\n",
    "# References:\n",
    "\n",
    "- Kernels from Kaggle competition: https://www.kaggle.com/c/zillow-prize-1/kernels\n",
    "- Scikitlearn logistic regression: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "- Scikitlearn linear SVC: http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "- Stackoverflow pandas questions: https://stackoverflow.com/questions/tagged/pandas\n",
    "- Deployment Reference: http://www.zdnet.com/article/zillow-machine-learning-and-data-in-real-estate/\n",
    "- Advantages of GaussianProcessRegressor http://scikit-learn.org/stable/modules/gaussian_process.html\n",
    "- Advantages of GaussianProcessRegressor https://stats.stackexchange.com/questions/207183/main-advantages-of-gaussian-process-models\n",
    "- Advantages of GaussianProcessRegressor https://www.quora.com/What-are-some-advantages-of-using-Gaussian-Process-Models-vs-SVMs\n",
    "- Advantages of RandomForestRegressor https://www.quora.com/What-are-some-advantages-of-using-a-random-forest-over-a-decision-tree-given-that-a-decision-tree-is-simpler\n",
    "- Advantages of RandomForestRegressor https://www.datasciencecentral.com/profiles/blogs/random-forests-algorithm\n",
    "- Advantages of KNeighborsRegressor https://stats.stackexchange.com/questions/104255/why-would-anyone-use-knn-for-regression\n",
    "- Advantages of KNeighborsRegressor https://machinelearningmastery.com/k-nearest-neighbors-for-machine-learning/\n",
    "- Advantages of KNeighborsRegressor https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/#pros-and-cons-of-knn\n",
    "- Imbalanced Learn http://contrib.scikit-learn.org/imbalanced-learn/stable/install.html\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
