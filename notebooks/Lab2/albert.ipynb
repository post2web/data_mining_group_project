{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FYI directions, delete later\n",
    "\n",
    "You are to build upon the predictive analysis (classification) that you already completed in the previous mini-project, adding additional modeling from new classification algorithms as well as more explanations that are inline with the CRISP-DM framework. You should use appropriate cross validation for all of your analysis (explain your chosen method of performance validation in detail). Try to use as much testing data as possible in a realistic manner (you should define what you think\n",
    "is realistic and why). \n",
    "\n",
    "This report is worth 20% of the final grade. Please upload a report (one per team) with all code used, visualizations, and text in a single document. The format of the document can be PDF, *.ipynb, or HTML. You can write the report in whatever format you like, but it is easiest to turn in the rendered Jupyter notebook. The results should be reproducible using your report. Please carefully describe every assumption and every step in your report.\n",
    "\n",
    "Dataset Selection\n",
    "\n",
    "Select a dataset identically to the way you selected for the first project work week and mini-project.\n",
    "You are not required to use the same dataset that you used in the past, but you are encouraged.\n",
    "You must identify two tasks from the dataset to regress or classify. That is:\n",
    "\n",
    "* two classification tasks OR\n",
    "* two regression tasks OR\n",
    "* one classification task and one regression task\n",
    "\n",
    "For example, if your dataset was from the diabetes data you might try to predict two tasks: (1)\n",
    "classifying if a patient will be readmitted within a 30 day period or not, and (2) regressing what the\n",
    "total number of days a patient will spend in the hospital, given their history and specifics of the\n",
    "encounter like tests administered and previous admittance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Zillow Dataset Classification \n",
    "MSDS 7331 Data Mining - Section 403 - Lab 2\n",
    "\n",
    "Team: Ivelin Angelov, Yao Yao, Kaitlin Kirasich, Albert Asuncion\n",
    "\n",
    "<a id=\"top\"></a>\n",
    "## Contents\n",
    "* <a href=\"#Imports\">Imports</a>\n",
    "* <a href=\"#Define\">Define and Prepare Class Variables</a>\n",
    "* <a href=\"#Describe\">Describe the Final Dataset</a>\n",
    "* <a href=\"#Evaluation\">Explain Evaluation Metrics</a>\n",
    "* <a href=\"#Splits\">Training and Testing Splits</a>\n",
    "* <a href=\"#Models\">Three Different Classification/Regression Models</a>\n",
    "* <a href=\"#Analysis\">Visualizations of Results and Analysis</a>\n",
    "* <a href=\"#Advantages\">Advantages of Each Model</a>\n",
    "* <a href=\"#Attributes\">Important Attributes</a>\n",
    "* <a href=\"#Deployment\">Deployment</a>\n",
    "* <a href=\"#Exceptional\">Exceptional Work</a>\n",
    "* <a href=\"#References\">References</a>\n",
    "________________________________________________________________________________________________________\n",
    "<a id=\"Imports\"></a>\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to Top</a>\n",
    "<a id=\"Define\"></a>\n",
    "# Define and Prepare Class Variables\n",
    "<b>10 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis.\n",
    "</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = pd.read_csv('../../datasets/variables.csv').set_index('name')\n",
    "dataset = pd.read_csv('../../datasets/train.csv', low_memory=False)\n",
    "\n",
    "# remove unneeded variables\n",
    "del dataset['logerror']\n",
    "del dataset['transactiondate']\n",
    "del dataset['city']\n",
    "del dataset['price_per_sqft']\n",
    "\n",
    "nominal = variables[variables['type'].isin(['nominal'])]\n",
    "nominal = nominal[nominal.index.isin(dataset.columns)]\n",
    "continuous = variables[~variables['type'].isin(['nominal'])]\n",
    "continuous = continuous[continuous.index.isin(dataset.columns)]\n",
    "\n",
    "nominal_data = dataset[nominal.index]\n",
    "nominal_data = pd.get_dummies(nominal_data, drop_first=True)\n",
    "nominal_data = nominal_data[nominal_data.columns[~nominal_data.columns.isin(nominal.index)]]\n",
    "\n",
    "continuous_data = dataset[continuous.index]\n",
    "\n",
    "dataset = pd.concat([continuous_data, nominal_data], axis=1)\n",
    "\n",
    "columns = dataset.columns\n",
    "variables = variables[variables.index.isin(dataset.columns)]\n",
    "\n",
    "# shuffle the dataset (just in case)\n",
    "dataset = dataset.sample(frac=1)\n",
    "\n",
    "'The dataset has %d rows and %d columns' % dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Regression task\n",
    "\n",
    "In this dataset we will predict the value of taxamount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = dataset.copy()\n",
    "y = X['taxamount']\n",
    "del X['taxamount']\n",
    "# del X['taxvaluedollarcnt']????\n",
    "\n",
    "dataset_reg = {\n",
    "    'X': X,\n",
    "    'y': y\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification task\n",
    "I this task we are creating a variable: has_garage with posible values 1 or 0 representing has garage or doesn't have garage. The class is a little bit skewed with only 0.368 percent having garage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.copy()\n",
    "y = (X['garagecarcnt'] > 0).astype(int)\n",
    "del X['garagetotalsqft']\n",
    "del X['garagecarcnt']\n",
    "\n",
    "dataset_class = {\n",
    "    'X': X,\n",
    "    'y': y\n",
    "}\n",
    "\n",
    "y.sum() / len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to Top</a>\n",
    "<a id=\"Describe\"></a>\n",
    "# Describe the Final Dataset\n",
    "<b>5 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).\n",
    "</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_variables_table(variables):\n",
    "    variables = variables.sort_index()\n",
    "    rows = ['<tr><th>Variable</th><th>Type</th><th>Scale</th><th>Description</th></tr>']\n",
    "    for vname, atts in variables.iterrows():\n",
    "        atts = atts.to_dict()\n",
    "        # add scale if TBD\n",
    "        if atts['scale'] == 'TBD':\n",
    "            if atts['type'] in ['nominal', 'ordinal']:\n",
    "                uniques = dataset[vname].unique()\n",
    "                uniques = list(uniques.astype(str))\n",
    "                if len(uniques) < 10:\n",
    "                    atts['scale'] = '[%s]' % ', '.join(uniques)\n",
    "                else:\n",
    "                    atts['scale'] = '[%s]' % (', '.join(uniques[:5]) + ', ... (%d More)' % len(uniques))\n",
    "            if atts['type'] in ['ratio', 'interval']:\n",
    "                atts['scale'] = '(%d, %d)' % (dataset[vname].min(), dataset[vname].max())\n",
    "        row = (vname, atts['type'], atts['scale'], atts['description'])\n",
    "        rows.append('<tr><td>%s</td><td>%s</td><td>%s</td><td>%s</td></tr>' % row)\n",
    "    return HTML('<table>%s</table>' % ''.join(rows))\n",
    "\n",
    "output_variables_table(variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to Top</a>\n",
    "<a id=\"Evaluation\"></a>\n",
    "# Explain Evaluation Metrics\n",
    "<b>10 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "Choose and explain your evaluation metrics that you will use (i.e., accuracy, precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up any assertions.\n",
    "</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For our evaluation, we will be taking into account the accuracy and F-measure.  In order to compute the F-measure, we will need the precision and recall.  Because F-measure is a weighted average of these, we think a better F-measure score means the model has a better precision and recall.\n",
    "\n",
    "Accuracy is the ratio of correct predictions to the total number of observations. It is calculated as: (TP+TN) / (TP+FP+FN+TN). The closer accuracy is to 1, the more accurate the model is, with one caveat. For high accuracy to be a reliable indicator, the dataset has to be symmetric, i.e. total false positives are about equal to false negatives. Otherwise, we need to review other parameters as well.\n",
    "\n",
    "Precision is the ratio of correctly predicted positive observations to the total positive observations. It is calculated as: TP / (TP+FP).  \n",
    "\n",
    "Recall is the ratio of correctly predicted positive observations to all actual positives. It is calculated as TP / (TP+FN). The consequences of type 2 errors, predicting a false negative, are not extreme so we think recall is an appropriate measure of completeness.\n",
    "\n",
    "Finally, we will also use F-measure which is essentially a weighted average of the precision and recall into one simple statistic. F-measure This will be a number between 0 and 1 where closer to 1 is better and approaching 0 is worse. It overcomes the limitations of accuracy whenever false positives and false negatives are not about equal or symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to Top</a>\n",
    "<a id=\"Imports\"></a>\n",
    "# Training and Testing Splits\n",
    "<b>10 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "Choose the method you will use for dividing your data into training and testing splits (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time.\n",
    "</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TODO build chart of train size while accuracy continues to increase\n",
    "\n",
    "We have a large dataset so 10 fold cross validation may take too long.  Because of this, we will be splitting our dataset into 80% train and 20% test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "X = dataset_class['X']\n",
    "y = dataset_class['y']\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=.1, random_state=0)\n",
    "\n",
    "result = []\n",
    "\n",
    "# uniform distribution values between 0 and 1\n",
    "mask = np.random.rand(len(X))\n",
    "\n",
    "for frac in np.linspace(.01, 1, 20):\n",
    "    mask_frac = mask<=frac\n",
    "    X_frac = X[mask_frac]\n",
    "    y_frac = y[mask_frac]\n",
    "    \n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(X_frac, y_frac)\n",
    "    y_hat = clf.predict(X_test)\n",
    "    \n",
    "    result.append({\n",
    "        'accuracy': mt.accuracy_score(y_test, y_hat),\n",
    "        'count': len(X_frac),\n",
    "        'frac': frac\n",
    "    })\n",
    "\n",
    "pd.DataFrame(result).plot('count', 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to Top</a>\n",
    "<a id=\"Models\"></a>\n",
    "# Three Different Classification/Regression Models\n",
    "<b>20 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!\n",
    "</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaitlin - 3 classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for train test split\n",
    "# Start with 80/20 split.  Keep same test and train for each classification model for now\n",
    "X = dataset_class['X']\n",
    "y = dataset_class['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import metrics standard across all\n",
    "from sklearn import metrics as mt\n",
    "# Define an accuracy plot\n",
    "def per_class_accuracy(ytrue,yhat):\n",
    "    conf = mt.confusion_matrix(ytrue,yhat)\n",
    "    norm_conf = conf.astype('float') / conf.sum(axis=1)[:, np.newaxis]\n",
    "    return np.diag(norm_conf)\n",
    "\n",
    "def plot_class_acc(ytrue,yhat, title=''):\n",
    "    acc_list = per_class_accuracy(ytrue,yhat)\n",
    "    plt.bar(range(len(acc_list)), acc_list)\n",
    "    plt.xlabel('Class value (one per face)')\n",
    "    plt.ylabel('Accuracy within class')\n",
    "    plt.title(title+\", Total Acc=%.1f\"%(100*mt.accuracy_score(ytrue,yhat)))\n",
    "    plt.grid()\n",
    "    plt.ylim([0,1])\n",
    "    plt.show()\n",
    "    \n",
    "# Plot the feature importances of the forest\n",
    "def plot_feature_importance(ytrue,yhat,rt, title=''):\n",
    "    importances = rt.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in rt.estimators_],\n",
    "             axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    for f in range(X.shape[1]):\n",
    "        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "    plt.figure()\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "    plt.xticks(range(X.shape[1]), indices)\n",
    "    plt.xlim([-1, X.shape[1]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# instantiate learning model (k = 3)\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# fitting the model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# predict the response\n",
    "yhat = knn.predict(X_test)\n",
    "\n",
    "# evaluate accuracy\n",
    "print ('KNN accuracy', mt.accuracy_score(y_test, yhat))\n",
    "prf = mt.precision_recall_fscore_support(y_test, yhat, average='macro')\n",
    "print('KNN Precision', prf[0])\n",
    "print('KNN Recall', prf[1])\n",
    "print('KNN F-measure', prf[2])\n",
    "plot_class_acc(y_test,yhat,title=\"KNN, Raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# instantiate learning model\n",
    "rf = RandomForestClassifier(max_depth=50)\n",
    "\n",
    "# fitting the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# predict the response\n",
    "yhat = rf.predict(X_test)\n",
    "\n",
    "# evaluate accuracy\n",
    "print ('Random Forest Accuracy', mt.accuracy_score(y_test, yhat))\n",
    "prf = mt.precision_recall_fscore_support(y_test, yhat, average='macro')\n",
    "print('Random Forest Precision', prf[0])\n",
    "print('Random Forest Recall', prf[1])\n",
    "print('Random Forest F-measure', prf[2])\n",
    "plot_class_acc(y_test,yhat,title=\"Random Forest, Raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Feature importance of Random Forest\n",
    "importances = rf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rf.estimators_],\n",
    "         axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Random Forest Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    if importances[indices[f]] > 0.05:\n",
    "        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.title(\"Random Forest Feature importances\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices],\n",
    "   color=\"r\", yerr=std[indices], align=\"center\")\n",
    "feature_names = X.columns\n",
    "plt.xticks(range(10), feature_names)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlim([-1, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# instantiate learning model\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# fitting the model\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# predict the response\n",
    "yhat = gnb.predict(X_test)\n",
    "\n",
    "# evaluate accuracy\n",
    "print ('Naive Bayes Accuracy', mt.accuracy_score(y_test, yhat))\n",
    "prf = mt.precision_recall_fscore_support(y_test, yhat, average='macro')\n",
    "print('Naive Bayes Precision', prf[0])\n",
    "print('Naive Bayes Recall', prf[1])\n",
    "print('Naive Bayes F-measure', prf[2])\n",
    "plot_class_acc(y_test,yhat,title=\"Naive Bayes, Raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to Top</a>\n",
    "<a id=\"Analysis\"></a>\n",
    "# Visualizations of Results and Analysis\n",
    "<b>10 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model.\n",
    "</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to Top</a>\n",
    "<a id=\"Advantages\"></a>\n",
    "# Advantages of Each Model\n",
    "<b>10 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniques—be sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course.\n",
    "</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to Top</a>\n",
    "<a id=\"Attributes\"></a>\n",
    "# Important Attributes\n",
    "<b>10 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task.\n",
    "</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to Top</a>\n",
    "<a id=\"Deployment\"></a>\n",
    "# Deployment\n",
    "<b>5 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "How useful is your model for interested parties (i.e., the companies or organizations that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.?\n",
    "</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to Top</a>\n",
    "<a id=\"Exceptional\"></a>\n",
    "# Exceptional Work\n",
    "<b>10 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "You have free reign to provide additional analyses. One idea: grid search parameters in a parallelized fashion and visualize the\n",
    "performances across attributes. Which parameters are most significant for making a good model for each classification algorithm?\n",
    "</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to Top</a>\n",
    "<a id=\"References\"></a>\n",
    "# References:\n",
    "\n",
    "- Kernels from Kaggle competition: https://www.kaggle.com/c/zillow-prize-1/kernels\n",
    "- Scikitlearn logistic regression: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "- Scikitlearn linear SVC: http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "- Stackoverflow pandas questions: https://stackoverflow.com/questions/tagged/pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
