{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Mini-lab 1: Zillow Dataset Logistic Regression and SVMs  \n",
    "MSDS 7331 Data Mining - Section 403 - Mini Lab 1\n",
    "\n",
    "Team: Ivelin Angelov, Yao Yao, Kaitlin Kirasich, Albert Asuncion\n",
    "\n",
    "<a id=\"top\"></a>\n",
    "## Contents\n",
    "* <a href=\"#Imports\">Imports</a>\n",
    "* <a href=\"#Models\">Models</a>\n",
    "* <a href=\"#Advantages\">Advantages of Each Model</a>\n",
    "* <a href=\"#Feature\">Feature Importance</a>\n",
    "* <a href=\"#Insights\">Insights</a>\n",
    "________________________________________________________________________________________________________\n",
    "\n",
    "<a id=\"Imports\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "We chose to use the same Zillow dataset from Lab 1. For origin and purpose of dataset as well as a detailed description of the dataset, refer to https://github.com/post2web/data_mining_group_project/blob/master/notebooks/lab1.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T17:34:15.015449Z",
     "start_time": "2017-09-30T17:34:14.408651Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data, Create y and X\n",
    "We will use the Zillow dataset from our previous lab, where the cleanup files were exported from lab 1 into mini-lab 1. Note that for logistic regression and support vector classifier models, we choose to use mostly complete continuous variables as well as create dummy variables for nominal variables to cross compare the performance, feature importance, and insights of each model. X is the training set and y is the test set, where we are testing if our models can accurately predict positive logerrror from that of negative. \n",
    "\n",
    "Data columns that are only available for the training set and not the test set (transaction date) were removed. Parcelid was removed because each individual property has its own ID and does not correlate well with regression or SVMs. The column that was created for \"New Features\" from Lab 1 (city and pricepersqft) were also removed for the sake of simplicity of only using original data for the prediction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T17:34:15.827124Z",
     "start_time": "2017-09-30T17:34:15.017532Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The dataset has 116761 rows and 49 columns'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load datasets here:\n",
    "variables = pd.read_csv('../datasets/variables.csv').set_index('name')\n",
    "X = pd.read_csv('../datasets/train.csv', low_memory=False)\n",
    "\n",
    "y = (X['logerror'] > 0).astype(np.int32)\n",
    "\n",
    "del X['logerror']\n",
    "del X['transactiondate']\n",
    "del X['parcelid']\n",
    "del X['city']\n",
    "# TODO fix me\n",
    "del X['price_per_sqft']\n",
    "\n",
    "'The dataset has %d rows and %d columns' % X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 116761 entries, 0 to 116760\n",
      "Data columns (total 49 columns):\n",
      "Unnamed: 0                      116761 non-null int64\n",
      "airconditioningtypeid           116761 non-null int64\n",
      "bathroomcnt                     116761 non-null float64\n",
      "bedroomcnt                      116761 non-null int64\n",
      "buildingqualitytypeid           116761 non-null int64\n",
      "calculatedbathnbr               116761 non-null float64\n",
      "calculatedfinishedsquarefeet    116761 non-null int64\n",
      "finishedsquarefeet12            116761 non-null int64\n",
      "finishedsquarefeet50            116761 non-null float64\n",
      "fips                            116761 non-null int64\n",
      "fireplacecnt                    116761 non-null int64\n",
      "fullbathcnt                     116761 non-null float64\n",
      "garagecarcnt                    116761 non-null float64\n",
      "garagetotalsqft                 116761 non-null float64\n",
      "hashottuborspa                  116761 non-null int64\n",
      "heatingorsystemtypeid           116761 non-null int64\n",
      "latitude                        116761 non-null float64\n",
      "longitude                       116761 non-null float64\n",
      "lotsizesquarefeet               116761 non-null float64\n",
      "poolcnt                         116761 non-null float64\n",
      "poolsizesum                     116761 non-null float64\n",
      "pooltypeid10                    116761 non-null int64\n",
      "pooltypeid2                     116761 non-null int64\n",
      "pooltypeid7                     116761 non-null int64\n",
      "propertycountylandusecode       116761 non-null object\n",
      "propertylandusetypeid           116761 non-null int64\n",
      "propertyzoningdesc              116761 non-null object\n",
      "rawcensustractandblock          116761 non-null int64\n",
      "regionidcity                    116761 non-null int64\n",
      "regionidcounty                  116761 non-null int64\n",
      "regionidneighborhood            116761 non-null int64\n",
      "regionidzip                     116761 non-null int64\n",
      "roomcnt                         116761 non-null int64\n",
      "threequarterbathnbr             116761 non-null int64\n",
      "unitcnt                         116761 non-null int64\n",
      "yardbuildingsqft17              116761 non-null int64\n",
      "yardbuildingsqft26              116761 non-null float64\n",
      "yearbuilt                       116761 non-null int64\n",
      "numberofstories                 116761 non-null int64\n",
      "structuretaxvaluedollarcnt      116761 non-null int64\n",
      "taxvaluedollarcnt               116761 non-null int64\n",
      "assessmentyear                  116761 non-null int64\n",
      "landtaxvaluedollarcnt           116761 non-null int64\n",
      "taxamount                       116761 non-null float64\n",
      "taxdelinquencyflag              116761 non-null int64\n",
      "taxdelinquencyyear              116761 non-null int64\n",
      "censustractandblock             116761 non-null float64\n",
      "zipcode_type                    66197 non-null object\n",
      "location_type                   66197 non-null object\n",
      "dtypes: float64(14), int64(31), object(4)\n",
      "memory usage: 43.7+ MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Nominal Data\n",
    "Nominal data ususally has more than two values. For logistic regression and SVMs, we created dummy variables that only factor in 0s and 1s for the prediction process of logistic regression and SVMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T17:34:17.076280Z",
     "start_time": "2017-09-30T17:34:15.828711Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nominal = variables[variables['type'].isin(['nominal'])]\n",
    "nominal = nominal[nominal.index.isin(X.columns)]\n",
    "nominal_data = X[nominal.index]\n",
    "\n",
    "nominal_data = pd.get_dummies(nominal_data, drop_first=True)\n",
    "nominal_data = nominal_data[nominal_data.columns[~nominal_data.columns.isin(nominal.index)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Continuous Data\n",
    "StandardScaler from sklean was applied to the continuous data columns to standardize the dataset around center 0 with equal variance for creating normal distribtions prior to the application of logistic regressio and SVMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T17:34:17.180736Z",
     "start_time": "2017-09-30T17:34:17.078300Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "continuous = variables[~variables['type'].isin(['nominal'])]\n",
    "continuous = continuous[continuous.index.isin(X.columns)]\n",
    "\n",
    "X = X[continuous.index]\n",
    "columns = X.columns\n",
    "\n",
    "# X = (X - µ) / σ\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "X = pd.DataFrame(X, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 116761 entries, 0 to 116760\n",
      "Data columns (total 30 columns):\n",
      "bathroomcnt                     116761 non-null float64\n",
      "bedroomcnt                      116761 non-null float64\n",
      "buildingqualitytypeid           116761 non-null float64\n",
      "calculatedbathnbr               116761 non-null float64\n",
      "calculatedfinishedsquarefeet    116761 non-null float64\n",
      "finishedsquarefeet12            116761 non-null float64\n",
      "finishedsquarefeet50            116761 non-null float64\n",
      "fireplacecnt                    116761 non-null float64\n",
      "fullbathcnt                     116761 non-null float64\n",
      "garagecarcnt                    116761 non-null float64\n",
      "garagetotalsqft                 116761 non-null float64\n",
      "hashottuborspa                  116761 non-null float64\n",
      "latitude                        116761 non-null float64\n",
      "longitude                       116761 non-null float64\n",
      "lotsizesquarefeet               116761 non-null float64\n",
      "poolcnt                         116761 non-null float64\n",
      "poolsizesum                     116761 non-null float64\n",
      "roomcnt                         116761 non-null float64\n",
      "threequarterbathnbr             116761 non-null float64\n",
      "unitcnt                         116761 non-null float64\n",
      "yardbuildingsqft17              116761 non-null float64\n",
      "yardbuildingsqft26              116761 non-null float64\n",
      "yearbuilt                       116761 non-null float64\n",
      "numberofstories                 116761 non-null float64\n",
      "structuretaxvaluedollarcnt      116761 non-null float64\n",
      "taxvaluedollarcnt               116761 non-null float64\n",
      "assessmentyear                  116761 non-null float64\n",
      "landtaxvaluedollarcnt           116761 non-null float64\n",
      "taxamount                       116761 non-null float64\n",
      "taxdelinquencyyear              116761 non-null float64\n",
      "dtypes: float64(30)\n",
      "memory usage: 26.7 MB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge data\n",
    "The data was then merged for the application of logistic regression and SVM prediction. The following shows the final shape of the dataset after the application of dummy variables and StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T17:34:17.259789Z",
     "start_time": "2017-09-30T17:34:17.183976Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The dataset has 116761 rows and 2107 columns'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.concat([X, nominal_data], axis=1)\n",
    "'The dataset has %d rows and %d columns' % X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.2777713394270958"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.loc[1,'bathroomcnt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Models\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "# Models\n",
    "\n",
    "<b>[50 points]</b>\n",
    "\n",
    "<i>\n",
    "Create a logistic regression model and a support vector machine model for the classification task involved with your dataset. Assess how well each model performs (use 80/20 training/testing split for your data). <b>Adjust parameters of the models to make them more accurate</b>. If your dataset size requires the use of stochastic gradient descent, then linear kernel only is fine to use. That is, the SGDClassifier is fine to use for optimizing logistic regression and linear support vector machines. For many problems, SGD will be required in order to train the SVM model in a reasonable timeframe. \n",
    "</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "For the logistic regression model, we created a function that took in X_train and Y_train from the original data set to test for X_test from the modified dataset. The accuracy of the logistic regression prediction for positive or negative logerror was compared with that of the original, where a confusion matrix was made to show percentage accuracy. Due to the complexity of the dataset, we are slightly better than 50% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T16:34:31.309948Z",
     "start_time": "2017-09-30T16:34:07.156641Z"
    }
   },
   "outputs": [],
   "source": [
    "def logistic_regression_model(X_train, y_train, X_test, **params):\n",
    "    clf = LogisticRegression(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf.predict(X_test), clf\n",
    "\n",
    "yhat, _ = logistic_regression_model(X_train=X, y_train=y, X_test=X)\n",
    "\n",
    "accuracy = float(sum(yhat==y)) / len(y)\n",
    "print('Percentage accuracy: %.12f' % (accuracy))\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y, yhat),\n",
    "    columns=['Predicted 1', 'Predicted 0'], \n",
    "    index=['Actual 1', 'Actual 0'], \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running logistic regression one time with the built in parameters, we got an accuracy of 0.59.  To try to improve this, we want to do a few things. \n",
    "\n",
    "First, we want to do the 80/20 split 5 times and average those results to get a better accuracy.  By splitting the training and test sets up multiple times, we can minimize the effects of outliers.\n",
    "\n",
    "Second, we want to see how changing the value of C will effect the accuracy.  To do this we have another for loop which sets C at 10 linear increments from 0.01 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T16:39:14.935404Z",
     "start_time": "2017-09-30T16:36:57.560239Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final average accuracy: 0.568663555004\n",
      "Final average accuracy: 0.569006123410\n",
      "Final average accuracy: 0.569337272870\n",
      "Final average accuracy: 0.569320858134\n",
      "Final average accuracy: 0.569612469490\n",
      "Final average accuracy: 0.569992434948\n",
      "Final average accuracy: 0.569932281567\n",
      "Final average accuracy: 0.570155868625\n",
      "Final average accuracy: 0.570145163362\n",
      "Final average accuracy: 0.570088639575\n",
      "Final average accuracy: 0.570094556666\n",
      "Final average accuracy: 0.570080931786\n",
      "Final average accuracy: 0.569979149442\n",
      "Final average accuracy: 0.569938398860\n",
      "Final average accuracy: 0.570014987368\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5           # number of times we will split test and training dataset\n",
    "accuracies = []        # array to hold accuracy to take average of at the end\n",
    "final_accuracies = {}  # list of final accuracies\n",
    "\n",
    "# Now for each value of C and for each train/ test split run logistic regression\n",
    "for C in np.linspace(.01, 50, 20):\n",
    "    for i in range(n_splits):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)\n",
    "\n",
    "        yhat, _ = logistic_regression_model(\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            C=C\n",
    "        )\n",
    "        \n",
    "        accuracy = float(sum(yhat==y_test)) / len(y_test)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    print('Final average accuracy: %.12f' % (np.mean(accuracies)))\n",
    "    final_accuracies[C] = np.mean(accuracies)\n",
    "    \n",
    "final_accuracies = pd.Series(final_accuracies)\n",
    "\n",
    "# Plot the final accuracy for each value of C\n",
    "plt.title('Accuracy of Logistic Regression Models Based on C Values', fontsize=15)\n",
    "plt.xlabel('C Values', fontsize=15)\n",
    "plt.ylabel('Accuracy', fontsize=15)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "final_accuracies.plot(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 VS L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further compared accuracy based on L1 or L2 penalty, where we found L1, the error, is slightly more accurate than L2, the squared error. Which is interesting because typically L2 is better for minimizing error than L1. We think this is because we have about 2000 variables and did not go through and check if every one made sense.  Since L1 acts as a built in feature selection by giving weights nearly 0, this could be why our L1 is better than L2.  This is also a concern for overfitting the model but we will continue as our metric goal here is to maximize accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_splits = 2\n",
    "accuracies = []\n",
    "final_accuracies = {}\n",
    "\n",
    "for penalty in ['l1', 'l2']:\n",
    "    for i in range(n_splits):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)\n",
    "\n",
    "        yhat, _ = logistic_regression_model(\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            penalty=penalty\n",
    "        )\n",
    "        \n",
    "        accuracy = float(sum(yhat==y_test)) / len(y_test)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    print('Final average accuracy for %s : %.12f' % (penalty, np.mean(accuracies)))\n",
    "    final_accuracies[penalty] = np.mean(accuracies)\n",
    "    \n",
    "final_accuracies = pd.Series(final_accuracies)\n",
    "\n",
    "plt.title('Accuracy of Logistic Regression Models Based on Penalty Type', fontsize=15)\n",
    "plt.xlabel('Penalty Type', fontsize=15)\n",
    "plt.ylabel('Accuracy', fontsize=15)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "final_accuracies.plot(kind='bar', ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine Classifier\n",
    "For the support vector machine model, we created a function that took in X_train and Y_train from the original data set to test for X_test from the modified dataset. The accuracy of the SVM prediction for positive or negative logerror was compared with that of the original, where a confusion matrix was made to show percentage accuracy. Due to the complexity of the dataset, we are slightly better than 50% accuracy.\n",
    "\n",
    "## Why we use SGDClassifier?\n",
    "\n",
    "We tried out a few sklearn support vector machine functions and noticed that the accuracy was similar for each but with such a large dataset we decided to use time as our performance metric.  \n",
    "\n",
    "First first tried SVC setting kernal = 'linear' but waited a long time for it to finish.\n",
    "Next, we tried LinearSVC because the liblinear library it uses tends to be faster to converge the larger the number of samples is than the libsvm library.  \n",
    "\n",
    "Lastly, we tried and found our winner, SGDClassifier with loss = 'hinge' to use a stochastic gradient descent, which was exponentially faster than the others because it only uses a subset of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T17:26:37.852537Z",
     "start_time": "2017-09-30T17:26:24.531165Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "def support_vector_machine_model(X_train, y_train, X_test, **params):\n",
    "    # takes about 1 minute\n",
    "    params['n_iter'] = 1\n",
    "    params['loss'] = 'hinge'\n",
    "    params['alpha'] = 0.9\n",
    "    params['epsilon'] = 0.00078\n",
    "    clf = SGDClassifier(**params)\n",
    "    # this takes too long\n",
    "    #clf = SVC(kernel='linear')\n",
    "    #clf = LinearSVC(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf.predict(X_test), clf\n",
    "\n",
    "yhat, _ = support_vector_machine_model(X_train=X, y_train=y, X_test=X)\n",
    "\n",
    "accuracy = float(sum(yhat==y)) / len(y)\n",
    "print('Accuracy: %.12f' % (accuracy))\n",
    "\n",
    "pd.DataFrame(confusion_matrix(y, yhat),\n",
    "    columns=['Predicted 1', 'Predicted 0'], \n",
    "    index=['Actual 1', 'Actual 0'], \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to do: evaluate performance for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-09-30T17:21:57.244Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_splits = 2           # number of times we will split test and training dataset\n",
    "accuracies = []        # array to hold accuracy to take average of at the end\n",
    "final_accuracies = {}  # list of final accuracies\n",
    "\n",
    "# Now for each value of C and for each train/ test split run logistic regression\n",
    "for testable in np.linspace(.00001, .01, 3):\n",
    "    for i in range(n_splits):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)\n",
    "\n",
    "        yhat, _ = support_vector_machine_model(\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            alpha=testable\n",
    "        )\n",
    "        \n",
    "        accuracy = float(sum(yhat==y_test)) / len(y_test)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    print('Final average accuracy: %.12f' % (np.mean(accuracies)))\n",
    "    final_accuracies[testable] = np.mean(accuracies)\n",
    "    \n",
    "final_accuracies = pd.Series(final_accuracies)\n",
    "\n",
    "# Plot the final accuracy for each value of C\n",
    "plt.title('Accuracy of CVM Models Based on Alpha Values', fontsize=15)\n",
    "plt.xlabel('Alpha Values', fontsize=15)\n",
    "plt.ylabel('Accuracy', fontsize=15)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "final_accuracies.plot(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO same with epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO same with l1 and l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T17:47:44.978836Z",
     "start_time": "2017-09-30T17:47:44.960014Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [[.55, 2000], [.57, 4000]]\n",
    "table = pd.DataFrame(data, columns=['Accuracy', 'Time it takes'], index=['Logistic Regression', 'Support Vecotr Machine'])\n",
    "print('Compare result ot the two models')\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Advantages\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "## Advantages of Each Model\n",
    "<b>[10 points]</b>\n",
    "\n",
    "<i>\n",
    "Discuss the advantages of each model for each classification task. Does one type of model offer superior performance over another in terms of prediction accuracy? In terms of training time or efficiency? Explain in detail. \n",
    "</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# to do: write advantage after SVM performance parameters are found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Feature\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "## Feature Importance\n",
    "<b>[30 points]</b>\n",
    "    \n",
    "<i>\n",
    "Use the weights from logistic regression to interpret the importance of different features for the classification task. Explain your interpretation in detail. Why do you think some variables are more important?\n",
    "</i>\n",
    "\n",
    "# Logistic Regression Feature Importance\n",
    "\n",
    "After scaling the continuous variables, the feature importance showed that propertyzoningdesc and propertycountylandusecode for multiple counties showed the most importance for predicting logerror. This might be due to the fact that neighborhoods based on location highly dictate the sales price of a property. Tax was also a big factor. Perhaps the more land owners pay for property tax could better predict property value because more amenities could be added for a richer neighborhood than that for a poorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T16:47:48.626349Z",
     "start_time": "2017-09-30T16:47:19.101205Z"
    },
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_, clf = logistic_regression_model(X_train=X, y_train=y, X_test=X, C=2)\n",
    "\n",
    "index = pd.Index(X.columns, name='Variable Name')\n",
    "\n",
    "importance = pd.Series(np.abs(clf.coef_[0]), index=index)\n",
    "importance = importance.sort_values()\n",
    "\n",
    "importance = importance.iloc[:50]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 20))\n",
    "importance.plot(kind='barh', ax=ax)\n",
    "plt.title('Logistic Regression Feature Importance (TOP 50 Variables)')\n",
    "plt.xlabel('Importance', fontsize=8)\n",
    "plt.ylabel('Feature', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Insights\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "## Insights\n",
    "<b>[10 points]</b>\n",
    "\n",
    "<i>\n",
    "Look at the chosen support vectors for the classification task. Do these provide any insight into the data? Explain. If you used stochastic gradient descent (and therefore did not explicitly solve for support vectors), try subsampling your data to train the SVC model — then analyze the support vectors from the subsampled dataset. \n",
    "</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insights from SVM Model\n",
    "\n",
    "We used support vectors for gradient descent to find the optimal accuracy of the SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-30T17:39:55.129226Z",
     "start_time": "2017-09-30T17:39:48.559995Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='linear', max_iter=10)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# this hold the indexes of the support vectors\n",
    "clf.support_\n",
    "\n",
    "# this holds a subset of the data which is used for support vectors\n",
    "clf.support_vectors_\n",
    "\n",
    "# get number of support vectors for each class\n",
    "print('Nubmer of support vectros for each feature:', clf.n_support_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.support_vectors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_clf_support = pd.DataFrame(clf.support_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_clf_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_clf_supportvectors = pd.DataFrame(clf.support_vectors_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_clf_supportvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_clf_supportvectors['logerror'] = y[clf.support_vectors_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to Top</a>\n",
    "# References:\n",
    "\n",
    "- Kernels from Kaggle competition: https://www.kaggle.com/c/zillow-prize-1/kernels\n",
    "- Scikitlearn logistic regression: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "- Scikitlearn linear SVC: http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "- Stackoverflow pandas questions: https://stackoverflow.com/questions/tagged/pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "426px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
