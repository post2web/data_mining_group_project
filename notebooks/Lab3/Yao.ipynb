{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FYI directions, delete later\n",
    "\n",
    "CRISP-DM Capstone: Association Rule Mining, Clustering, or Collaborative Filtering\n",
    "\n",
    "In the final assignment for this course, you will be using one of three different analysis methods:\n",
    "\n",
    "* Option A: Use clustering on an unlabeled dataset to provide insight or features\n",
    "* Option B: Use transaction data for mining associations rules\n",
    "* Option C: Use collaborative filtering to build a custom recommendation system\n",
    "\n",
    "Your choice of dataset will largely determine the task that you are trying to achieve. Though the dataset does not need to change from your previous tasks. For example, you might choose to use clustering on your data as a preprocessing step that extracts different features. Then you can use those features to build a classifier and analyze its performance in terms of accuracy (precision, recall) and speed. Alternatively, you might choose a completely different dataset and perform rule mining or build a recommendation system.\n",
    "\n",
    "Dataset Selection and Toolkits\n",
    "\n",
    "As before, you need to choose a dataset that is not small. It might be massive in terms of the number of attributes (or transactions), classes (or items, users, etc.) or whatever is appropriate for the task you are performing. Note that scikit-learn can be used for clustering analysis, but not for Association Rule Mining (you should use R) or collaborative filtering (you should use graphlabcreate from Dato). Both can be run using Jupyter notebooks as shown in lecture.\n",
    "\n",
    "* One example of a recommendation dataset is the movie lens rating data: http://grouplens.org/\n",
    "datasets/movielens/\n",
    "* Some examples of association rule mining datasets: http://fimi.ua.ac.be/data/\n",
    "\n",
    "Write a report covering in detail all the steps of the project. The results need to be reproducible using only this report. Describe all assumptions you make and include all code you use in the Jupyter notebook or as supplemental functions. Follow the CRISP-DM framework in your analysis (you are performing all of the CRISP-DM outline). This report is worth 20% of the final grade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Zillow Dataset CRISP-DM Capstone: Association Rule Mining, Clustering, or Collaborative Filtering\n",
    "MSDS 7331 Data Mining - Section 403 - Lab 3\n",
    "\n",
    "Team: Ivelin Angelov, Yao Yao, Kaitlin Kirasich, Albert Asuncion\n",
    "\n",
    "<a id=\"top\"></a>\n",
    "## Contents\n",
    "* <a href=\"#Imports\">Imports of libraries and data files</a>\n",
    "* <a href=\"#Business\">Business Understanding</a>\n",
    "* <a href=\"#Description\">Dataset Description</a>\n",
    "    - <a href=\"#Description_1\">Describtion of meaning and type of data attributes before the cleaning process</a>\n",
    "    - <a href=\"#Description_2\">Verification and description of data quality</a>\n",
    "    - <a href=\"#Description_3\">Cleaning process</a>\n",
    "    - <a href=\"#Description_4\">Final verification of data quality</a>\n",
    "    - <a href=\"#Description_5\">Describtion of meaning and type of final data attributes after the cleaning process</a>\n",
    "    - <a href=\"#Description_6\">Table of Binary Variables</a>\n",
    "    - <a href=\"#save_state\">Save cleaned dataset</a>\n",
    "* <a href=\"#restore_state\">Load cleaned dataset [Checkpoint]</a>\n",
    "* <a href=\"#Attribute\">Attribute Visualizion</a>\n",
    "    - Attribute A\n",
    "    - Attribute B\n",
    "* <a href=\"#Train\">Train and Adjust Parameters</a>\n",
    "    - <a href=\"#train_kmeans\">KMeans Clustering</a>\n",
    "    - <a href=\"#train_agglomerative\">Agglomerative Clustering</a>\n",
    "    - <a href=\"#train_spectral\">Spectral Clustering</a>\n",
    "\n",
    "* <a href=\"#Evaluate\">Evaluate and Compare</a>\n",
    "* <a href=\"#Visualize\">Visualize Results</a>\n",
    "* <a href=\"#Ramifications\">Summarize the Ramifications</a>\n",
    "* <a href=\"#Deployment\">Deployment</a>\n",
    "* <a href=\"#Exceptional\">Exceptional Work</a>\n",
    "* <a href=\"#References\">References</a>\n",
    "________________________________________________________________________________________________________\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Imports\"></a>\n",
    "# Imports of libraries and utility functions\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import zipfile\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "variables_description = [\n",
    "['airconditioningtypeid', 'nominal', 'TBD', 'Type of cooling system present in the home (if any)']\n",
    ",['architecturalstyletypeid', 'nominal', 'TBD', 'Architectural style of the home (i.e. ranch, colonial, split-level, etc…)']\n",
    ",['assessmentyear', 'interval', 'TBD', 'The year of the property tax assessment']\n",
    ",['basementsqft', 'ratio', 'TBD', 'Finished living area below or partially below ground level']\n",
    ",['bathroomcnt', 'ordinal', 'TBD', 'Number of bathrooms in home including fractional bathrooms']\n",
    ",['bedroomcnt', 'ordinal', 'TBD', 'Number of bedrooms in home']\n",
    ",['buildingclasstypeid', 'nominal', 'TBD', 'The building framing type (steel frame, wood frame, concrete/brick)']\n",
    ",['buildingqualitytypeid', 'ordinal', 'TBD', 'Overall assessment of condition of the building from best (lowest) to worst (highest)']\n",
    ",['calculatedbathnbr', 'ordinal', 'TBD', 'Number of bathrooms in home including fractional bathroom']\n",
    ",['calculatedfinishedsquarefeet', 'ratio', 'TBD', 'Calculated total finished living area of the home']\n",
    ",['censustractandblock', 'nominal', 'TBD', 'Census tract and block ID combined - also contains blockgroup assignment by extension']\n",
    ",['decktypeid', 'nominal', 'TBD', 'Type of deck (if any) present on parcel']\n",
    ",['finishedfloor1squarefeet', 'ratio', 'TBD', 'Size of the finished living area on the first (entry) floor of the home']\n",
    ",['finishedsquarefeet12', 'ratio', 'TBD', 'Finished living area']\n",
    ",['finishedsquarefeet13', 'ratio', 'TBD', 'Perimeter living area']\n",
    ",['finishedsquarefeet15', 'ratio', 'TBD', 'Total area']\n",
    ",['finishedsquarefeet50', 'ratio', 'TBD', 'Size of the finished living area on the first (entry) floor of the home']\n",
    ",['finishedsquarefeet6', 'ratio', 'TBD', 'Base unfinished and finished area']\n",
    ",['fips', 'nominal', 'TBD', 'Federal Information Processing Standard code - see https://en.wikipedia.org/wiki/FIPS_county_code for more details']\n",
    ",['fireplacecnt', 'ordinal', 'TBD', 'Number of fireplaces in a home (if any)']\n",
    ",['fireplaceflag', 'ordinal', 'TBD', 'Is a fireplace present in this home']\n",
    ",['fullbathcnt', 'ordinal', 'TBD', 'Number of full bathrooms (sink, shower + bathtub, and toilet) present in home']\n",
    ",['garagecarcnt', 'ordinal', 'TBD', 'Total number of garages on the lot including an attached garage']\n",
    ",['garagetotalsqft', 'ratio', 'TBD', 'Total number of square feet of all garages on lot including an attached garage']\n",
    ",['hashottuborspa', 'ordinal', 'TBD', 'Does the home have a hot tub or spa']\n",
    ",['heatingorsystemtypeid', 'nominal', 'TBD', 'Type of home heating system']\n",
    ",['landtaxvaluedollarcnt', 'ratio', 'TBD', 'The assessed value of the land area of the parcel']\n",
    ",['latitude', 'interval', 'TBD', 'Latitude of the middle of the parcel multiplied by 10e6']\n",
    ",['logerror', 'interval', 'TBD', 'Error or the Zillow model response variable']\n",
    ",['longitude', 'interval', 'TBD', 'Longitude of the middle of the parcel multiplied by 10e6']\n",
    ",['lotsizesquarefeet', 'ratio', 'TBD', 'Area of the lot in square feet']\n",
    ",['numberofstories', 'ordinal', 'TBD', 'Number of stories or levels the home has']\n",
    ",['parcelid', 'nominal', 'TBD', 'Unique identifier for parcels (lots)']\n",
    ",['poolcnt', 'ordinal', 'TBD', 'Number of pools on the lot (if any)']\n",
    ",['poolsizesum', 'ratio', 'TBD', 'Total square footage of all pools on property']\n",
    ",['pooltypeid10', 'nominal', 'TBD', 'Spa or Hot Tub']\n",
    ",['pooltypeid2', 'nominal', 'TBD', 'Pool with Spa/Hot Tub']\n",
    ",['pooltypeid7', 'nominal', 'TBD', 'Pool without hot tub']\n",
    ",['propertycountylandusecode', 'nominal', 'TBD', 'County land use code i.e. it\\'s zoning at the county level']\n",
    ",['propertylandusetypeid', 'nominal', 'TBD', 'Type of land use the property is zoned for']\n",
    ",['propertyzoningdesc', 'nominal', 'TBD', 'Description of the allowed land uses (zoning) for that property']\n",
    ",['rawcensustractandblock', 'nominal', 'TBD', 'Census tract and block ID combined - also contains blockgroup assignment by extension']\n",
    ",['regionidcity', 'nominal', 'TBD', 'City in which the property is located (if any)']\n",
    ",['regionidcounty', 'nominal', 'TBD', 'County in which the property is located']\n",
    ",['regionidneighborhood', 'nominal', 'TBD', 'Neighborhood in which the property is located']\n",
    ",['regionidzip', 'nominal', 'TBD', 'Zip code in which the property is located']\n",
    ",['roomcnt', 'ordinal', 'TBD', 'Total number of rooms in the principal residence']\n",
    ",['storytypeid', 'nominal', 'TBD', 'Type of floors in a multi-story house (i.e. basement and main level, split-level, attic, etc.). See tab for details.']\n",
    ",['structuretaxvaluedollarcnt', 'ratio', 'TBD', 'The assessed value of the built structure on the parcel']\n",
    ",['taxamount', 'ratio', 'TBD', 'The total property tax assessed for that assessment year']\n",
    ",['taxdelinquencyflag', 'nominal', 'TBD', 'Property taxes for this parcel are past due as of 2015']\n",
    ",['taxdelinquencyyear', 'interval', 'TBD', 'Year']\n",
    ",['taxvaluedollarcnt', 'ratio', 'TBD', 'The total tax assessed value of the parcel']\n",
    ",['threequarterbathnbr', 'ordinal', 'TBD', 'Number of 3/4 bathrooms in house (shower + sink + toilet)']\n",
    ",['transactiondate', 'nominal', 'TBD', 'Date of the transaction response variable']    \n",
    ",['typeconstructiontypeid', 'nominal', 'TBD', 'What type of construction material was used to construct the home']\n",
    ",['unitcnt', 'ordinal', 'TBD', 'Number of units the structure is built into (i.e. 2 = duplex, 3 = triplex, etc...)']\n",
    ",['yardbuildingsqft17', 'interval', 'TBD', 'Patio in yard']\n",
    ",['yardbuildingsqft26', 'interval', 'TBD', 'Storage shed/building in yard']\n",
    ",['yearbuilt', 'interval', 'TBD', 'The Year the principal residence was built']\n",
    "]\n",
    "variables = pd.DataFrame(variables_description, columns=['name', 'type', 'scale','description'])\n",
    "variables = variables.set_index('name')\n",
    "\n",
    "def output_variables_table(variables, dataset):\n",
    "    variables = variables.sort_index()\n",
    "    rows = ['<tr><th>Variable</th><th>Type</th><th>Scale</th><th>Meaning</th></tr>']\n",
    "    for vname, atts in variables.iterrows():\n",
    "        if vname not in dataset.columns:\n",
    "            continue\n",
    "        atts = atts.to_dict()\n",
    "        # add scale if TBD\n",
    "        if atts['scale'] == 'TBD':\n",
    "            if atts['type'] in ['nominal', 'ordinal']:\n",
    "                uniques = dataset[vname].unique()\n",
    "                uniques = list(uniques.astype(str))\n",
    "                if len(uniques) < 10:\n",
    "                    atts['scale'] = '[%s]' % ', '.join(uniques)\n",
    "                else:\n",
    "                    atts['scale'] = '[%s]' % (', '.join(uniques[:5]) + ', ... (%d More)' % len(uniques))\n",
    "            if atts['type'] in ['ratio', 'interval']:\n",
    "                atts['scale'] = '(%d, %d)' % (dataset[vname].min(), dataset[vname].max())\n",
    "        row = (vname, atts['type'], atts['scale'], atts['description'])\n",
    "        rows.append('<tr><td>%s</td><td>%s</td><td>%s</td><td>%s</td></tr>' % row)\n",
    "    return HTML('<table>%s</table>' % ''.join(rows))\n",
    "\n",
    "def fix_outliers(data, column):\n",
    "    \"\"\"\n",
    "        An utility function used to find and fix outliers\n",
    "    \"\"\"\n",
    "    mean = data[column].mean()\n",
    "    std = data[column].std()\n",
    "    max_value = mean + std * 5\n",
    "    min_value = mean - std * 5\n",
    "    \n",
    "    if data[column].max() < max_value and data[column].min() > min_value:\n",
    "        print('No outliers found')\n",
    "        return\n",
    "    \n",
    "    print('Outliers found!')\n",
    "    \n",
    "    f, ((ax0, ax1), (ax2, ax3)) = plt.subplots(nrows=2, ncols=2, figsize=[15, 7])\n",
    "    \n",
    "    f.subplots_adjust(hspace=.4)\n",
    "    \n",
    "    sns.boxplot(data[column].dropna(), ax=ax0, color=\"#34495e\").set_title('Before')\n",
    "    sns.distplot(data[column].dropna(), ax=ax2, color=\"#34495e\").set_title('Before')\n",
    "\n",
    "    data.loc[data[column] > max_value, column] = max_value\n",
    "    data.loc[data[column] < min_value, column] = min_value\n",
    "    \n",
    "    sns.boxplot(data[column].dropna(), ax=ax1, color=\"#34495e\").set_title('After')\n",
    "    sns.distplot(data[column].dropna(), ax=ax3, color=\"#34495e\").set_title('After')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset file\n",
    "\n",
    "You can skip this and go directly to checkpoint to load a clean dataset\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datafilename = '../../input/properties_2016.csv'\n",
    "\n",
    "if not os.path.exists(datafilename):\n",
    "    raise Exception('Download properties_2016.csv.zip file from https://www.kaggle.com/c/zillow-prize-1/data and extract it into the input foulder')\n",
    "\n",
    "dataset = pd.read_csv(datafilename, low_memory=False)\n",
    "variables = variables.loc[dataset.columns]\n",
    "'The dataset has %d rows and %d columns' % dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Business\"></a>\n",
    "# Business Understanding\n",
    "<b>10 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "Describe the purpose of the data set you selected (i.e., why was this data collected in the first place?). How will you measure the effectiveness of a good algorithm? Why does your chosen validation method make sense for this specific\n",
    "dataset and the stakeholders needs?\n",
    "</i>\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Description\"></a>\n",
    "# Dataset Description\n",
    "<b>10 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "Describe the meaning and type of data (scale, values, etc.) for each attribute in the data file. Verify data quality: Are there missing values? Duplicate data? Outliers? Are those mistakes? How do you deal with these problems?\n",
    "</i>\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO update percent missing for every variable in the cleaning section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Description_1\"></a>\n",
    "## Meaning and type of data for each attribute in the data file before data cleaning\n",
    "<br/>\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_variables_table(variables, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Description_2\"></a>\n",
    "## Verification and description of data quality\n",
    "\n",
    "<a href=\"#top\">\n",
    "\n",
    "⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining Distribution of Missing Values\n",
    "From the observations, most of the rows have about 30 missing values. For the observations that have 57 missing values, it means that most of the features are missing and we choose to remove those. We will add in values to those missing where appropriate, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 7]\n",
    "\n",
    "number_missing_per_row = dataset.isnull().sum(axis=1)\n",
    "sns.distplot(number_missing_per_row, color=\"#34495e\", kde=False);\n",
    "plt.title('Distribution of Missing Values', fontsize=15)\n",
    "plt.xlabel('Number of Missing Values', fontsize=15)\n",
    "plt.ylabel('Number of Rows', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All observations have a value for parcelid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset['parcelid'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 percent of the data has only parcelid present and all other variables missing\n",
    "We choose to remove those observations because they don't present any value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(round(len(number_missing_per_row[number_missing_per_row >= 57]) / len(dataset) * 100, 2),\n",
    "      'percent of the data has no data features outside of parcelid and will be removed')\n",
    "dataset = dataset[number_missing_per_row < 57]\n",
    "print('The dataset now has %d rows' % len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Missing Values\n",
    "\n",
    "Of the available variables, here is a table that describes the number of missing values as well as the percent missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_values = dataset.isnull().sum().reset_index()\n",
    "missing_values.columns = ['Variable Name', 'Number Missing Values']\n",
    "missing_values['Percent Missing'] = missing_values['Number Missing Values'] / len(dataset) * 100\n",
    "missing_values['Percent Missing'] = missing_values['Percent Missing'].replace(np.inf, 0)\n",
    "missing_values.set_index('Variable Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Description_3\"></a>\n",
    "## Cleaning process\n",
    "\n",
    "For variables that are nominal, ratio, and interval where appropriate, we wrote a function that replaces outliers 5 standard deviations from the mean and assigning them as 5 standard deviations above or below the mean, respectively.\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: airconditioningtypeid - Type of cooling system present in the home (if any)\n",
    "Has datatype: nominal and 72.710860 percent of values missing \n",
    "\n",
    "For this variable, missing values indicate the absence of a cooling system.  We replace all missing values with 0 to represent no cooling system. We changed the column datatype to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Before', dataset['airconditioningtypeid'].unique())\n",
    "dataset['airconditioningtypeid'] = dataset['airconditioningtypeid'].fillna(0).astype(np.int32)\n",
    "print('After', dataset['airconditioningtypeid'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: architecturalstyletypeid - Architectural style of the home (i.e. ranch, colonial, split-level, etc…)\n",
    "Has datatype: nominal and 99.796185 percent of values missing \n",
    "\n",
    "Architectural style describes the home design. As such, it is not something we can extrapolate a value for. With over 99% of values missing, we decided to eliminate this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del dataset['architecturalstyletypeid']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: assessmentyear - year of the property tax assessment\n",
    "\n",
    "Has datatype: interval and has 2 values missing\n",
    "\n",
    "We replaced the missing values with the latest tax year which also happens to be the median tax year. We changed the column datatype to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Before', dataset['assessmentyear'].unique()[:8].tolist() + ['...'])\n",
    "median_value = dataset['assessmentyear'].median()\n",
    "dataset['assessmentyear'] = dataset['assessmentyear'].fillna(median_value).astype(np.int32)\n",
    "print('After', dataset['assessmentyear'].unique()[:8].tolist() + ['...'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: basementsqft - Finished living area below or partially below ground level\n",
    "Has datatype: ratio and 99.945255 percent of values missing\n",
    "\n",
    "Basements are not standard home features. Whenever a basement is not a feature of the home, the value for area was entered as a missing value. With over 99% of values missing, we decided to eliminate this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del dataset['basementsqft'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: bathroomcnt - Number of bathrooms in home including fractional bathrooms\n",
    "Has datatype: ordinal and 0.000841 percent of values missing\n",
    "\n",
    "We decided it is potentially possible for the property to not have a bathroom so we decided to replace missing values with zeros since there are only very few. We changed the column datatype to a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Before', dataset['bathroomcnt'].unique()[:8].tolist() + ['...'])\n",
    "dataset['bathroomcnt'] = dataset['bathroomcnt'].fillna(0).astype(np.float32)\n",
    "print('After', dataset['bathroomcnt'].unique()[:8].tolist() + ['...'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: bedroomcnt - Number of bedrooms in home\n",
    "Has datatype: ordinal and 0.000437 percent of values missing\n",
    "\n",
    "We decided to replace missing values with zeros since there are only very few to represent a studio apartment. We changed the column datatype to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Before', dataset['bedroomcnt'].unique()[:8].tolist() + ['...'])\n",
    "dataset['bedroomcnt'] = dataset['bedroomcnt'].fillna(0).astype(np.int32)\n",
    "print('After', dataset['bedroomcnt'].unique()[:8].tolist() + ['...'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: buildingclasstypeid - The building framing type (steel frame, wood frame, concrete/brick)\n",
    "Has datatype: nominal and 99.576949 percent of values missing\n",
    "\n",
    "With this much missing values and the difficulty of assigning a building framing type, we decided to remove this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del dataset['buildingclasstypeid']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: buildingqualitytypeid - Overall assessment of condition of the building from best (lowest) to worst (highest)\n",
    "Has datatype: ordinal and 34.81 percent of values missing\n",
    "\n",
    "We chose to replace the missing values with the median of the condition assessment instead of giving the missing values the best or worst value. We changed the column datatype to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Before', dataset['buildingqualitytypeid'].unique())\n",
    "medianQuality = dataset['buildingqualitytypeid'].median()\n",
    "dataset['buildingqualitytypeid'] = dataset['buildingqualitytypeid'].fillna(medianQuality).astype(np.int32)\n",
    "print('After', dataset['buildingqualitytypeid'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: calculatedbathnbr - Number of bathrooms in home including fractional bathroom\n",
    "Has datatype: ordinal and 3.95 percent of values missing\n",
    "\n",
    "With a low number of missing values, we decided to assign 0 to all missing values since we decided above it is possible that a property could have 0 bathrooms. We changed the column datatype to a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Before', dataset['calculatedbathnbr'].unique()[:8].tolist() + ['...'])\n",
    "dataset['calculatedbathnbr'] = dataset['calculatedbathnbr'].fillna(0).astype(np.float32)\n",
    "print('After', dataset['calculatedbathnbr'].unique()[:8].tolist() + ['...'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: calculatedfinishedsquarefeet - Calculated total finished living area of the home\n",
    "Has datatype: ratio and 1.48 percent of values missing\n",
    "\n",
    "These missing values appear to be consistent with 0 or missing values for variables associated with a building or structure on the property such as bathroomcnt, bedroomcnt, or architecturalstyletypeid. We can assume that no structures exist on these properties and we decided to impute zeros to these. We changed the column datatype to integer. We then replaced all outliers with a maximum and minimum value of (mean ± 5 * std), respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fix_outliers(dataset, 'calculatedfinishedsquarefeet')\n",
    "\n",
    "print('Before', dataset['calculatedfinishedsquarefeet'].unique()[:8].tolist() + ['...'])\n",
    "dataset['calculatedfinishedsquarefeet'] = dataset['calculatedfinishedsquarefeet'].fillna(0).astype(np.int32)\n",
    "print('After', dataset['calculatedfinishedsquarefeet'].unique()[:8].tolist() + ['...'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: censustractandblock - census tract and census block ID\n",
    "\n",
    "Has datatype: nominal and 2.14 percent of values missing\n",
    "\n",
    "With such a small amount of missing values, we decided to replace them with the median.  A better approach in the future could be taking into account zip code and then median for the missing values.  We changed the column datatype to a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Before', dataset['censustractandblock'].unique()[:5].tolist() + ['...'])\n",
    "median_value = dataset['censustractandblock'].median()\n",
    "dataset['censustractandblock'] = dataset['censustractandblock'].fillna(median_value)\n",
    "dataset['censustractandblock'] = dataset['censustractandblock'].astype(np.float32)\n",
    "print('After', dataset['censustractandblock'].unique()[:5].tolist() + ['...'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: decktypeid - Type of deck (if any) present on parcel\n",
    "Has datatype: nominal and 99.427311 percent of values missing\n",
    "\n",
    "Missing values is most likely an indication of an absence of this feature in the property. With 99% missing values, we will remove this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "del dataset['decktypeid']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: finishedfloor1squarefeet - Size of the finished living area on the first (entry) floor of the home\n",
    "Has datatype: ratio and 93.18 percent of values missing\n",
    "\n",
    "Having this much missing values and the availability of an alternate variable - calculatedfinishedsquarefeet - with very few missing values, we decided to eliminate this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del dataset['finishedfloor1squarefeet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: finishedsquarefeet12 - Finished living area\n",
    "Has datatype: ratio and 8.89 percent of values missing\n",
    "\n",
    "The finishedsquarefeet fields add up to the calculatedfinishedsquarefeet. Missing values are therefore zeros. We changed the column datatype to integer. We then replaced all outliers with a maximum and minimum value of (mean ± 5 * std), respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fix_outliers(dataset, 'finishedsquarefeet12')\n",
    "\n",
    "print('Before', dataset['finishedsquarefeet12'].unique())\n",
    "dataset['finishedsquarefeet12'] = dataset['finishedsquarefeet12'].fillna(0).astype(np.int32)\n",
    "print('After', dataset['finishedsquarefeet12'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: finishedsquarefeet13 - Finished living area\n",
    "Has datatype: ratio and 99.743000 percent of values missing\n",
    "\n",
    "The finishedsquarefeet fields add up to the calculatedfinishedsquarefeet. Since there are 99% missing values we will remove this from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del dataset['finishedsquarefeet13']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: finishedsquarefeet15 - Total area\n",
    "Has datatype: ratio and 93.58 percent of values missing\n",
    "\n",
    "The finishedsquarefeet fields add up to the calculatedfinishedsquarefeet. Since there are 93% missing values we will remove this from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del dataset['finishedsquarefeet15']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: finishedsquarefeet50 - Size of the finished living area on the first (entry) floor of the home\n",
    "Has datatype: ratio and 93.18 percent of values missing\n",
    "\n",
    "The finishedsquarefeet fields add up to the calculatedfinishedsquarefeet. Since there are 93% missing values we will replace the missing values with 0. We changed the column datatype to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset['finishedsquarefeet50'] = dataset['finishedsquarefeet50'].fillna(0).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: finishedsquarefeet6 - Base unfinished and finished area\n",
    "Has datatype: ratio and 99.26 percent of values missing\n",
    "\n",
    "With 99% missing values, we decided to delete this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del dataset['finishedsquarefeet6']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: fips - Federal Information Processing Standard code - see https://en.wikipedia.org/wiki/FIPS_county_code for more details\n",
    "Has datatype: nominal with values [6037.0, 6059.0, 6111.0] and no missing values\n",
    "\n",
    "We changed the column datatype to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset['fips'] = dataset['fips'].astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: fireplacecnt - Number of fireplaces in a home (if any)\n",
    "Has datatype: ordinal and 89.486882 percent of values missing \n",
    "\n",
    "In this dataset, missing value represents 0 fireplaces. We replaced all missing values with zero and change the column datatype to integer. We changed the column datatype to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Before', dataset['fireplacecnt'].unique())\n",
    "dataset['fireplacecnt'] = dataset['fireplacecnt'].fillna(0).astype(np.int32)\n",
    "print('After', dataset['fireplacecnt'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: fireplaceflag - does the home have a fireplace\n",
    "\n",
    "Has datatype: ordinal and 99.82 percent of values missing\n",
    "\n",
    "With 99% missing values, we decided to delete the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del dataset['fireplaceflag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: fullbathcnt - Number of full bathrooms (sink, shower + bathtub, and toilet) present in home\n",
    "Has datatype: ordinal and 3.95 percent of values missing \n",
    "\n",
    "We first replaced its missing values with the values of bathroomcnt which is a similar measure. After that, we have 25 observations missing and we replace them with 0. We changed the column datatype to a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Before', dataset['fullbathcnt'].unique()[:8].tolist() + ['...'])\n",
    "missing_fullbathcnt = dataset['fullbathcnt'].isnull()\n",
    "dataset.loc[missing_fullbathcnt, 'fullbathcnt'] = dataset['bathroomcnt'][missing_fullbathcnt].fillna(0)\n",
    "dataset['fullbathcnt'] = dataset['fullbathcnt'].astype(np.float32)\n",
    "print('After', dataset['fullbathcnt'].unique()[:8].tolist() + ['...'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: garagecarcnt - Total number of garages on the lot including an attached garage\n",
    "\n",
    "Has datatype: ordinal and 70.298173 percent of values missing \n",
    "\n",
    "We assume that missing values will represent no garage and replace all missing values with zero. We changed the column datatype to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset['garagecarcnt'] = dataset['garagecarcnt'].fillna(0).astype(np.int32)\n",
    "print(dataset['garagecarcnt'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: garagetotalsqft - Total number of garages on the lot including an attached garage\n",
    "\n",
    "Has datatype: ratio and 70.298173 percent of values missing \n",
    "\n",
    "We first replaced missing values where garagecarcnt is 0 with 0 garagetotalsqft. We changed the column datatype to a float.\n",
    "We then replaced all outliers with a maximum and minimum value of (mean ± 5 * std), respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fix_outliers(dataset, 'garagetotalsqft')\n",
    "\n",
    "dataset.loc[dataset['garagecarcnt'] == 0, 'garagetotalsqft'] = 0\n",
    "dataset['garagecarcnt'] = dataset['garagecarcnt'].astype(np.float32)\n",
    "assert dataset['garagetotalsqft'].isnull().sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: hashottuborspa - Does the home have a hot tub or spa\n",
    "\n",
    "Has datatype: ordinal and 97.679250 percent of values missing \n",
    "\n",
    "In this dataset missing values represent doesn't have a hot tub or spa. we replaced all missing values with 0 and all True values with 1. We changed the column datatype to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Before', dataset['hashottuborspa'].unique())\n",
    "dataset['hashottuborspa'] = dataset['hashottuborspa'].fillna(0).replace('True', 1).astype(np.int32)\n",
    "print('After', dataset['hashottuborspa'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: heatingorsystemtypeid - Type of home heating system\n",
    "\n",
    "Has datatype: nominal and 39.255728 percent of values missing \n",
    "\n",
    "We replaced all missing values with 0 which will represent a missing heating system type id. We changed the column datatype to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Before', dataset['heatingorsystemtypeid'].unique()[:8].tolist() + ['...'])\n",
    "dataset['heatingorsystemtypeid'] = dataset['heatingorsystemtypeid'].fillna(0).astype(np.int32)\n",
    "print('After', dataset['heatingorsystemtypeid'].unique()[:8].tolist() + ['...'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: landtaxvaluedollarcnt - the assessed value of the land\n",
    "\n",
    "Has datatype: ratio and 1.89 percent of values missing\n",
    "\n",
    "We replaced all missing values with the median assessed land values. We changed the column datatype to integer. We then replaced all outliers with a maximum and minimum value of (mean ± 5 * std), respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fix_outliers(dataset, 'landtaxvaluedollarcnt')\n",
    "median_value = dataset['landtaxvaluedollarcnt'].median()\n",
    "dataset['landtaxvaluedollarcnt'] = dataset['landtaxvaluedollarcnt'].fillna(median_value).astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables: latitude and longitude\n",
    "\n",
    "Has datatype: interval and no missing values. We changed the column datatype to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset[['latitude', 'longitude']] = dataset[['latitude', 'longitude']].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: lotsizesquarefeet - Area of the lot in square feet\n",
    "\n",
    "Has datatype: ratio and 8.9 percent of values missing \n",
    "\n",
    "We replace all missing values with 0 which will represent no lot.\n",
    "We changed the column datatype to integer. We then replaced all outliers with a maximum and minimum value of (mean ± 5 * std), respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fix_outliers(dataset, 'lotsizesquarefeet')\n",
    "dataset['lotsizesquarefeet'] = dataset['lotsizesquarefeet'].fillna(0).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: numberofstories - number of stories or levels the home has\n",
    "\n",
    "Has datatype: ordinal and 77.06 percent of values missing\n",
    "\n",
    "We replace all missing values with 1 after removing all outliers to represent a single story home. We changed the column datatype to integer. We then replaced all outliers with a maximum and minimum value of (mean ± 5 * std), respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fix_outliers(dataset, 'numberofstories')\n",
    "\n",
    "print('Before', dataset['numberofstories'].unique()[:8].tolist() + ['...'])\n",
    "dataset['numberofstories'] = dataset['numberofstories'].fillna(1).astype(np.int32)\n",
    "print('After', dataset['numberofstories'].unique()[:8].tolist() + ['...'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: parcelid - Unique identifier for parcels (lots)\n",
    "Has datatype: nominal and no values missing. We changed the column datatype to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset['parcelid'] = dataset['parcelid'].astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: poolcnt - Number of pools on the lot (if any)\n",
    "\n",
    "Has datatype: ordinal and 82.6 percent of values missing \n",
    "\n",
    "We replaced all missing values with 0 which will represent no pools. We changed the column datatype to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Before', dataset['poolcnt'].unique())\n",
    "dataset['poolcnt'] = dataset['poolcnt'].fillna(0).astype(np.int32)\n",
    "print('After', dataset['poolcnt'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: poolsizesum - Total square footage of all pools on property\n",
    "\n",
    "Has datatype: ratio and 99 percent of values missing \n",
    "\n",
    "We replaced all missing values with 0 if number of pools is 0 or with the average poolsizesum otherwise. We changed the column datatype to a float. We then replaced all outliers with a maximum and minimum value of (mean ± 5 * std), respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fix_outliers(dataset, 'poolsizesum')\n",
    "dataset.loc[dataset['poolsizesum'].isnull(), 'poolsizesum'] = int(dataset['poolsizesum'].mean())\n",
    "dataset.loc[dataset['poolcnt'] == 0, 'poolsizesum'] = 0\n",
    "dataset['poolcnt'] = dataset['poolcnt'].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: pooltypeid10 - Spa or Hot Tub\n",
    "\n",
    "Has datatype: nominal and 98.8 percent of values missing \n",
    "\n",
    "We replaced all missing values with 0 which will represent no Spa or Hot Tub. We changed the column datatype to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Before', dataset['pooltypeid10'].unique())\n",
    "dataset['pooltypeid10'] = dataset['pooltypeid10'].fillna(0).astype(np.int32)\n",
    "print('After', dataset['pooltypeid10'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: pooltypeid2 - Pool with Spa/Hot Tub\n",
    "\n",
    "Has datatype: nominal and 98.9 percent of values missing \n",
    "\n",
    "We replaced all missing values with 0 which will represent no Pool with Spa/Hot Tub. We changed the column datatype to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Before', dataset['pooltypeid2'].unique())\n",
    "dataset['pooltypeid2'] = dataset['pooltypeid2'].fillna(0).astype(np.int32)\n",
    "print('After', dataset['pooltypeid2'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: pooltypeid7 - Pool without hot tub\n",
    "\n",
    "Has datatype: nominal and 83.6 percent of values missing \n",
    "\n",
    "We replaced all missing values with 0 which will represent no pool without hot tub. We changed the column datatype to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Before', dataset['pooltypeid7'].unique())\n",
    "dataset['pooltypeid7'] = dataset['pooltypeid7'].fillna(0).astype(np.int32)\n",
    "print('After', dataset['pooltypeid7'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: propertycountylandusecode - County land use code i.e. it's zoning at the county level\n",
    "\n",
    "Has datatype: nominal and 0.02 percent of values missing \n",
    "\n",
    "We replaced all missing values with 0 which will represent no county land use code. We changed the column datatype to string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Before', dataset['propertycountylandusecode'].unique()[:8].tolist() + ['...'])\n",
    "dataset['propertycountylandusecode'] = dataset['propertycountylandusecode'].fillna(0).astype(np.str)\n",
    "print('After', dataset['propertycountylandusecode'].unique()[:8].tolist() + ['...'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: propertylandusetypeid - Type of land use the property is zoned for\n",
    "\n",
    "Has datatype: nominal and 0 percent of values missing.\n",
    "\n",
    "We are just changing the datatype to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset['propertylandusetypeid'] = dataset['propertylandusetypeid'].astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: propertyzoningdesc - Description of the allowed land uses (zoning) for that property\n",
    "\n",
    "Has datatype: nominal and 33.4 percent of values missing \n",
    "\n",
    "We replaced all missing values with 0 which will represent no description of the allowed land uses.  We changed the column datatype to string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Before', dataset['propertyzoningdesc'].unique())\n",
    "dataset['propertyzoningdesc'] = dataset['propertyzoningdesc'].fillna(0).astype(np.str)\n",
    "print('After', dataset['propertyzoningdesc'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: rawcensustractandblock - Census tract and block ID combined - also contains blockgroup assignment by extension\n",
    "\n",
    "Has datatype: nominal and 0 percent of values missing\n",
    "\n",
    "We are just changing the datatype to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Before', dataset['rawcensustractandblock'].unique()[:5].tolist() + ['...'])\n",
    "dataset['rawcensustractandblock'] = dataset['rawcensustractandblock'].fillna(0).astype(np.int32)\n",
    "print('After', dataset['rawcensustractandblock'].unique()[:5].tolist() + ['...'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: regionidcity - City in which the property is located (if any)\n",
    "\n",
    "Has datatype: nominal and 1.72 percent of values missing\n",
    "\n",
    "we will replace any missing values with 0 to represent no city ID. We are just changing the datatype to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Before', dataset['regionidcity'].unique()[:8].tolist() + ['...'])\n",
    "dataset['regionidcity'] = dataset['regionidcity'].fillna(0).astype(np.int32)\n",
    "print('After', dataset['regionidcity'].unique()[:8].tolist() + ['...'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: regionidcounty - County in which the property is located\n",
    "\n",
    "Has datatype: nominal and 0 percent of values missing. We changed the column datatype to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Before', dataset['regionidcounty'].unique())\n",
    "dataset['regionidcounty'] = dataset['regionidcounty'].astype(np.int32)\n",
    "print('After', dataset['regionidcounty'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: regionidneighborhood - Neighborhood in which the property is located\n",
    "\n",
    "Has datatype: nominal and 61.1 percent of values missing \n",
    "\n",
    "We replaced all missing values with 0 which will represent no region ID neighborhood. We changed the column datatype to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Before', dataset['regionidneighborhood'].unique()[:8].tolist() + ['...'])\n",
    "dataset['regionidneighborhood'] = dataset['regionidneighborhood'].fillna(0).astype(np.int32)\n",
    "print('After', dataset['regionidneighborhood'].unique()[:8].tolist() + ['...'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: regionidzip - Zip code in which the property is located\n",
    "\n",
    "Has datatype: nominal and 0.08 percent of values missing \n",
    "\n",
    "We replaced all missing values with 0 which will represent no zip code. We changed the column datatype to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Before', dataset['regionidzip'].unique()[:8].tolist() + ['...'])\n",
    "dataset['regionidzip'] = dataset['regionidzip'].fillna(0).astype(np.int32)\n",
    "print('After', dataset['regionidzip'].unique()[:8].tolist() + ['...'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: roomcnt - Total number of rooms in the principal residence\n",
    "\n",
    "Has datatype: nominal and 0.001 percent of values missing \n",
    "\n",
    "We replaced all missing values with 1 which will represent no Total number of rooms in the principal residence reported. We changed the column datatype to integer. We then replaced all outliers with a maximum and minimum value of (mean ± 5 * std), respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fix_outliers(dataset, 'roomcnt')\n",
    "\n",
    "print('Before', dataset['roomcnt'].unique())\n",
    "dataset['roomcnt'] = dataset['roomcnt'].fillna(1).astype(np.int32)\n",
    "print('After', dataset['roomcnt'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: storytypeid - Type of floors in a multi-story house (i.e. basement and main level, split-level, attic, etc.). See tab for details.\n",
    "\n",
    "Has datatype: nominal and 99.9 percent of values missing \n",
    "\n",
    "With 99% missing values, we decided to remove this variable.\n",
    "\n",
    "# TODO investigate why this is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "del dataset['storytypeid']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: structuretaxvaluedollarcnt - the assessed value of the building\n",
    "\n",
    "Has datatype: ratio and 1.46 percent of values missing\n",
    "\n",
    "We replaced all missing values with the median assessed building tax. We changed the column datatype to integer. We then replaced all outliers with a maximum and minimum value of (mean ± 5 * std), respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fix_outliers(dataset, 'structuretaxvaluedollarcnt')\n",
    "\n",
    "medTax = np.nanmedian(dataset['structuretaxvaluedollarcnt'])\n",
    "\n",
    "dataset['structuretaxvaluedollarcnt'] = dataset['structuretaxvaluedollarcnt'].fillna(medTax).astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: taxamount - property tax for the assessment year\n",
    "\n",
    "Has datatype: ratio and 0.66 percent of values missing\n",
    "\n",
    "We replaced all missing values with the median property taxes for the assessment year. We changed the column datatype to integer. We then replaced all outliers with a maximum and minimum value of (mean ± 5 * std), respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fix_outliers(dataset, 'taxamount')\n",
    "median_value = dataset['taxamount'].median()\n",
    "dataset['taxamount'] = dataset['taxamount'].fillna(median_value).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: taxdelinquencyflag - property taxes from 2015 that are past due\n",
    "\n",
    "Has datatype: nominal and 98.10 percent of values missing\n",
    "\n",
    "We replaced all missing values with 0 representing no past due property taxes and all Y values with 1 representing that there are past due property taxes. We changed the column datatype to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Before', dataset['taxdelinquencyflag'].unique())\n",
    "dataset['taxdelinquencyflag'] = dataset['taxdelinquencyflag'].fillna(0).replace('Y', 1).astype(np.int32)\n",
    "print('After', dataset['taxdelinquencyflag'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: taxdelinquencyyear - years of delinquency\n",
    "\n",
    "Has datatype: interval and 98.10 percent of values missing\n",
    "\n",
    "We replaced all missing values with 0 representing no years of property tax delinquencies. We changed the column datatype to integer. We then replaced all outliers with a maximum and minimum value of (mean ± 5 * std), respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fix_outliers(dataset, 'taxdelinquencyyear')\n",
    "dataset['taxdelinquencyyear'] = dataset['taxdelinquencyyear'].fillna(0).astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: taxvaluedollarcnt - total tax \n",
    "\n",
    "Has datatype: ratio and 1.04 percent of values missing\n",
    "\n",
    "We replaced all missing values with the median total tax amount. We changed the column datatype to integer. We then replaced all outliers with a maximum and minimum value of (mean ± 5 * std), respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fix_outliers(dataset, 'taxvaluedollarcnt')\n",
    "median_value = dataset['taxvaluedollarcnt'].median()\n",
    "dataset['taxvaluedollarcnt'] = dataset['taxvaluedollarcnt'].fillna(median_value).astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: threequarterbathnbr - Number of 3/4 bathrooms in house (shower + sink + toilet)\n",
    "\n",
    "Has datatype: ordinal and 89.5 percent of values missing \n",
    "\n",
    "We replaced all missing values with 0 which will represent no Number of 3/4 bathrooms in the property. We changed the column datatype to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Before', dataset['threequarterbathnbr'].unique())\n",
    "dataset['threequarterbathnbr'] = dataset['threequarterbathnbr'].fillna(0).astype(np.int32)\n",
    "print('After', dataset['threequarterbathnbr'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: typeconstructiontypeid - What type of construction material was used to construct the home\n",
    "\n",
    "Has datatype: nominal and 99.7 percent of values missing \n",
    "\n",
    "With 99% missing values, we decided to remove this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del dataset['typeconstructiontypeid']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: unitcnt - number of units in the building\n",
    "\n",
    "Has datatype: ordinal and 33.5 percent of values missing\n",
    "\n",
    "We replaced all missing values with 1 to represent a single family home for any with no values.  We changed the column datatype to integer. We then replaced all outliers with a maximum and minimum value of (mean ± 5 * std), respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fix_outliers(dataset, 'unitcnt')\n",
    "\n",
    "print('Before', dataset['unitcnt'].unique()[:8].tolist() + ['...'])\n",
    "dataset['unitcnt'] = dataset['unitcnt'].fillna(1).astype(np.int32)\n",
    "print('After', dataset['unitcnt'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: yardbuildingsqft17 - sq feet of patio in yard\n",
    "\n",
    "Has datatype: interval and 97.29 percent of values missing\n",
    "\n",
    "We replaced all missing values with 0 representing no patio. We changed the column datatype to integer. We then replaced all outliers with a maximum and minimum value of (mean ± 5 * std), respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fix_outliers(dataset, 'yardbuildingsqft17')\n",
    "dataset['yardbuildingsqft17'] = dataset['yardbuildingsqft17'].fillna(0).astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: yardbuildingsqft26 - storage shed/building in yard\n",
    "\n",
    "Has datatype: interval and 99.91 percent of values missing \n",
    "\n",
    "We replaced all missing values with 0 which will represent no (square ft) storage shed or building in the yard. We changed the column datatype to integer. We then replaced all outliers with a maximum and minimum value of (mean ± 5 * std), respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fix_outliers(dataset, 'yardbuildingsqft26')\n",
    "dataset['yardbuildingsqft26'] = dataset['yardbuildingsqft26'].fillna(0).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: yearbuilt - The Year the residence was built\n",
    "\n",
    "Has datatype: interval and 1.63 percent of values missing \n",
    "\n",
    "We replaced all missing values with the median year built of 1963 until we have a better method to impute. We changed the column datatype to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Before', dataset['yearbuilt'].unique()[:8].tolist() + ['...'])\n",
    "medYear = dataset['yearbuilt'].median()\n",
    "dataset['yearbuilt'] = dataset['yearbuilt'].fillna(medYear).astype(np.int32)\n",
    "print('After', dataset['yearbuilt'].unique()[:8].tolist() + ['...'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Description_4\"></a>\n",
    "## Final verification of data quality\n",
    "\n",
    "We went through every variable and next cell will confirm that the dataset has no missing values.\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert np.all(~dataset.isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Description_5\"></a>\n",
    "## Meaning and type of data for each attribute in the data file after data cleaning\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_variables_table(variables, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Description_6\"></a>\n",
    "## Table of Binary Variables (0 or 1)\n",
    "We standardized all Yes/No and True/False variables to 1 or 0, respectively. The table below shows that all binary flags in this dataset represent rare features such a pool, hot tub, tax delinquency flag, and three quarter bathroom.\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bin_vars = ['hashottuborspa', 'poolcnt', 'pooltypeid2', 'pooltypeid7', 'pooltypeid10', 'taxdelinquencyflag', 'threequarterbathnbr']\n",
    "pd.DataFrame(dataset[bin_vars].mean() * 100, columns=['Percent with value equal to 1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"save_state\"></a>\n",
    "## Save dataset state\n",
    "\n",
    "We save the cleaned dataset in order to faster load.\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.to_csv('../../datasets/clean_dataset.csv', index=False)\n",
    "variables = variables.loc[dataset.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"restore_state\"></a>\n",
    "# Restore Cleaned Dataset [CHECKPOINT]\n",
    "\n",
    "Next cell will restore the cleanded dataset. You could run this cell instead of runing all above cleaning cells.\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../../datasets/clean_dataset.csv')\n",
    "variables = variables.loc[dataset.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Attribute\"></a>\n",
    "# Attribute Visualizion\n",
    "<b>10 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "Visualize the any important attributes appropriately. Important: Provide an interpretation for any charts or graphs.\n",
    "</i>\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>\n",
    "\n",
    "# TODO decide which visualization to keep and remove all others\n",
    "# TODO make the visualizations run\n",
    "# TODO add subcategories with links to the Contents menu on top for every visualization you do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../../input/train_2016_v2.csv')\n",
    "data = pd.read_csv('../../datasets/clean_dataset.csv', low_memory=False)\n",
    "data = pd.merge(data, train_data, how='left', on='parcelid')\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "variables_description = [\n",
    "['airconditioningtypeid', 'nominal', 'TBD', 'Type of cooling system present in the home (if any)']\n",
    ",['architecturalstyletypeid', 'nominal', 'TBD', 'Architectural style of the home (i.e. ranch, colonial, split-level, etc…)']\n",
    ",['assessmentyear', 'interval', 'TBD', 'The year of the property tax assessment']\n",
    ",['basementsqft', 'ratio', 'TBD', 'Finished living area below or partially below ground level']\n",
    ",['bathroomcnt', 'ordinal', 'TBD', 'Number of bathrooms in home including fractional bathrooms']\n",
    ",['bedroomcnt', 'ordinal', 'TBD', 'Number of bedrooms in home']\n",
    ",['buildingclasstypeid', 'nominal', 'TBD', 'The building framing type (steel frame, wood frame, concrete/brick)']\n",
    ",['buildingqualitytypeid', 'ordinal', 'TBD', 'Overall assessment of condition of the building from best (lowest) to worst (highest)']\n",
    ",['calculatedbathnbr', 'ordinal', 'TBD', 'Number of bathrooms in home including fractional bathroom']\n",
    ",['calculatedfinishedsquarefeet', 'ratio', 'TBD', 'Calculated total finished living area of the home']\n",
    ",['censustractandblock', 'nominal', 'TBD', 'Census tract and block ID combined - also contains blockgroup assignment by extension']\n",
    ",['decktypeid', 'nominal', 'TBD', 'Type of deck (if any) present on parcel']\n",
    ",['finishedfloor1squarefeet', 'ratio', 'TBD', 'Size of the finished living area on the first (entry) floor of the home']\n",
    ",['finishedsquarefeet12', 'ratio', 'TBD', 'Finished living area']\n",
    ",['finishedsquarefeet13', 'ratio', 'TBD', 'Perimeter living area']\n",
    ",['finishedsquarefeet15', 'ratio', 'TBD', 'Total area']\n",
    ",['finishedsquarefeet50', 'ratio', 'TBD', 'Size of the finished living area on the first (entry) floor of the home']\n",
    ",['finishedsquarefeet6', 'ratio', 'TBD', 'Base unfinished and finished area']\n",
    ",['fips', 'nominal', 'TBD', 'Federal Information Processing Standard code - see https://en.wikipedia.org/wiki/FIPS_county_code for more details']\n",
    ",['fireplacecnt', 'ordinal', 'TBD', 'Number of fireplaces in a home (if any)']\n",
    ",['fireplaceflag', 'ordinal', 'TBD', 'Is a fireplace present in this home']\n",
    ",['fullbathcnt', 'ordinal', 'TBD', 'Number of full bathrooms (sink, shower + bathtub, and toilet) present in home']\n",
    ",['garagecarcnt', 'ordinal', 'TBD', 'Total number of garages on the lot including an attached garage']\n",
    ",['garagetotalsqft', 'ratio', 'TBD', 'Total number of square feet of all garages on lot including an attached garage']\n",
    ",['hashottuborspa', 'ordinal', 'TBD', 'Does the home have a hot tub or spa']\n",
    ",['heatingorsystemtypeid', 'nominal', 'TBD', 'Type of home heating system']\n",
    ",['landtaxvaluedollarcnt', 'ratio', 'TBD', 'The assessed value of the land area of the parcel']\n",
    ",['latitude', 'interval', 'TBD', 'Latitude of the middle of the parcel multiplied by 10e6']\n",
    ",['logerror', 'interval', 'TBD', 'Error or the Zillow model response variable']\n",
    ",['longitude', 'interval', 'TBD', 'Longitude of the middle of the parcel multiplied by 10e6']\n",
    ",['lotsizesquarefeet', 'ratio', 'TBD', 'Area of the lot in square feet']\n",
    ",['numberofstories', 'ordinal', 'TBD', 'Number of stories or levels the home has']\n",
    ",['parcelid', 'nominal', 'TBD', 'Unique identifier for parcels (lots)']\n",
    ",['poolcnt', 'ordinal', 'TBD', 'Number of pools on the lot (if any)']\n",
    ",['poolsizesum', 'ratio', 'TBD', 'Total square footage of all pools on property']\n",
    ",['pooltypeid10', 'nominal', 'TBD', 'Spa or Hot Tub']\n",
    ",['pooltypeid2', 'nominal', 'TBD', 'Pool with Spa/Hot Tub']\n",
    ",['pooltypeid7', 'nominal', 'TBD', 'Pool without hot tub']\n",
    ",['propertycountylandusecode', 'nominal', 'TBD', 'County land use code i.e. it\\'s zoning at the county level']\n",
    ",['propertylandusetypeid', 'nominal', 'TBD', 'Type of land use the property is zoned for']\n",
    ",['propertyzoningdesc', 'nominal', 'TBD', 'Description of the allowed land uses (zoning) for that property']\n",
    ",['rawcensustractandblock', 'nominal', 'TBD', 'Census tract and block ID combined - also contains blockgroup assignment by extension']\n",
    ",['regionidcity', 'nominal', 'TBD', 'City in which the property is located (if any)']\n",
    ",['regionidcounty', 'nominal', 'TBD', 'County in which the property is located']\n",
    ",['regionidneighborhood', 'nominal', 'TBD', 'Neighborhood in which the property is located']\n",
    ",['regionidzip', 'nominal', 'TBD', 'Zip code in which the property is located']\n",
    ",['roomcnt', 'ordinal', 'TBD', 'Total number of rooms in the principal residence']\n",
    ",['storytypeid', 'nominal', 'TBD', 'Type of floors in a multi-story house (i.e. basement and main level, split-level, attic, etc.). See tab for details.']\n",
    ",['structuretaxvaluedollarcnt', 'ratio', 'TBD', 'The assessed value of the built structure on the parcel']\n",
    ",['taxamount', 'ratio', 'TBD', 'The total property tax assessed for that assessment year']\n",
    ",['taxdelinquencyflag', 'nominal', 'TBD', 'Property taxes for this parcel are past due as of 2015']\n",
    ",['taxdelinquencyyear', 'interval', 'TBD', 'Year']\n",
    ",['taxvaluedollarcnt', 'ratio', 'TBD', 'The total tax assessed value of the parcel']\n",
    ",['threequarterbathnbr', 'ordinal', 'TBD', 'Number of 3/4 bathrooms in house (shower + sink + toilet)']\n",
    ",['transactiondate', 'nominal', 'TBD', 'Date of the transaction response variable']    \n",
    ",['typeconstructiontypeid', 'nominal', 'TBD', 'What type of construction material was used to construct the home']\n",
    ",['unitcnt', 'ordinal', 'TBD', 'Number of units the structure is built into (i.e. 2 = duplex, 3 = triplex, etc...)']\n",
    ",['yardbuildingsqft17', 'interval', 'TBD', 'Patio in yard']\n",
    ",['yardbuildingsqft26', 'interval', 'TBD', 'Storage shed/building in yard']\n",
    ",['yearbuilt', 'interval', 'TBD', 'The Year the principal residence was built']\n",
    "]\n",
    "variables = pd.DataFrame(variables_description, columns=['name', 'type', 'scale','description'])\n",
    "variables = variables.set_index('name')\n",
    "variables = variables.loc[data.columns]\n",
    "\n",
    "def output_variables_table(variables):\n",
    "    variables = variables.sort_index()\n",
    "    rows = ['<tr><th>Variable</th><th>Type</th><th>Scale</th><th>Description</th></tr>']\n",
    "    for vname, atts in variables.iterrows():\n",
    "        atts = atts.to_dict()\n",
    "        # add scale if TBD\n",
    "        if atts['scale'] == 'TBD':\n",
    "            if atts['type'] in ['nominal', 'ordinal']:\n",
    "                uniques = data[vname].unique()\n",
    "                uniques = list(uniques.astype(str))\n",
    "                if len(uniques) < 10:\n",
    "                    atts['scale'] = '[%s]' % ', '.join(uniques)\n",
    "                else:\n",
    "                    atts['scale'] = '[%s]' % (', '.join(uniques[:5]) + ', ... (%d More)' % len(uniques))\n",
    "            if atts['type'] in ['ratio', 'interval']:\n",
    "                atts['scale'] = '(%d, %d)' % (data[vname].min(), data[vname].max())\n",
    "        row = (vname, atts['type'], atts['scale'], atts['description'])\n",
    "        rows.append('<tr><td>%s</td><td>%s</td><td>%s</td><td>%s</td></tr>' % row)\n",
    "    return HTML('<table>%s</table>' % ''.join(rows))\n",
    "\n",
    "output_variables_table(variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics of All Continuous Variables\n",
    "\n",
    "To make the table more readable, we converted all simple statistics of continuous variables to integers. We lose some precision but we get a better overview. For each variable, we have already accounted for outliers and standardized missing values. We can immediately see that 0 is the most common value for many of the variables. To explore further, we chose to visualize each variable that had non-zero 25% to 75% values in the form of a boxplot and histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_data = data[~data['logerror'].isnull()]\n",
    "\n",
    "continous_vars = variables[variables['type'].isin(['ratio', 'interval'])].index\n",
    "continous_vars = continous_vars[continous_vars.isin(data.columns)]\n",
    "continous_vars = continous_vars[~continous_vars.isin(['longitude', 'latitude'])]\n",
    "\n",
    "output_table = data[continous_vars].describe().T\n",
    "mode_range = data[continous_vars].mode().T\n",
    "mode_range.columns = ['mode']\n",
    "mode_range['range'] = data[continous_vars].max() - data[continous_vars].min()\n",
    "output_table = output_table.join(mode_range)\n",
    "output_table.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculated Finished Square Feet\n",
    "For calculated square feet, most values were 0 with a range from 0 to 10898 sqft.  Note that we removed outliers earlier while cleaning the data.  The median of 1561 was a little smaller than the mean of 1784 so we expect to see a slight right skew, which we do below.  What is interesting here is the peak at 0 of values and then another peak around 1600 to 1800.  We continue to have few properties with very large (higher than the 75th percentile of 2124) which is fairly normal for any area to have the middle class homes with few larger homes mixed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f, (ax0,ax2) = plt.subplots(nrows=2, ncols=1, figsize=[15, 7])\n",
    "sns.boxplot(data['calculatedfinishedsquarefeet'], ax=ax0, color=\"#34495e\").set_title('Finished square feet distribution');\n",
    "sns.distplot(data['calculatedfinishedsquarefeet'], ax=ax2, color=\"#34495e\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finished Living Area\n",
    "Similar to calculated finished square feet, finished living area had outliers which we already fixed above.  The range for finished living area is 0 to 6871 with 0 being the mode of the data.  The mean (1596) is about 100sqft larger than the median (1466) so they are relatively the same since the variance is 962.\n",
    "\n",
    "This variable is bimodal in with a large spike at 0 and another peak with a fairly normal distribution and long right tail at around 1400.\n",
    "\n",
    "We also see a slight spike at the very end of the tail of the dataset.  This means there were a lot of outliers that were set to the maximum (mean + 6 * std)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f, (ax0,ax2) = plt.subplots(nrows=2, ncols=1, figsize=[15, 7])\n",
    "sns.boxplot(data['finishedsquarefeet12'], ax=ax0, color=\"#34495e\").set_title('Finished living area distribution');\n",
    "sns.distplot(data['finishedsquarefeet12'], ax=ax2, color=\"#34495e\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lot Size Square Feet\n",
    "Lot size square feet has the largest range from 0 to 1,710,750 even after removing all outliers (mean + std * 5).  The mode for this variable is 0 so we see below a spike at 0 and a very long right tail.\n",
    "\n",
    "What is interesting with this variable is the large variance of 73796. The 25th to 75th percentile values are 5200 and 9243 respectively so we will skipped over the box plot and plotted the histogram below.\n",
    "\n",
    "In the histogram, we see a right skewed distribution which makes sense considering the mean is 19810 and the median is 6700 - again, with such a large variance it is difficult for the eye to see the difference.  The main takeaway here is the large number of 0s.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f, (ax0) = plt.subplots(nrows=1, ncols=1)\n",
    "sns.distplot(data['lotsizesquarefeet'], ax=ax0, color=\"#34495e\").set_title('Lot size distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Year Built\n",
    "The year the properties were built ranges from 1801 to 2015.  The mode and median of 1963 is only a year difference from the mean of 1964.  The distribution seems to be fairly normal with the peak in the early 1960s and dropping off on both sides.  We see a number of homes that were built before 1905 (the low whisker of the boxplot) which gives us a long left tail.  \n",
    "\n",
    "We see a few other spikes in homes built which could correlate to a number of other factors such as healthy economic growth, political backing on mortgages, or rises in population.  The baby boomers born early 1960 shows many houses being built and around the time they turned 18 more houses seem to have been built.   We see an apparent fall right before 2000 which could be the dot com burst and another drop in the housing burst of 2007. Because our data was collected in 2016, we expect to see fewer homes built the previous year.\n",
    "\n",
    "What will be interesting with this variable is how old a home has to be to begin to \"fall apart\" or need major renovations to the piping or foundation.  Will a home built in a certain year have many homes made from a faulty material that causes damages later on?  Will the Zestimate take into account the disclosures of a home that each sale price typically does?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f, (ax0,ax2) = plt.subplots(nrows=2, ncols=1)\n",
    "sns.boxplot(data['yearbuilt'].dropna(), ax=ax0, color=\"#34495e\").set_title('Year built distribution');\n",
    "sns.distplot(data['yearbuilt'].dropna(), ax=ax2, color=\"#34495e\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Tax Value\n",
    "The total tax value of the property ranges from 1 to 4,052,186.  The median of 306,086 is the same as the mode and a little smaller than the mean of 407,695 which is evident in the right skewed distribution below.  These values have already been adjusted for outliers which is why we see a slight spike at the maximum value for larger developments and unique mansions.\n",
    "\n",
    "The distribution is fairly similar to square feet above because the tax is calculated by value assessed * square feet. What is interesting to note here is the missing values for tax were replaced by the median (hence the median and mode being the same) where the square footage missing values were replaced with 0s (hence the 0 as the mode and second peak in the distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f, (ax0,ax2) = plt.subplots(nrows=2, ncols=1)\n",
    "sns.boxplot(data['taxvaluedollarcnt'], ax=ax0, color=\"#34495e\").set_title('Total tax value distribution');\n",
    "sns.distplot(data['taxvaluedollarcnt'], ax=ax2, color=\"#34495e\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Land Tax\n",
    "The building or structure tax has a similar right skewed distribution as total tax.  The values range from 1 to 2,165,929, already adjusted for outliers and cleaned up with missing values set to median.  That being said, the median and mode are the same at 122,590 which is lower than the mean of 166,344.\n",
    "\n",
    "The land tax values range from 1 to 2,477,536, also adjusted for outliers and cleaned up with missing values set to median.  Because of this, the median and mode are the same at 167,043 which is lower than the mean of 242,391.\n",
    "\n",
    "Land tax seems to have a larger range of values from the 25th to 75th percentile than the building tax.  This means that the land is valued at a greater variance (287k) than the buildings in certain areas (variance of 179k). We think this could be due to location itself as better neighborhoods, safer areas, or better schools could result in a higher assessment than other locations, thus widening the variance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f, (ax0,ax2,ax3,ax4) = plt.subplots(nrows=4, ncols=1, figsize=[15, 14])\n",
    "sns.boxplot(data['structuretaxvaluedollarcnt'], ax=ax0, color=\"#34495e\").set_title('Building tax distribution');\n",
    "sns.distplot(data['structuretaxvaluedollarcnt'], ax=ax2, color=\"#34495e\");\n",
    "sns.boxplot(data['landtaxvaluedollarcnt'], ax=ax3, color=\"#34495e\").set_title('Land tax distribution');\n",
    "sns.distplot(data['landtaxvaluedollarcnt'], ax=ax4, color=\"#34495e\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessment Year\n",
    "Assessment year is the year that the property was assessed.  The 25th through 75th percentile of values are all from the year 2015 so reading a box plot is not very helpful.  Instead we list out the unique values for assessment year along with our histogram.\n",
    "\n",
    "In the state of California, the base year value is set when you originally purchase the property, based on the sales price listed on the deed.  However, there are exceptions which is why we see a few assessment years from 2000 to 2016 thrown in.\n",
    "\n",
    "In order for assessment year to be useful for our predictions, we should find out what each exception is and what the cause of it not to be assessed at the point of sale.  This could affect the predicted log error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Unique years:', data['assessmentyear'].unique())\n",
    "f, (ax2) = plt.subplots(nrows=1, ncols=1, figsize=[15, 4])\n",
    "sns.distplot(data['assessmentyear'], ax=ax2, color=\"#34495e\")\n",
    "plt.title('Assessment year distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Distribution of Target Variable: Logerror\n",
    "\n",
    "In the training dataset, logerror is the response variable so we are interested in seeing the distribution of log error that we are training on. We visualize this using a boxplot and histogram to get a general picture of the overall distribution. It is symmetric around zero which implies that the model generating the logerror has no bias and is very accurate in most instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = data[~data['logerror'].isnull()]\n",
    "\n",
    "x = train_data['logerror']\n",
    "f, (ax_box, ax_hist) = plt.subplots(2, sharex=True,\n",
    "    gridspec_kw={\"height_ratios\": (.15, .85)}, figsize=(10, 10))\n",
    "sns.boxplot(train_data['logerror'][train_data['logerror'].abs()<1], ax=ax_box, color=\"#34495e\")\n",
    "sns.distplot(\n",
    "    train_data['logerror'][train_data['logerror'].abs()<1],\n",
    "    ax=ax_hist, bins=400, kde=False, color=\"#34495e\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count of Bathrooms\n",
    "We think that the number of bathrooms in a home could be interesting because our data was collected in California where rent is very high.  It is common to buy a rental property and have random tenants.  Tenants that do not know each other may want their own bathroom.  In our case, most homes have 2 bathrooms. Notably, there are outliers with no bathrooms or suspiciously high counts. We see records in the dataset with no bathroom which we justified above as being possible.  Because we are looking at frequency, we chose to visualize the sum of each number of bathrooms (as a category) in a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(data['bathroomcnt'], color=\"#34495e\")\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xlabel('Bathrooms', fontsize=12)\n",
    "plt.title(\"Frequency of Bathroom count\", fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count of Bedrooms\n",
    "For the same reasons we were interested in the number of bathrooms, we are also interested in the number of bedrooms.  In our dataset, most properties have 3 bedrooms and we see fewer instances as we go up or down one bedroom in the data. Here we still see records without any bedrooms which we justified as studios above.  We chose the same visualization (using number of bedrooms as a category and counting the frequency of each category) displayed in a bar chart below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xlabel('Bedrooms', fontsize=12)\n",
    "plt.title(\"Frequency of Bedrooms count\", fontsize=15)\n",
    "sns.countplot(data['bedroomcnt'], color=\"#34495e\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bed to Bath Ratio\n",
    "After visualizing the distribution of bathroom and bedroom counts, we also thought it would be interesting to try to see if the number of bathrooms were dependent on the number of bedrooms. We chose to stick with a bar chart, only this time using the ratio of bedrooms to bathrooms as the category to find the sum counts of.  What we found is most homes have about a ratio of 1.5 bedrooms per 1 bathroom in a property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "non_zero_mask = data['bathroomcnt'] > 0\n",
    "bedroom = data[non_zero_mask]['bedroomcnt']\n",
    "bathroom = data[non_zero_mask]['bathroomcnt']\n",
    "\n",
    "bedroom_to_bath_ratio = bedroom / bathroom\n",
    "bedroom_to_bath_ratio = bedroom_to_bath_ratio[bedroom_to_bath_ratio<6]\n",
    "sns.distplot(bedroom_to_bath_ratio, color=\"#34495e\", kde=False)\n",
    "\n",
    "plt.title('Bed to Bath ratio', fontsize=15)\n",
    "plt.xlabel('Ratio', fontsize=15)\n",
    "plt.ylabel('Count', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Tax Per Square Feet\n",
    "\n",
    "For our last attribute, we calculated the tax per square foot to see if we could find any trends.  We again chose to use a bar chart to plot the ratio and the sum counts.  What we found is that plotting this exposes extreme outliers for possible elimination. Most properties are under a few dollars per square foot but as the visualization reveals, there are suspicious records. However, because this is southern California and land space is limited for continuous growth, there could be a reason that some places have high tax per square feet due to better real estate areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "non_zero_mask = data['calculatedfinishedsquarefeet'] > 0\n",
    "tax = data[non_zero_mask]['taxamount']\n",
    "sqft = data[non_zero_mask]['calculatedfinishedsquarefeet']\n",
    "\n",
    "tax_per_sqft = tax / sqft\n",
    "tax_per_sqft = tax_per_sqft[tax_per_sqft<10]\n",
    "sns.distplot(tax_per_sqft, color=\"#34495e\", kde=False)\n",
    "\n",
    "plt.title('Tax Per Square Feet', fontsize=15)\n",
    "plt.xlabel('Ratio', fontsize=15)\n",
    "plt.ylabel('Count', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absolute Log Error and Number of Occurrences Per Month\n",
    "\n",
    "We compared amount of absolute error based monthly average and found out that the error could be cyclical for the year, where the error dips during the Spring and Summer months and rises during the Winter months.\n",
    "\n",
    "We compared amount of transactions based monthly average and the transactions are the highest during the Spring, Summer, and Fall seasons possibly due to an optimal time to sell property. The transactions are at its lowest during the Winter season.\n",
    "\n",
    "For a cross comparison, we have high number of transactions during the Spring and Summer seasons while the log error is relatively low and we have low number of transactions during the Winter season while the log error is relatively high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "months = train_data['transactiondate'].dt.month\n",
    "month_names = ['January','February','March','April','May','June','July','August','September','October','November','December']\n",
    "train_data['abs_logerror'] = train_data['logerror'].abs()\n",
    "\n",
    "f, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=[17, 7])\n",
    "\n",
    "per_month = train_data.groupby(months)[\"abs_logerror\"].mean()\n",
    "per_month.index = month_names\n",
    "ax0.set_title('Average Log Error Across Month Of 2016')\n",
    "ax0.set_xlabel('Month Of The Year', fontsize=15)\n",
    "ax0.set_ylabel('Log Error', fontsize=15)\n",
    "sns.pointplot(x=per_month.index, y=per_month, color=\"#34495e\", ax=ax0)\n",
    "\n",
    "per_month = train_data.groupby(months)[\"logerror\"].count()\n",
    "per_month.index = month_names\n",
    "ax1.set_title('No Of Occurunces per month In 2016')\n",
    "ax1.set_xlabel('Month Of The Year', fontsize=15)\n",
    "ax1.set_ylabel('Nimber of Occurences', fontsize=15)\n",
    "sns.barplot(x=per_month.index, y=per_month, color=\"#34495e\", ax=ax1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Transactions and Mean Absolute Log Error Per Day of the Week\n",
    "\n",
    "Saturdays and Sundays are non-work days, hence why there is a dip in absolute log error and number of transactions\n",
    "\n",
    "For the workdays, Friday has the most transactions while Monday has the least.\n",
    "\n",
    "For the workdays, Monday has relatively the most log errors while Friday has relatively the least log errors.\n",
    "\n",
    "For cross analysis, Monday has the least transactions with the most error while Friday has the most transactions with the least errors. Sunday and Saturday are special cases and does not have substantial evidence to provide any trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weekday = train_data['transactiondate'].dt.weekday\n",
    "weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "abs_logerror = train_data['logerror'].abs()\n",
    "\n",
    "f, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=[17, 7])\n",
    "\n",
    "to_plot = abs_logerror.groupby(weekday).count()\n",
    "to_plot.index = weekdays\n",
    "to_plot.plot(color=\"#34495e\", linewidth=4, ax=ax0)\n",
    "ax0.set_title('Number of Transactions Per Day')\n",
    "ax0.set_ylabel('Number of Transactions', fontsize=15)\n",
    "ax0.set_xlabel('Day', fontsize=15)\n",
    "\n",
    "to_plot = abs_logerror.groupby(weekday).mean()\n",
    "to_plot.index = weekdays\n",
    "to_plot.plot(color=\"#34495e\", linewidth=4, ax=ax1)\n",
    "ax1.set_title('Mean Absolute Log Error Per Day')\n",
    "ax1.set_ylabel('Mean Absolute Log Error', fontsize=15)\n",
    "ax1.set_xlabel('Day', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Variable Correlation Heatmap\n",
    "\n",
    "Heatmap of correlations are represented where the warmer colors are highly correlated, white is non-correlated, and colder colors are negatively correlated. We see that calculated finished square feet is correlated with finished square feet, due to collinearity. Tax amounts and year built are also highly correlated to finished square feet as well as with one another. \n",
    "\n",
    "Latitude and longitude are negatively correlated with each other possibly because the beachfront properties are more expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = data[~data['logerror'].isnull()]\n",
    "\n",
    "continuous_vars = variables[variables['type'].isin(['ratio', 'interval'])].index\n",
    "continuous_vars = continuous_vars[continuous_vars.isin(data.columns)]\n",
    "continuous_vars = continuous_vars.sort_values()\n",
    "\n",
    "corrs = train_data[continuous_vars].corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(corrs, ax=ax)\n",
    "plt.title(\"Variables correlation map\", fontsize=20)\n",
    "plt.xlabel('Continuous Variables', fontsize=15)\n",
    "plt.ylabel('Continuous Variables', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Longitude and Latitude Data Points\n",
    "From a simple graph, we can see the shoreline of California as well as possible areas of obstruction, such as mountains that prevent property growth in those areas. The majority of properties are in the center to upper left of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12));\n",
    "sns.jointplot(x=data.latitude.values, y=data.longitude.values, size=10, color=\"#34495e\");\n",
    "plt.ylabel('Longitude', fontsize=15)\n",
    "plt.xlabel('Latitude', fontsize=15)\n",
    "plt.title('Longitude and Latitude Data Points', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Stories vs Year Built\n",
    "As architectural feats improved, we started to see more properties with 2 or more stories by 1950. The number of one story properties also increased during that time. The baby boomers, the end of WWII and readily available steel, and mortgage incentives may be the cause of the increase of more properties being built as well as more stories per property. Note: because we filled in missing values as the median value, the 1965 spike in the data is artificial until we use other methods to impute year built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,ax1= plt.subplots()\n",
    "fig.set_size_inches(20,10)\n",
    "yearMerged = data.groupby(['yearbuilt', 'numberofstories'])[\"parcelid\"].count().unstack('numberofstories').fillna(0)\n",
    "yearMerged = yearMerged.loc[1900:]\n",
    "yearMerged.index.name = 'Year Built'\n",
    "\n",
    "plt.title('Number of Stories Per Year Built', fontsize=15)\n",
    "plt.ylabel('Count', fontsize=15);\n",
    "\n",
    "yearMerged.plot(ax=ax1, linewidth=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatterplot of Log Error and Calculated Finished Square Feet\n",
    "\n",
    "We are plotting our best correlated variable calculatedfinishedsquarefeet against the logerror. We don't see any linear relationship from the scatter plot below even though it is evenly distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "column = \"calculatedfinishedsquarefeet\"\n",
    "train_data = data[~data['logerror'].isnull()]\n",
    "\n",
    "sns.jointplot(train_data[column], train_data['logerror'], size=10, color=\"#34495e\")\n",
    "plt.ylabel('Log Error', fontsize=12)\n",
    "plt.xlabel('Calculated Finished Square Feet', fontsize=15)\n",
    "plt.title(\"Calculated Finished Square Feet Vs Log Error\", fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FYI Modeling and Evaluation, delete later\n",
    "\n",
    "Different tasks will require different evaluation methods. Be as thorough as possible when analyzing the data you have chosen and use visualizations of the results to explain the performance and expected outcomes whenever possible. Guide the reader through your analysis with plenty of discussion of the results. Each option is broken down by:\n",
    "\n",
    "* [10 Points] Train and adjust parameters\n",
    "* [10 Points] Evaluate and Compare\n",
    "* [10 Points] Visualize Results\n",
    "* [20 Points] Summarize the Ramifications\n",
    "\n",
    "Option A: Cluster Analysis\n",
    "* Train: Perform cluster analysis using several clustering methods (adjust parameters).\n",
    "* Eval: Use internal and/or external validation measures to describe and compare the clusterings and the clusters— how did you determine a suitable number of clusters foreach method?\n",
    "* Visualize: Use tables/visualization to discuss the found results. Explain each visualization in detail.\n",
    "* Summarize: Describe your results. What findings are the most interesting and why?\n",
    "\n",
    "Option B: Association Rule Mining\n",
    "* Train: Create frequent itemsets and association rules (adjust parameters).\n",
    "* Eval: Use several measure for evaluating how interesting different rules are.\n",
    "* Visualize: Use tables/visualization to discuss the found results.\n",
    "* Summarize: Describe your results. What findings are the most compelling and why?\n",
    "\n",
    "Option C: Collaborative Filtering\n",
    "* Train: Create user-item matrices or item-item matrices using collaborative filtering (adjust parameters).\n",
    "* Eval: Determine performance of the recommendations using different performance measures (explain the ramifications of each measure).\n",
    "* Visualize: Use tables/visualization to discuss the found results. Explain each visualization in detail.\n",
    "* Summarize: Describe your results. What findings are the most compelling and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Train\"></a>\n",
    "# Train and Adjust Parameters\n",
    "<b>10 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "Perform cluster analysis using several clustering methods (adjust parameters).\n",
    "</i>\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "def calculate_max_distance(group, radius = 3959):\n",
    "    \"\"\"\n",
    "    This computes max distance from points in the cluster\n",
    "    \n",
    "    radius = 3959 # miles (some algorithms use 3956)\n",
    "    radius = 6371 # kilometers (some algorithms use 6367)\n",
    "    radius = 3959 * 5280 # feet\n",
    "    radius = 6371 * 1000 # meters\n",
    "    \n",
    "    Refference: https://gist.github.com/rochacbruno/2883505\n",
    "    \"\"\"\n",
    "    lat1, lon1 = group[['latitude','longitude']].min()\n",
    "    lat2, lon2 = group[['latitude','longitude']].max()\n",
    "    \n",
    "    dlat = math.radians(lat2-lat1)\n",
    "    dlon = math.radians(lon2-lon1)\n",
    "    a = math.sin(dlat/2) * math.sin(dlat/2) + math.cos(math.radians(lat1)) \\\n",
    "        * math.cos(math.radians(lat2)) * math.sin(dlon/2) * math.sin(dlon/2)\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    distance = radius * c\n",
    "    return distance\n",
    "\n",
    "def cluster_analysis(data, labels, per_cluster=True):\n",
    "    metrics = ['cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan']\n",
    "    if per_cluster:\n",
    "        for metric in metrics:\n",
    "            data[metric] = silhouette_samples(data, labels, metric=metric)\n",
    "\n",
    "        per_cluster_metrics = pd.DataFrame()\n",
    "        for cluster_id, group in data.groupby(labels):\n",
    "            row = group[metrics].mean()\n",
    "\n",
    "            row['max distance in miles'] = calculate_max_distance(group)\n",
    "            row['cluster_id'] = cluster_id\n",
    "            per_cluster_metrics = per_cluster_metrics.append([row])\n",
    "        per_cluster_metrics = per_cluster_metrics.set_index('cluster_id')\n",
    "    else:\n",
    "        per_cluster_metrics = None\n",
    "    \n",
    "    avg_metrics = {}\n",
    "    for metric in 'cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan':\n",
    "        avg_metrics[metric] = silhouette_score(data, labels, metric=metric)\n",
    "\n",
    "    avg_metrics = pd.DataFrame([avg_metrics], index = ['Average for all Clusters'])\n",
    "    \n",
    "    return per_cluster_metrics, avg_metrics\n",
    "\n",
    "data = dataset[['latitude', 'longitude', 'taxamount']].sample(n=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train_kmeans\"></a>\n",
    "\n",
    "## KMeans\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "cls = MiniBatchKMeans(n_clusters=20, random_state=1)\n",
    "labels = cls.fit_predict(data)\n",
    "\n",
    "plt.scatter(\n",
    "    data['latitude'], data['longitude'],\n",
    "    c=labels, cmap=plt.cm.rainbow, s=5,\n",
    "    linewidths=0\n",
    ")\n",
    "\n",
    "centroids = cls.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], marker='+', s=200, linewidths=3, color='k')\n",
    "\n",
    "per_cluster_metrics, avg_metrics = cluster_analysis(data, labels)\n",
    "display(per_cluster_metrics), display(avg_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train_spectral\"></a>\n",
    "\n",
    "## Spectral Clustering\n",
    "\n",
    "In practice Spectral Clustering is very useful when the structure of the individual clusters is highly non-convex or more generally when a measure of the center and spread of the cluster is not a suitable description of the complete cluster. For instance when clusters are nested circles on the 2D plan.\n",
    "\n",
    "What we have is geo data and it is a good candidate for this type of clustering.\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "cls = SpectralClustering(n_clusters=20, affinity = 'nearest_neighbors')\n",
    "labels = cls.fit_predict(data)\n",
    "\n",
    "plt.scatter(\n",
    "    data['latitude'], data['longitude'],\n",
    "    c=labels, cmap=plt.cm.rainbow, s=5,\n",
    "    linewidths=0\n",
    ")\n",
    "\n",
    "per_cluster_metrics, avg_metrics = cluster_analysis(data, labels)\n",
    "display(per_cluster_metrics), display(avg_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train_agglomerative\"></a>\n",
    "\n",
    "## Agglomerative Clustering\n",
    "Recursively merges the pair of clusters that minimally increases a given linkage distance.\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "data = dataset[['latitude', 'longitude', 'taxamount']].sample(n=10000)\n",
    "\n",
    "for linkage in 'average', 'complete', 'ward':\n",
    "    cls = AgglomerativeClustering(n_clusters=20, linkage=linkage)\n",
    "    labels = cls.fit_predict(data)\n",
    "\n",
    "    print('-' * 80)\n",
    "    print(linkage)\n",
    "    per_cluster_metrics, avg_metrics = cluster_analysis(data, labels)\n",
    "    display(per_cluster_metrics), display(avg_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Evaluate\"></a>\n",
    "# Evaluate and Compare\n",
    "<b>10 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "Use internal and/or external validation measures to describe and compare the clusterings and the clusters— how did you determine a suitable number of clusters foreach method?\n",
    "</i>\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from pprint import pprint\n",
    "\n",
    "result = pd.DataFrame()\n",
    "\n",
    "for n_clusters in [5, 20, 50, 100, 150, 250, 400]:\n",
    "    cls = MiniBatchKMeans(n_clusters=n_clusters, random_state=1)\n",
    "    labels = cls.fit_predict(data)\n",
    "    \n",
    "    _, avg_metrics = cluster_analysis(data, labels, per_cluster=False)\n",
    "    avg_metrics['n_clusters'] = n_clusters\n",
    "\n",
    "    result = result.append(avg_metrics)\n",
    "result = result.set_index('n_clusters')\n",
    "\n",
    "result[['cosine']].plot()\n",
    "\n",
    "all_others = result.columns\n",
    "all_others = all_others[~all_others.isin(['cosine'])]\n",
    "\n",
    "result[all_others].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Visualize\"></a>\n",
    "# Visualize Results\n",
    "<b>10 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "Use tables/visualization to discuss the found results. Explain each visualization in detail.\n",
    "</i>\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "data = dataset[['latitude', 'longitude', 'taxamount']].sample(n=10000)\n",
    "\n",
    "cls = MiniBatchKMeans(n_clusters=20, random_state=1)\n",
    "cls.fit(data)\n",
    "centroids = cls.cluster_centers_\n",
    "labels = cls.labels_\n",
    "\n",
    "for cluster_id in np.unique(labels):\n",
    "    d = data[labels == cluster_id]\n",
    "    label = str(cluster_id) + ' mean tax: $' + str(int(d['taxamount'].mean()))\n",
    "    plt.scatter(d['latitude'], d['longitude'], label=label)\n",
    "plt.legend()\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], marker='+', s=200, linewidths=3, color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Ramifications\"></a>\n",
    "# Summarize the Ramifications\n",
    "<b>20 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "Describe your results. What findings are the most interesting and why?\n",
    "</i>\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Deployment\"></a>\n",
    "# Deployment\n",
    "<b>10 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "Be critical of your performance and tell the reader how you current model might be usable by other parties. Did you achieve your goals? If not, can you reign in the utility of your modeling?\n",
    "* How useful is your model for interested parties (i.e., the companies or organizations\n",
    "that might want to use it)?\n",
    "* How would your deploy your model for interested parties?\n",
    "* What other data should be collected?\n",
    "* How often would the model need to be updated, etc.?\n",
    "</i>\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.loc[:,'latitude'] /= 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.loc[:,'longitude'] /= 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del data['taxamount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(data.loc[:,'latitude'].max())\n",
    "print(data.loc[:,'latitude'].min())\n",
    "print(data.loc[:,'longitude'].max())\n",
    "print(data.loc[:,'longitude'].min())\n",
    "print((34.769356+33.340212)/2)\n",
    "print((-117.556224-119.412512)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Exceptional\"></a>\n",
    "# Exceptional Work\n",
    "<b>10 points</b>\n",
    "\n",
    "<i>\n",
    "<b>Description:</b><br/>\n",
    "You have free reign to provide additional analyses or combine analyses.\n",
    "</i>\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    " \n",
    "map = Basemap(projection='merc', lat_0 = 34.054784, lon_0 = -118.484368,\n",
    "    resolution = 'h', area_thresh = 0.1,\n",
    "    llcrnrlon=data.loc[:,'longitude'].min(), llcrnrlat=data.loc[:,'latitude'].min(),\n",
    "    urcrnrlon=data.loc[:,'longitude'].max(), urcrnrlat=data.loc[:,'latitude'].max())\n",
    " \n",
    "map.drawcoastlines()\n",
    "map.drawcountries()\n",
    "map.fillcontinents(color = 'coral')\n",
    "map.drawmapboundary()\n",
    "\n",
    "lats = data['latitude'].tolist()\n",
    "lons = data['longitude'].tolist()\n",
    "\n",
    "x,y = map(lons, lats)\n",
    "map.plot(x, y, 'bo', markersize=2)\n",
    " \n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gmplot\n",
    "from gmplot import GoogleMapPlotter as gmp\n",
    "\n",
    "gmap = gmplot.GoogleMapPlotter(34.054784, -118.484368, 16)\n",
    "\n",
    "gmap.plot(lats, lons, 'cornflowerblue', edge_width=10)\n",
    "gmap.scatter(lats, lons, '#3B0B39', size=40, marker=False)\n",
    "#gmap.scatter(marker_lats, marker_lngs, 'k', marker=True)\n",
    "#gmap.heatmap(heat_lats, heat_lngs)\n",
    "\n",
    "gmap.draw(\"mymap.html\")\n",
    "gmap = gmp.from_geocode(\"Los Angeles\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "folium.Map(location=[34.054784, -118.484368],tiles='Stamen Toner',zoom_start=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "m = folium.Map(location=[34.054784, -118.484368],tiles='Stamen Toner',zoom_start=9)\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    folium.Marker(\n",
    "        location=[row['latitude'], row['longitude']]\n",
    "    ).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = dataset[['latitude', 'longitude', 'taxamount']].sample(n=5000)\n",
    "data.loc[:,'latitude'] /= 1000000\n",
    "data.loc[:,'longitude'] /= 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data=data.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gmaps\n",
    "gmaps.configure(api_key=\"AIzaSyBWNBsS3ZUCCiZKUCdLD685NVJzB5fEJNY\") # Your Google API key\n",
    "\n",
    "fig = gmaps.figure()\n",
    "fig.add_layer(gmaps.heatmap_layer(data))\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['latitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_amount = float(data['taxamount'].max())\n",
    "max_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd \n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "max_amount = float(data['taxamount'].max())\n",
    "\n",
    "hmap = folium.Map(location=[34.054784, -118.484368], zoom_start=7, )\n",
    "\n",
    "hm_wide = HeatMap( zip(data['latitude'], data['longitude'],data['taxamount']), \n",
    "                   min_opacity=0.2,\n",
    "                   max_val=max_amount,\n",
    "                   radius=17, blur=15, \n",
    "                   max_zoom=1, \n",
    "                 )\n",
    "\n",
    "hmap.add_children(plugins.HeatMap(data, radius=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = folium.Map([34.054784, -118.484368], zoom_start=9)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, row in data.iterrows():\n",
    "    folium.Marker([row['latitude'], row['longitude']]).add_to(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from folium import plugins\n",
    "\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "#max_amount = float(data['taxamount'].max())\n",
    "\n",
    "hmap = folium.Map(location=[34.054784, -118.484368], zoom_start=7, )\n",
    "\n",
    "hm_wide = HeatMap( data, \n",
    "                   min_opacity=0.2,\n",
    "                   max_val=51293.14453125,\n",
    "                   radius=17, blur=15, \n",
    "                   max_zoom=1, \n",
    "                 )\n",
    "\n",
    "hmap.add_child(hm_wide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- Kernels from Kaggle competition: https://www.kaggle.com/c/zillow-prize-1/kernels\n",
    "- Pandas cookbook: https://pandas.pydata.org/pandas-docs/stable/cookbook.html\n",
    "- Stackoverflow pandas questions: https://stackoverflow.com/questions/tagged/pandas\n",
    "- Sklearn Clustering: http://scikit-learn.org/stable/modules/clustering.html\n",
    "- Data Mining Notebooks: https://github.com/eclarson/DataMiningNotebooks/blob/master/09.%20Clustering%20and%20Discretization.ipynb\n",
    "\n",
    "<a href=\"#top\">⏫ Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
